{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#installing num2words library\n",
        "!pip install num2words"
      ],
      "metadata": {
        "id": "4qk-Qco95pLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422a1eb0-c5e2-45a4-d0a6-2d5334456f28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tCriNgkHt-p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2201f0fd-3e80-416d-88f5-fad6baf30dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#importing packages\n",
        "import gzip\n",
        "import shutil\n",
        "import json\n",
        "from pprint import pprint\n",
        "import gensim\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Make sure you have NLTK's stopwords downloaded\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "nltk.download('punkt')\n",
        "from num2words import num2words\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports the NLTK library, downloads the NLTK stopwords dataset, and then imports the stopwords dataset from the NLTK corpus.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "3LsNKa4k5tAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94474fb6-58a0-4790-ad7e-81560f179bca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import string\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "# NLTK\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) #this depends on each language\n",
        "stop_words.add(\"rs\")\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "\n",
        "\n",
        "# Plotting tools\n",
        "# import pyLDAvis\n",
        "# import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
      ],
      "metadata": {
        "id": "1Xs5wqks5vPB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download and install the spaCy English language model \"en_core_web_sm\".\n",
        "import spacy\n",
        "from spacy.cli.download import download\n",
        "download(model=\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7znvwnaC5xSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787c2729-9493-4b90-fc8c-919abf22868e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read data from file into a dataframe using pandas.\n",
        "df = pd.read_csv(\"/content/cs_domain_final.csv\")"
      ],
      "metadata": {
        "id": "iYVDG5w15zdW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display column names of dataframe.\n",
        "df.columns"
      ],
      "metadata": {
        "id": "3MGXyvea8CyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b813fc69-4d76-413b-c167-15add5b952c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['paperId', 'id', 'title', 'fieldsOfStudy', 'source', 'version', 'added',\n",
              "       'created', 'text'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replace newline characters with spaces in the 'text' column of the DataFrame 'df'.\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: str(x).replace('\\n', ' '))"
      ],
      "metadata": {
        "id": "156Im1sU6E9G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download the NLTK 'punkt' and 'stopwords' resources and import necessary modules for text processing.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "_drLo8wT6Lrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580df27a-eb4e-4b72-fad1-99297c1c2a1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "telFvVSD1ghH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function 'tokenize_sentences' that tokenizes input text into sentences using NLTK's 'sent_tokenize' method.\n",
        "def tokenize_sentences(text):\n",
        "    return sent_tokenize(text)"
      ],
      "metadata": {
        "id": "Kai7vIev6N8Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the 'text' column into sentences and store the result in a new 'sentences' column in the DataFrame 'df'\n",
        "df['sentences'] = df['text'].apply(tokenize_sentences)"
      ],
      "metadata": {
        "id": "2P8I-Bzu6OhC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function 'remove_stopwords_from_list' that removes stopwords from a list of sentences and returns the cleaned sentences.\n",
        "def remove_stopwords_from_list(sentences):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        cleaned_sentences.append(' '.join(filtered_words))\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "-KivJvnN6ROu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_stopwords_from_list' function to remove stopwords from the 'sentences' column in the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_stopwords_from_list)"
      ],
      "metadata": {
        "id": "1VjstsOi6RyM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function 'lemmatize' that lemmatizes words in a list of sentences using NLTK's WordNetLemmatizer.\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize(sentences):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sent_list = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent)\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        words = \" \".join(words)\n",
        "        sent_list.append(words)\n",
        "\n",
        "    return sent_list\n"
      ],
      "metadata": {
        "id": "fcUPHh0xTvId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d451c1-f1e3-4972-ab39-1ac49ee252c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'lemmatize' function to lemmatize words in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(lemmatize)"
      ],
      "metadata": {
        "id": "Q3ccooNyT3jT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove URLs from a list of sentences using regular expressions and store the cleaned sentences.\n",
        "def remove_urls_from_list(sentences_list):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        if isinstance(sentence, str):\n",
        "            # Use a regular expression to find and remove URLs\n",
        "            sentence = re.sub(r'http\\S+|www\\S+|https\\S+', '', sentence, flags=re.MULTILINE)\n",
        "        cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "AdJYthOn6Tye"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_urls_from_list' function to remove urls from the beginning of sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_urls_from_list)"
      ],
      "metadata": {
        "id": "gRhXoVn86WYw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove bullet points ('*' or 'â€¢') at the beginning of each sentence in a list of sentences.\n",
        "def remove_bullet_points_from_list(sentences_list):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        if isinstance(sentence, str):\n",
        "            # Remove bullet points ('*' or 'â€¢') at the beginning of each sentence\n",
        "            sentence = re.sub(r'^\\s*[\\*\\â€¢]+\\s*', '', sentence)\n",
        "        cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "t3zCDog56YcN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_bullet_points_from_list' function to remove bullet points from the beginning of sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_bullet_points_from_list)"
      ],
      "metadata": {
        "id": "STrP3uAU6aGs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove apostrophes (' and â€™) from sentences in a list of sentences.\n",
        "def remove_apostrophes_from_list(sentences_list):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        if isinstance(sentence, str):\n",
        "            # Use a regular expression to remove apostrophes\n",
        "            sentence = re.sub(r\"[\\']\", '', sentence)\n",
        "            sentence = re.sub(r\"['â€™]\", '', sentence)\n",
        "        cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "qbvrKZI86bwj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_apostrophes_from_list' function to remove apostrophes from sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_apostrophes_from_list)"
      ],
      "metadata": {
        "id": "o1AmvYn_6fI8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove hyphens between words in sentences in a list of sentences.\n",
        "def remove_hyphens_from_list(sentences_list):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        if isinstance(sentence, str):\n",
        "            # Remove hyphens from the sentence\n",
        "            sentence = re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', sentence)\n",
        "        cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "XOQZMFI26hUp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_hypens_from_list' function to remove hypens from the beginning of sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_hyphens_from_list)"
      ],
      "metadata": {
        "id": "HVZOzMOz6i_D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove enumerations (e.g., numbers followed by periods) from sentences in a list of sentences.\n",
        "def remove_enumerations_from_list(sentences_list):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        if isinstance(sentence, str):\n",
        "            # Remove enumerations from the sentence\n",
        "            sentence = re.sub(r'^\\s*\\d+\\.\\s*', '', sentence)\n",
        "        cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "4kt_YaOx6ku4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_enumerations_from_list' function to remove enumerations from sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_enumerations_from_list)"
      ],
      "metadata": {
        "id": "MJtsHtIc6nn8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert numerical values to their text representations in sentences in a list of sentences.\n",
        "def numerical_to_text_conversion(sentences_list):\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for sentence in sentences_list:\n",
        "        if isinstance(sentence, str):\n",
        "            # Find and replace numerical values with their text representations\n",
        "            sentence = re.sub(r'\\b\\d+\\b', lambda match: num2words(int(match.group())), sentence)\n",
        "        cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences"
      ],
      "metadata": {
        "id": "6Ty-sIgW6oRq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'numerical_to_text_conversion' function to convert numerical values to their text representations in sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(numerical_to_text_conversion)"
      ],
      "metadata": {
        "id": "jKhC7Ln16rDn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation from sentences in a list of sentences.\n",
        "def remove_punctuation(sentences):\n",
        "  sent_list=[]\n",
        "  for sent in sentences:\n",
        "    words=word_tokenize(sent)\n",
        "    str=\"\"\n",
        "    for i in range(len(words)):\n",
        "      x=re.sub(\"[^\\w\\s]|_\",\"\",words[i]).strip()\n",
        "      str=str+\" \"+x\n",
        "      str=str.strip()\n",
        "    sent_list.append(str)\n",
        "  return sent_list\n"
      ],
      "metadata": {
        "id": "vvikVK4I6rhu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'remove_punctuation' function to remove punctuation from sentences in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "t3a3_CTq6s8v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert sentences to lowercase in a list of sentences.\n",
        "def lowercase_sentences(sentences):\n",
        "    # Convert each sentence to lowercase\n",
        "    lowercased_sentences = [sentence.lower() for sentence in sentences]\n",
        "    return lowercased_sentences"
      ],
      "metadata": {
        "id": "zybkbCYDS-tO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply the 'lowercase_sentences' function to convert sentences to lowercase in the 'sentences' column of the DataFrame 'df'.\n",
        "df['sentences'] = df['sentences'].apply(lowercase_sentences)"
      ],
      "metadata": {
        "id": "DFVyqsJ4S_5X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spilitting of train and test data"
      ],
      "metadata": {
        "id": "XvKUkCTR2UsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split the DataFrame 'df' into a training set 'train' containing the first 35 rows and a test set 'test' containing rows 3000 to 3999.\n",
        "train = df[:30]\n",
        "test = df[3000:4000]"
      ],
      "metadata": {
        "id": "GhXmF4EP6zTE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the 'math' module and the 'defaultdict' class from the 'collections' module.\n",
        "import math\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "TM6GMefP7Vkz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the 'sentences' column of the 'train' DataFrame to a list and print it.\n",
        "sentences = train['sentences'].tolist()\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "ZlpQTZZw7j8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d69eef0-0ed4-48c6-fd0f-9389aa492026"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['complete targets coverage energy harvesting internet things ambient backscatter article considers deriving set cover set active node responsible monitoring target internet things iot network', 'key distinction prior work article considers sensor node aided backscatter communication allow communicate negligible energy cost using ambient radio frequency rf signal', 'article contains three main novelty', 'first present mixed integer linear program milp used compute global optimal solution', 'second also outline centralized greedy scheduling cgs algorithm selects node based energy number covered target', 'third present distributed greedy scheduling dgs algorithm selects node according energy level', 'simulation result show equipping node ambient backscattering capability increase ratio complete target coverage one hundred compared existing technique'], ['nonorthogonal multiple access enabled two way relay system using signal alignment conventional two way relay nonorthogonal multiple access twr noma system diversity order equal zero even perfect successive interference cancellation psic', 'article leverage multiple inputmultiple output mimo technique twr noma system extract spatial diversity gain performance enhancement', 'communications two user pair assisted relay', 'precoding vector user well matrix relay specially designed perform signal alignment sa', 'derive analytical closed form expression outage probability diversity order psic imperfect successive interference cancellation ipsic', 'addition ergodic rate ergodic sum rate analyzed', 'optimal power allocation system outage probability ergodic sum rate analyzed based quality service', 'simulation result provided confirm derived analytical result', 'result show combination sa twr noma decrease outage probability improve diversity order effectively', 'ergodic rate finally arise ceiling psic ipsic ergodic sum rate keep increasing power allocated distant user increase', 'however conventional twr noma complete opposite', 'summary compared twr noma twr nomasa higher diversity order enables distant user contribute system performance'], ['event triggered adaptive output feedback control stochastic nonlinear systems time varying full state constraints brief investigates issue event triggered adaptive output feedback control stochastic nonlinear system time varying full state constraint', 'firstly unmeasurable state estimated fuzzy observer', 'secondly quartic time varying barrier lyapunov function constructed avoid violation time varying constraint', 'thirdly command filter technique error compensation mechanism incorporated controller design get issue explosion complexity compensate filtered error', 'then event triggered mechanism introduced improve efficiency resource utilization', 'shown tracking error converge desired neighborhood origin signal closed loop system bounded', 'finally validity control strategy demonstrated physical example'], ['mmu mpu adaptation pip kernel constrained devices article present hardware based memory isolation solution constrained device', 'existing solution target high end embedded system typically arm cortex a memory management unit mmu sel4 pip formally verified kernel target low end device aces minion trustlite ewok limited flexibility proposing single level isolation', 'approach consists adapting pip inherit flexibility multiple level isolation using memory protection unit mpu instead mmu since mpu commonly available constrained embedded system typically armv7 cortex m4 armv8 cortex m33 similar device', 'paper describes design pip mpu pip s variant based mpu rationale behind choice', 'validate proposal implementation nrf52840 development kit perform various evaluation memory footprint cpu cycle energy consumption', 'demonstrate although prototyped pip mpu cause sixteen overhead performance energy consumption reduce attack surface accessible application memory one hundred two privileged operation ninetynine', 'pip mpu take le ten kb flash six kb core component five hundred and fifty b ram'], ['data driven cyber attack detection intelligent attacks islanded dc microgrids letter data driven cyber attack detection method islanded dc microgrids proposed', 'data collected monitoring behavior intelligent attacker able bypass conventional cyber attack detection algorithm disrupt operation system', 'reinforcement learning algorithm emulates action intelligent attacker exploit vulnerability index based cyber attack detection method discordant detection algorithm', 'data used train neural networkbased detector complement conventional method additional capability detect larger set possible attack', 'experiment effectiveness proposed method validated'], ['algorithm determining types inverse kinematics solutions sequential planar robots representation configuration space work defines new way different type solution inverse kinematics ik problem planar robot serial topology present algorithm solving it', 'developed algorithm allows finding solution wide range robot using geometric approach representing point polar coordinate system', 'inverse kinematics one important studied challenging problem robotics aim calculate value joint variable given desired position orientation robot s end effector', 'configuration space defined joint angle basis motion planning algorithm', 'areas working configuration space generated reachable different type solution', 'programs created use proposed algorithm robot two three rotational degree freedom graphically present result workspace configuration space', 'possibility transitioning one type solution another passing singular configuration discussed', 'result important planning motion workspace configuration space well design kinematic analysis robot'], ['ultra wideband swarm ranging protocol dynamic dense networks nowadays wearable portable device also aerial ground robot made smaller lighter cheaper thus large hundred may form swarm participate complicated cooperative application searching rescuing mapping war battling', 'devices robot swarm three important feature namely large number high mobility short distance hence form dynamic dense wireless network', 'successful swarm cooperative application require low latency communication real time localization', 'paper proposes use ultra wideband uwb radio technology implement functionality uwb time sensitive accurate distance calculated using transmission reception timestamps data message', 'uwb swarm ranging protocol designed achieve simultaneously wireless data communication swarm ranging allows devicerobot compute distance peer neighbor time', 'protocol designed dynamic dense network meanwhile also used various wireless network implemented various type devicesrobots including low end one', 'experiment protocol implemented crazyflies stm32 microcontroller powered micro drone onboard uwb wireless transceiver chip dw1000', 'extensive real world experiment conducted verify proposed protocol various performance aspect total nine crazyflie drone compact area', 'implemented swarm ranging protocol open sourced http githubcomseu netsicrazyflie firmware'], ['enhancing robustness deep learning based fingerprinting improve deepfake attribution artificial fingerprinting af so called digital watermarking technique used conduct deepfake attribution ensuring medium authenticity', 'however af prioritize robustness certain kind distortion making embedded watermark vulnerable standard image processing operation', 'insufficient robustness reduces practicality digital watermarking technique', 'address issue propose enhanced distortion agnostic artificial fingerprinting eda af framework introduces novel noise layer consisting attack booster followed convolutional network based attacker', 'attacker simulates various distortion exploiting adversarial learning af distortion agnostic robustness', 'meanwhile due modeling limitation convolutional network also employ attack booster apply set differentiable image distortion can not well simulated attacker', 'extensive experimental result show proposed approach improves quality extracted fingerprint', 'eda af improve bitwise accuracy thirtysix take another step forward road deepfake attribution'], ['fpga based updatable packet classification using tss combined bit selecting tree openflow switch deployed sdn enable wide spectrum non traditional application', 'promising alternative brutal force tcams fpga based packet classification actively investigated', 'however none existing fpga design achieve high performance search update large scale rule set', 'address issue propose tcbtree fpga based algorithmic scheme packet classification', 'specifically algorithmic side i two stage framework consisting heterogeneous algorithm proposed rule mapped several balanced tree without rule replication ii remaining rule centralized tss tuple space search architecture together real time feedback scheme designed enhance efficiency tss search fpga iii tree dilution method designed equalize rule distribution tree latency tree search reduced', 'hardware side i efficient data structure set designed convert tree traversal addressing process break constraint limited tree depth imbalanced node distribution ii distinct fully pipelined design multiple level parallelism efficiently explored multi core multi searchengine coarse grained pipeline herein', 'experimental result using classbench show that implementation tcbtree fpga average classification throughput 1k 10k 32k 100k rule set achieve seven hundred and eightyeighteight mpps four hundred and fourthree mpps two hundred and thirtyseven mpps fortyoneeight mpps respectively update throughput benchmark rule set one mups'], ['integrated digital twin simulation scheduling system cyber physical digital twin environment paper described research study integrated digital twin simulation scheduling system cyber physical digital twin environment manufacturing', 'proposed approach provides optimal production schedule based real time information established cyber physical digital twin platform', 'system integrated existing manufacturing system erp mes scada system via cyber physical digital twin platform scheduling result adaptive dynamically changing environment market demand', 'proposed system designed leverage advantage digital twin simulation optimization scheduling technique', 'prototype developed illustrate application case electronics manufacturing industry'], ['prototype instrumented rock bolt continuous monitoring roof fall hazard deep underground mines roof fall currently one dangerous threat associated underground mining great depth', 'every occurrence event pose significant risk mining crew disturbs continuity mining process clearly affect economy exploitation process', 'development reliable monitoring system may significantly reduce impact eventual roof failure positive effect sustainability extraction process', 'within research study prototype instrumented rock bolt developed continuous stress measurement presented', 'procedure four groove multilevel instrumented rock bolt described calibration process shown', 'then preliminary result long term situ monitoring presented', 'based continuous monitoring stress distribution within immediate roof stratum concluded developed instrumented rock bolt provides reliable result useful device ensuring possibility early warning miner increasing roof fall risk'], ['repqc threefour ujop fortyeight kops post quantum crypto processor multiple mathematical problems post quantum cryptography pqc investigated replace classical public cryptography algorithm would completely broken large scale quantum computer', 'however current pqc scheme completely different mathematical foundation parameter set make implementation unified pqc processor extremely challenging', 'address issue agile pqc processor repqc proposed work support scheme multiple mathematical problem', 'first hierarchical calculation framework ranging algorithm level task level coefficient level proposed achieve desirable flexibility energy efficiency', 'second hybrid processing element array built support arithmetic logical operation simultaneously algorithm hardware co design utilized task level scheduler improve algorithm oriented energy efficiency', 'finally parallelism exploration algorithm level computation transformation utilized optimize configuration repqc higher throughput', 'fabricated twentyeight nm process repqc achieves energy efficiency threefour ujop throughput fortyeight kops twotimes twentythreetimes higher state ofthe art work respectively', 'best knowledge repqc first silicon proven pqc processor different mathematical problem'], ['tackling climate change machine learning climate change one greatest challenge facing humanity we machine learning ml expert may wonder help', 'describe ml powerful tool reducing greenhouse gas emission helping society adapt changing climate', 'smart grid disaster management identify high impact problem existing gap filled ml collaboration field', 'recommendation encompass exciting research question well promising business opportunity', 'call ml community join global effort climate change'], ['many objective optimization based federal deep generation model enhancing data processing capability iot rapid progress artificial intelligence expands wide applicability internet things iot', 'meanwhile data insufficient data source privacy key supply chain challenge facing iot especially healthcare industry', 'address problem healthcare iot article propose skin cancer detection model based federated learning integrated deep generation model', 'first employ dual generative adversarial network address problem insufficient data', 'addition improve quality generated image synchronously optimize sharpness image frechet inception distance image diversity loss using knee point driven evolutionary algorithm knea', 'then protect patient information privacy training federated skin cancer framework', 'finally employ isic two thousand and eighteen dataset test performance proposed training model different situation including using identically distributed data nonidentically distributed data sparse convolutional neural network fully connected convolutional neural network', 'experiment result demonstrate accuracy area curve reach ninetyone eightyeight respectively', 'model help resolve problem insufficient data smart medicine iot protect privacy user data also providing excellent detection rate'], ['exploiting combined gracegrace fo solutions determine gravimetric excitations polar motion observations gravity recovery climate experiment grace grace follow on grace fo mission used estimate gravimetric excitation polar motion pm reflects contribution mass change continental hydrosphere cryosphere pm variation', 'many solution earth s gravity field variation developed institute around world based gracegrace fo data however remains inconclusive reliable determination pm excitation', 'study present combined series gracegrace fobased gravimetric excitation pm computed using three corneredhat tch method wherein internal noise level combined solution reduced minimum', 'compare combined series result obtained combined gracegrace fo solution provided cost g international combination service time variable gravity fields single solution elaborated center space research csr', 'gravimetric excitation series evaluated comparison sum hydrological cryospheric signal geodetically observed pm excitation called gao', 'result show minimizing internal noise level combined excitation series using tch method receive higher consistency gao case cost g csr solution especially non seasonal oscillation', 'spectral band obtained correlation gao best combined series high zerosixtyfive zeroseventytwo Ï‡1 Ï‡2 equatorial component pm excitation respectively', 'corresponding value seasonal oscillation zeroninetyone Ï‡1 zeroeightynine Ï‡2', 'combined series developed study explain sixtyeight sixty overall gao variability Ï‡1 Ï‡2 respectively'], ['correlations emg structure movement patterns activity postural muscles able bodied wheelchair fencers study involved paralympic wheelchair fencer n seven two disability category able bodied female epee fencer n seven member polish paralympic fencing team', 'performance postural muscle sword arm muscle group fencer front rear leg muscle able bodied fencer examined using surface electromyography accelerometer optitrack motion analysis system well ground force reaction platform', 'activation sequence individual muscle determined structure movement pattern able bodied wheelchair fencer formulated', 'statistically significant correlation found complex motor reaction time latissimus dorsi muscle activation p zerothirtynine z twosixtytwo wheelchair fencer', 'high correlation vertical force emg signal value gastrocnemius caput laterale muscle zeroeightyfive p zerotwentytwo found able bodied fencer', 'heuristic analysis indicated significance postural muscle movement pattern wheelchair able bodied fencer', 'muscle play crucial role anticipatory postural adjustment trunk technical fencing action including attack opponent s body'], ['effects inter intra specific interactions moose habitat selection limited temperature habitat selection daily activity pattern large herbivore might affected inter intra specific interaction change spatial scale seasonal temperature', 'reveal factor driving habitat selection moose collected moose alces alces roe deer capreolus pygargus bedfordi occurrence data analyzed multi scale habitat selection daily activity pattern moose quantified effect spatial heterogeneity distribution temperature well occurrence roe deer habitat selection process', 'result suggested moose roe deer distribution spatially overlap moose habitat selection especially sensitive landscape variable large scale', 'also found activity pattern sex moose degree temporal separation roe deer', 'snow free season temperature drove moose habitat selection limited threshold temperature seventeen c snowy season similar temperature driving pattern due severe cold environment', 'daily activity pattern moose showed seasonal change active dawn nightfall avoid heat pressure snow free season active daytime cold adaptation snow season', 'consequently study provides new insight comprehensive effect environmental change inter intra specific relationship influence habitat selection daily activity pattern moose heat sensitive animal global warming'], ['capability exploration extended state observer based control uncertain case disturbance actuator saturation considering class nonlinear system subject actuator saturation uncertain control input article explored control capability extended state observer based active disturbance rejection controller presence disturbance uncertainty', 'first actuator saturation handled convex hull extended state observer based control scheme ellipsoid invariant set inside domain attraction control system obtained using lyapunov method', 'second enlargement problem invariant set also studied illustrates convex hull ellipsoid invariant set also enlargement invariant set', 'finally comprehensive comparison existing method brushless dc motor demonstrate stronger capability designed controller disturbance rejection actuator saturation', 'result show disturbance offset performance index integrated time absolute error integral absolute error half classical proportionalintegralderivative pid double integral sliding mode control disturbance rejection capability twice case'], ['sequential frame interpolation dct based video compression framework video data ubiquitous capturing transferring storing even compressed video data challenging requires substantial resource', 'large amount video traffic transmitted internet improvement compressing data even small drastically impact resource consumption', 'paper present hybrid video compression framework unites advantage dct based interpolation based video compression method single framework', 'show work deliver visual quality or case improve visual quality reducing bandwidth ten twenty'], ['detecting continuous integration skip commits using multi objective evolutionary search continuous integration ci consists integrating change introduced different developer frequently automation build process', 'nevertheless ci build process seen major barrier cause delay product release date', 'one main reason delay simple change ie skipped trigger build represents unnecessary overhead particularly painful large project', 'order cut expense ci build time propose paper skipci novel search based approach automatically detect ci skip commits based adaptation strength pareto evolutionary algorithm spea two', 'approach aim provide optimal trade off two conflicting objective deal skipped non skipped commits', 'evaluate approach investigate performance within cross project validation benchmark fourteen two hundred and ninetyfour ci commits fifteen project use travis ci system', 'statistical test revealed approach show clear advantage baseline approach average score ninetytwo eightyfour term auc cross validation cross project validation respectively', 'furthermore feature analysis reveals documentation change term appearing commit message committer experience prominent feature ci skip detection', 'come cross project scenario result reveal besides documentation change strong link current previous commits result', 'moreover deployed evaluated usefulness skipci industrial partner', 'qualitative result demonstrate effectiveness skipci providing relevant ci skip commit recommendation developer two large software project practitioner s point view'], ['heimdallr fingerprinting sd wan control plane architecture via encrypted control traffic software defined wide area network sd wan emerged new paradigm steering large scale network flexibly adopting distributed software defined network sdn controller', 'key building logically centralized physically distributed control plane running diverse cluster management protocol achieve consistency exchange control traffic', 'meanwhile observe control traffic expose unique time series pattern directional relationship due operational structure even though traffic encrypted pattern disclose confidential information control plane topology protocol dependency exploited severe attack', 'insight propose new sd wan fingerprinting system called heimdallr', 'analyzes periodical operational pattern sd wan cluster management protocol context flow direction collected control traffic utilizing deep learning based approach classify cluster management protocol automatically miscellaneous control traffic datasets', 'evaluation performed realistic sd wan environment consisting geographically distant three campus network one enterprise network show heimdallr classify sd wan control traffic ninetythree identify individual protocol eighty macro f one score finally infer control plane topology seventy similarity'], ['question answering stack applying string similarity present method evaluate fill inthe blank student answer stack using string metric possible current version stack', 'increase quality evaluation use whitelist blacklist instead single teacher answer', 'performance stack question equipped string metric quantitatively demonstrated evaluating use mathematics course'], ['regulation tracking control omnidirectional rotation spherical motors spherical motor capable omnidirectional rotation ball joint provide highly dexterous actuator system yet also present great challenge developing high performance regulation tracking controller due complex rotor dynamic', 'article present complementary italic h italic sub two sub italic h sub sub italic italic c italic italic h italic sub two sub italic h sub sub italic control method precisely controlling multidegree freedom dof orientation spherical motor presence external disturbance well model uncertainty inaccuracy', 'order deal tradeoff robustness control performance resulted conventional control method proposed controller featured two dof structure present italic h italic sub two sub controller nominal plant complement additional regulator designed italic h sub sub italic sense assure robustness', 'unlike traditional italic h sub sub italic mixed italic h italic sub two sub and italic h sub sub italic controller italic h sub sub italic regulation conducted online accordance monitored modeling mismatch way adversely degrade performance delivered italic h italic sub two sub control hence improving conservativeness total control performance leading significant improvement tracking accuracy response time time', 'numerical simulation experiment spherical motor testbed conducted validate superior performance proposed controller versus conventional control method'], ['investigating effects synchronized visuo tactile stimuli inducing kinesthetic illusion observational learning whole body movements skill improvement sport essential factor keeping motivation continue', 'previous study showed kinesthetic illusion enhances observational learning', 'however study dealt learning movement using single part body whether kinesthetic illusion induced observational learning whole body movement clarified', 'study conducted experiment involving human subject confirmed synchronized visuo tactile stimulus induce kinesthetic illusion even whole body movement', 'moreover also demonstrated complete mediation model synchronization visuo tactile stimulus influence kinesthetic illusion mediated body ownership'], ['imu smartphone camera fusion knee adduction knee flexion moment estimation walking wearable sensing computer vision could move biomechanics specialized laboratory natural environment better algorithm needed extract meaningful outcome emerging modality', 'article present new model estimating biomechanical outcomesthe knee adduction moment kam knee flexion moment kfm from fusion smartphone camera wearable inertial measurement unit imus among young healthy nonobese male', 'deep learning model developed extract feature fuse multimodal data estimate kam kfm', 'walking data seventeen subject recorded eight imus two smartphone camera', 'model used imu camera fusion significantly accurate using imus camera alone', 'root meansquare error fusion model zerofortynine inline formula tex math notation latex mathbf bw cdot mathbf bh tex math inline formula kam zerosixtysix inline formula tex math notation latex mathbf bw cdot mathbf bh tex math inline formula kfm estimation lower clinically significant threshold', 'larger diverse data model could enable assessment knee moment clinic home'], ['mfra max flowbased routing future interplanetary networks artificial satellite space station lander rover continuously deployed deep space explore planet potential resource solar system', 'data transmission deep space therefore prescheduled timebandwidth specific communication exists now', 'number source deep space may simultaneously transmit vast amount sensitive data earth station destination using limited bandwidth multiple hop', 'article using brute force approach first show computing maximum flow node deep space network superexponential nature', 'evolve number pruning technique reduce search space complex augmented deep space network trivial case maximum flow and corresponding routing may easily derived', 'case possible reduce heuristic developed find good solution maximizing data flow', 'finally give comparative simulation study analysis proposed technique standard contact graph routing cgr protocol', 'algorithm outperforms cgr significant margin tested different network topology various traffic generation rate source'], ['embodied virtual interactions equity mean you', 'preliminary results impact transgender avatar embodiment empathy embodied virtual interaction contribute immersive perspective taking experience turn increase empathy affiliation', 'study sought investigate outcome context unconscious bias related gender identity interpersonal interaction', 'conducted simulated interview virtual reality participant embodied transgender cisgender avatar interacted human controlled agent transgender woman', 'preliminary result reveal difference woman men experience empathy emotional state embodied transgender avatar'], ['eco driving scientometric bibliometric analysis fuel efficiency transportation sector become key factor reduce greenhouse gas emission fuel consumption response negative impact global warming', 'approach energy saving environmental sustainability eco driving attracted considerable research interest past decade', 'review aim provide comprehensive review research eco driving using methodology literature bibliometrics content analysis vosviewer software', 'following keywords ecological driving ecological routing ecological bus ecological car ecological vehicle eco driving eco routing eco driver eco bus eco car eco vehicle used paper retrieval', 'query conducted january twenty two thousand and twentyone', 'result take account journal article proceeding paper review without time limitation', 'finally total seven hundred and sixtyseven document retrieved total publication viewed period two thousand and onetwo thousand and twenty based web science wos core collection database', 'publication year leading country leading source leading institution leading author document citation document co citation analyzed explore primary trend', 'in depth analysis reveals five cluster keywords review relevant study eco driving five different perspective carried identify potential trend future research hot spot eco driving'], ['variational bayesian gaussian mixture nonnegative matrix factorization model extract movement primitives robust control nonnegative matrix factorization nmf powerful tool parameter estimation applied numerous robotics application path planning motion trajectory prediction motion intention detection', 'particular nmf successfully used extract simplified organized movement primitive myoelectric signal mes robust control multi degree freedom humanoid robot', 'however mes typically contaminated complex noise source', 'system performance often degrades due simplified gaussian assumption noise distribution existing nmf method', 'furthermore existing nmf model unable automatically determine rank latent matrix', 'address issue article present hybrid variational bayesian gaussian mixture nmf gmnmf model finite gaussian mixture model adopted fit mixed noise density function mes', 'addition automatic relevant determination criterion applied automatically infer number movement primitive', 'coordinate descent update rule proposed model formulated mean field variational bayesian inference', 'ass model performance five synthetic noise distribution function experimental mes dataset perform six wrist movement', 'result demonstrate gmnmf yield low error high robustness extracting movement primitive four competitive method robust cybernetic control'], ['fast actuation mechanism energy effective driving method pulse closers recloser detects fault current electric power distribution line break line restores line several fault test', 'check whether fault remains recloser performs co operation opening contact reclosing while', 'operation fault current flow line potentially causing damage equipment connected line', 'therefore reducing current carrying time fault test desirable reduction difficult slow actuation mechanism reclosers', 'article novel actuation mechanism combining permanent magnetic actuator pma solenoid actuator sa proposed', 'co operation fault test proposed mechanism rapidly break closed contact hitting moving contact sa', 'increase energy efficiency actuation mechanism distinctive pma driving method also proposed weakens strong holding force pma using assistive magneto motive force mmf', 'structure configuration proposed mechanism explained', 'performance mechanism effectiveness driving method studied cosimulations', 'finally prototype developed practicality verified experiment']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate and print the total number of sentences in the 'sentences' list.\n",
        "sentence_counts = 0\n",
        "\n",
        "for inner_list in sentences:\n",
        "    sentence_count = len(inner_list)\n",
        "    sentence_counts += sentence_count\n",
        "\n",
        "print(sentence_counts)"
      ],
      "metadata": {
        "id": "nUsOq6QrnGY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0615ff1e-a230-4b54-f0bf-32f524b9983a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract information about each sentence, including its position in nested lists and word count, and store the results in 'sent_info'.\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def sentence_information(sentences):\n",
        "    sent_info = []\n",
        "    for outer_idx, inner_sentences in enumerate(sentences):\n",
        "        for inner_idx, sent in enumerate(inner_sentences):\n",
        "            words = word_tokenize(sent)\n",
        "\n",
        "            sent_dict = {\"outer_idx\": outer_idx, \"inner_idx\": inner_idx, \"sent_word_count\": len(words)}\n",
        "            sent_info.append(sent_dict)\n",
        "\n",
        "    return sent_info\n",
        "\n",
        "sent_info=sentence_information(sentences)\n",
        "print(sent_info)"
      ],
      "metadata": {
        "id": "0P7IpFXxirH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6adf64-4b61-4b5a-90ab-1a59bfbfadf2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'outer_idx': 0, 'inner_idx': 0, 'sent_word_count': 24}, {'outer_idx': 0, 'inner_idx': 1, 'sent_word_count': 22}, {'outer_idx': 0, 'inner_idx': 2, 'sent_word_count': 5}, {'outer_idx': 0, 'inner_idx': 3, 'sent_word_count': 12}, {'outer_idx': 0, 'inner_idx': 4, 'sent_word_count': 15}, {'outer_idx': 0, 'inner_idx': 5, 'sent_word_count': 12}, {'outer_idx': 0, 'inner_idx': 6, 'sent_word_count': 18}, {'outer_idx': 1, 'inner_idx': 0, 'sent_word_count': 31}, {'outer_idx': 1, 'inner_idx': 1, 'sent_word_count': 16}, {'outer_idx': 1, 'inner_idx': 2, 'sent_word_count': 6}, {'outer_idx': 1, 'inner_idx': 3, 'sent_word_count': 12}, {'outer_idx': 1, 'inner_idx': 4, 'sent_word_count': 15}, {'outer_idx': 1, 'inner_idx': 5, 'sent_word_count': 7}, {'outer_idx': 1, 'inner_idx': 6, 'sent_word_count': 13}, {'outer_idx': 1, 'inner_idx': 7, 'sent_word_count': 7}, {'outer_idx': 1, 'inner_idx': 8, 'sent_word_count': 13}, {'outer_idx': 1, 'inner_idx': 9, 'sent_word_count': 17}, {'outer_idx': 1, 'inner_idx': 10, 'sent_word_count': 6}, {'outer_idx': 1, 'inner_idx': 11, 'sent_word_count': 15}, {'outer_idx': 2, 'inner_idx': 0, 'sent_word_count': 31}, {'outer_idx': 2, 'inner_idx': 1, 'sent_word_count': 6}, {'outer_idx': 2, 'inner_idx': 2, 'sent_word_count': 13}, {'outer_idx': 2, 'inner_idx': 3, 'sent_word_count': 17}, {'outer_idx': 2, 'inner_idx': 4, 'sent_word_count': 9}, {'outer_idx': 2, 'inner_idx': 5, 'sent_word_count': 12}, {'outer_idx': 2, 'inner_idx': 6, 'sent_word_count': 7}, {'outer_idx': 3, 'inner_idx': 0, 'sent_word_count': 16}, {'outer_idx': 3, 'inner_idx': 1, 'sent_word_count': 34}, {'outer_idx': 3, 'inner_idx': 2, 'sent_word_count': 32}, {'outer_idx': 3, 'inner_idx': 3, 'sent_word_count': 13}, {'outer_idx': 3, 'inner_idx': 4, 'sent_word_count': 15}, {'outer_idx': 3, 'inner_idx': 5, 'sent_word_count': 23}, {'outer_idx': 3, 'inner_idx': 6, 'sent_word_count': 17}, {'outer_idx': 4, 'inner_idx': 0, 'sent_word_count': 21}, {'outer_idx': 4, 'inner_idx': 1, 'sent_word_count': 16}, {'outer_idx': 4, 'inner_idx': 2, 'sent_word_count': 18}, {'outer_idx': 4, 'inner_idx': 3, 'sent_word_count': 16}, {'outer_idx': 4, 'inner_idx': 4, 'sent_word_count': 5}, {'outer_idx': 5, 'inner_idx': 0, 'sent_word_count': 31}, {'outer_idx': 5, 'inner_idx': 1, 'sent_word_count': 16}, {'outer_idx': 5, 'inner_idx': 2, 'sent_word_count': 21}, {'outer_idx': 5, 'inner_idx': 3, 'sent_word_count': 9}, {'outer_idx': 5, 'inner_idx': 4, 'sent_word_count': 9}, {'outer_idx': 5, 'inner_idx': 5, 'sent_word_count': 17}, {'outer_idx': 5, 'inner_idx': 6, 'sent_word_count': 10}, {'outer_idx': 5, 'inner_idx': 7, 'sent_word_count': 12}, {'outer_idx': 6, 'inner_idx': 0, 'sent_word_count': 35}, {'outer_idx': 6, 'inner_idx': 1, 'sent_word_count': 19}, {'outer_idx': 6, 'inner_idx': 2, 'sent_word_count': 11}, {'outer_idx': 6, 'inner_idx': 3, 'sent_word_count': 22}, {'outer_idx': 6, 'inner_idx': 4, 'sent_word_count': 19}, {'outer_idx': 6, 'inner_idx': 5, 'sent_word_count': 19}, {'outer_idx': 6, 'inner_idx': 6, 'sent_word_count': 15}, {'outer_idx': 6, 'inner_idx': 7, 'sent_word_count': 17}, {'outer_idx': 6, 'inner_idx': 8, 'sent_word_count': 10}, {'outer_idx': 7, 'inner_idx': 0, 'sent_word_count': 24}, {'outer_idx': 7, 'inner_idx': 1, 'sent_word_count': 15}, {'outer_idx': 7, 'inner_idx': 2, 'sent_word_count': 7}, {'outer_idx': 7, 'inner_idx': 3, 'sent_word_count': 23}, {'outer_idx': 7, 'inner_idx': 4, 'sent_word_count': 11}, {'outer_idx': 7, 'inner_idx': 5, 'sent_word_count': 20}, {'outer_idx': 7, 'inner_idx': 6, 'sent_word_count': 10}, {'outer_idx': 7, 'inner_idx': 7, 'sent_word_count': 13}, {'outer_idx': 8, 'inner_idx': 0, 'sent_word_count': 21}, {'outer_idx': 8, 'inner_idx': 1, 'sent_word_count': 11}, {'outer_idx': 8, 'inner_idx': 2, 'sent_word_count': 14}, {'outer_idx': 8, 'inner_idx': 3, 'sent_word_count': 10}, {'outer_idx': 8, 'inner_idx': 4, 'sent_word_count': 52}, {'outer_idx': 8, 'inner_idx': 5, 'sent_word_count': 39}, {'outer_idx': 8, 'inner_idx': 6, 'sent_word_count': 44}, {'outer_idx': 9, 'inner_idx': 0, 'sent_word_count': 27}, {'outer_idx': 9, 'inner_idx': 1, 'sent_word_count': 16}, {'outer_idx': 9, 'inner_idx': 2, 'sent_word_count': 23}, {'outer_idx': 9, 'inner_idx': 3, 'sent_word_count': 11}, {'outer_idx': 9, 'inner_idx': 4, 'sent_word_count': 8}, {'outer_idx': 10, 'inner_idx': 0, 'sent_word_count': 23}, {'outer_idx': 10, 'inner_idx': 1, 'sent_word_count': 17}, {'outer_idx': 10, 'inner_idx': 2, 'sent_word_count': 16}, {'outer_idx': 10, 'inner_idx': 3, 'sent_word_count': 12}, {'outer_idx': 10, 'inner_idx': 4, 'sent_word_count': 11}, {'outer_idx': 10, 'inner_idx': 5, 'sent_word_count': 8}, {'outer_idx': 10, 'inner_idx': 6, 'sent_word_count': 28}, {'outer_idx': 11, 'inner_idx': 0, 'sent_word_count': 29}, {'outer_idx': 11, 'inner_idx': 1, 'sent_word_count': 17}, {'outer_idx': 11, 'inner_idx': 2, 'sent_word_count': 13}, {'outer_idx': 11, 'inner_idx': 3, 'sent_word_count': 17}, {'outer_idx': 11, 'inner_idx': 4, 'sent_word_count': 24}, {'outer_idx': 11, 'inner_idx': 5, 'sent_word_count': 13}, {'outer_idx': 11, 'inner_idx': 6, 'sent_word_count': 21}, {'outer_idx': 11, 'inner_idx': 7, 'sent_word_count': 11}, {'outer_idx': 12, 'inner_idx': 0, 'sent_word_count': 20}, {'outer_idx': 12, 'inner_idx': 1, 'sent_word_count': 13}, {'outer_idx': 12, 'inner_idx': 2, 'sent_word_count': 14}, {'outer_idx': 12, 'inner_idx': 3, 'sent_word_count': 9}, {'outer_idx': 12, 'inner_idx': 4, 'sent_word_count': 8}, {'outer_idx': 13, 'inner_idx': 0, 'sent_word_count': 23}, {'outer_idx': 13, 'inner_idx': 1, 'sent_word_count': 15}, {'outer_idx': 13, 'inner_idx': 2, 'sent_word_count': 17}, {'outer_idx': 13, 'inner_idx': 3, 'sent_word_count': 10}, {'outer_idx': 13, 'inner_idx': 4, 'sent_word_count': 22}, {'outer_idx': 13, 'inner_idx': 5, 'sent_word_count': 10}, {'outer_idx': 13, 'inner_idx': 6, 'sent_word_count': 32}, {'outer_idx': 13, 'inner_idx': 7, 'sent_word_count': 10}, {'outer_idx': 13, 'inner_idx': 8, 'sent_word_count': 18}, {'outer_idx': 14, 'inner_idx': 0, 'sent_word_count': 38}, {'outer_idx': 14, 'inner_idx': 1, 'sent_word_count': 22}, {'outer_idx': 14, 'inner_idx': 2, 'sent_word_count': 23}, {'outer_idx': 14, 'inner_idx': 3, 'sent_word_count': 26}, {'outer_idx': 14, 'inner_idx': 4, 'sent_word_count': 15}, {'outer_idx': 14, 'inner_idx': 5, 'sent_word_count': 25}, {'outer_idx': 14, 'inner_idx': 6, 'sent_word_count': 18}, {'outer_idx': 14, 'inner_idx': 7, 'sent_word_count': 8}, {'outer_idx': 14, 'inner_idx': 8, 'sent_word_count': 13}, {'outer_idx': 15, 'inner_idx': 0, 'sent_word_count': 34}, {'outer_idx': 15, 'inner_idx': 1, 'sent_word_count': 29}, {'outer_idx': 15, 'inner_idx': 2, 'sent_word_count': 13}, {'outer_idx': 15, 'inner_idx': 3, 'sent_word_count': 18}, {'outer_idx': 15, 'inner_idx': 4, 'sent_word_count': 18}, {'outer_idx': 15, 'inner_idx': 5, 'sent_word_count': 12}, {'outer_idx': 15, 'inner_idx': 6, 'sent_word_count': 16}, {'outer_idx': 16, 'inner_idx': 0, 'sent_word_count': 28}, {'outer_idx': 16, 'inner_idx': 1, 'sent_word_count': 39}, {'outer_idx': 16, 'inner_idx': 2, 'sent_word_count': 17}, {'outer_idx': 16, 'inner_idx': 3, 'sent_word_count': 11}, {'outer_idx': 16, 'inner_idx': 4, 'sent_word_count': 23}, {'outer_idx': 16, 'inner_idx': 5, 'sent_word_count': 22}, {'outer_idx': 16, 'inner_idx': 6, 'sent_word_count': 25}, {'outer_idx': 17, 'inner_idx': 0, 'sent_word_count': 37}, {'outer_idx': 17, 'inner_idx': 1, 'sent_word_count': 24}, {'outer_idx': 17, 'inner_idx': 2, 'sent_word_count': 17}, {'outer_idx': 17, 'inner_idx': 3, 'sent_word_count': 17}, {'outer_idx': 17, 'inner_idx': 4, 'sent_word_count': 27}, {'outer_idx': 18, 'inner_idx': 0, 'sent_word_count': 22}, {'outer_idx': 18, 'inner_idx': 1, 'sent_word_count': 15}, {'outer_idx': 18, 'inner_idx': 2, 'sent_word_count': 17}, {'outer_idx': 18, 'inner_idx': 3, 'sent_word_count': 14}, {'outer_idx': 19, 'inner_idx': 0, 'sent_word_count': 23}, {'outer_idx': 19, 'inner_idx': 1, 'sent_word_count': 12}, {'outer_idx': 19, 'inner_idx': 2, 'sent_word_count': 17}, {'outer_idx': 19, 'inner_idx': 3, 'sent_word_count': 26}, {'outer_idx': 19, 'inner_idx': 4, 'sent_word_count': 14}, {'outer_idx': 19, 'inner_idx': 5, 'sent_word_count': 22}, {'outer_idx': 19, 'inner_idx': 6, 'sent_word_count': 21}, {'outer_idx': 19, 'inner_idx': 7, 'sent_word_count': 17}, {'outer_idx': 19, 'inner_idx': 8, 'sent_word_count': 15}, {'outer_idx': 19, 'inner_idx': 9, 'sent_word_count': 7}, {'outer_idx': 19, 'inner_idx': 10, 'sent_word_count': 20}, {'outer_idx': 20, 'inner_idx': 0, 'sent_word_count': 33}, {'outer_idx': 20, 'inner_idx': 1, 'sent_word_count': 18}, {'outer_idx': 20, 'inner_idx': 2, 'sent_word_count': 30}, {'outer_idx': 20, 'inner_idx': 3, 'sent_word_count': 9}, {'outer_idx': 20, 'inner_idx': 4, 'sent_word_count': 29}, {'outer_idx': 20, 'inner_idx': 5, 'sent_word_count': 38}, {'outer_idx': 21, 'inner_idx': 0, 'sent_word_count': 22}, {'outer_idx': 21, 'inner_idx': 1, 'sent_word_count': 10}, {'outer_idx': 21, 'inner_idx': 2, 'sent_word_count': 12}, {'outer_idx': 22, 'inner_idx': 0, 'sent_word_count': 34}, {'outer_idx': 22, 'inner_idx': 1, 'sent_word_count': 45}, {'outer_idx': 22, 'inner_idx': 2, 'sent_word_count': 38}, {'outer_idx': 22, 'inner_idx': 3, 'sent_word_count': 59}, {'outer_idx': 22, 'inner_idx': 4, 'sent_word_count': 16}, {'outer_idx': 23, 'inner_idx': 0, 'sent_word_count': 22}, {'outer_idx': 23, 'inner_idx': 1, 'sent_word_count': 8}, {'outer_idx': 23, 'inner_idx': 2, 'sent_word_count': 19}, {'outer_idx': 23, 'inner_idx': 3, 'sent_word_count': 18}, {'outer_idx': 23, 'inner_idx': 4, 'sent_word_count': 16}, {'outer_idx': 24, 'inner_idx': 0, 'sent_word_count': 30}, {'outer_idx': 24, 'inner_idx': 1, 'sent_word_count': 29}, {'outer_idx': 24, 'inner_idx': 2, 'sent_word_count': 12}, {'outer_idx': 24, 'inner_idx': 3, 'sent_word_count': 10}, {'outer_idx': 24, 'inner_idx': 4, 'sent_word_count': 11}, {'outer_idx': 24, 'inner_idx': 5, 'sent_word_count': 44}, {'outer_idx': 24, 'inner_idx': 6, 'sent_word_count': 11}, {'outer_idx': 25, 'inner_idx': 0, 'sent_word_count': 23}, {'outer_idx': 25, 'inner_idx': 1, 'sent_word_count': 11}, {'outer_idx': 25, 'inner_idx': 2, 'sent_word_count': 19}, {'outer_idx': 25, 'inner_idx': 3, 'sent_word_count': 16}, {'outer_idx': 25, 'inner_idx': 4, 'sent_word_count': 22}, {'outer_idx': 25, 'inner_idx': 5, 'sent_word_count': 11}, {'outer_idx': 25, 'inner_idx': 6, 'sent_word_count': 14}, {'outer_idx': 25, 'inner_idx': 7, 'sent_word_count': 14}, {'outer_idx': 26, 'inner_idx': 0, 'sent_word_count': 6}, {'outer_idx': 26, 'inner_idx': 1, 'sent_word_count': 19}, {'outer_idx': 26, 'inner_idx': 2, 'sent_word_count': 12}, {'outer_idx': 26, 'inner_idx': 3, 'sent_word_count': 16}, {'outer_idx': 26, 'inner_idx': 4, 'sent_word_count': 13}, {'outer_idx': 27, 'inner_idx': 0, 'sent_word_count': 23}, {'outer_idx': 27, 'inner_idx': 1, 'sent_word_count': 13}, {'outer_idx': 27, 'inner_idx': 2, 'sent_word_count': 16}, {'outer_idx': 27, 'inner_idx': 3, 'sent_word_count': 27}, {'outer_idx': 27, 'inner_idx': 4, 'sent_word_count': 8}, {'outer_idx': 27, 'inner_idx': 5, 'sent_word_count': 11}, {'outer_idx': 27, 'inner_idx': 6, 'sent_word_count': 26}, {'outer_idx': 27, 'inner_idx': 7, 'sent_word_count': 19}, {'outer_idx': 27, 'inner_idx': 8, 'sent_word_count': 25}, {'outer_idx': 28, 'inner_idx': 0, 'sent_word_count': 33}, {'outer_idx': 28, 'inner_idx': 1, 'sent_word_count': 19}, {'outer_idx': 28, 'inner_idx': 2, 'sent_word_count': 7}, {'outer_idx': 28, 'inner_idx': 3, 'sent_word_count': 13}, {'outer_idx': 28, 'inner_idx': 4, 'sent_word_count': 10}, {'outer_idx': 28, 'inner_idx': 5, 'sent_word_count': 23}, {'outer_idx': 28, 'inner_idx': 6, 'sent_word_count': 11}, {'outer_idx': 28, 'inner_idx': 7, 'sent_word_count': 12}, {'outer_idx': 28, 'inner_idx': 8, 'sent_word_count': 15}, {'outer_idx': 28, 'inner_idx': 9, 'sent_word_count': 17}, {'outer_idx': 29, 'inner_idx': 0, 'sent_word_count': 24}, {'outer_idx': 29, 'inner_idx': 1, 'sent_word_count': 12}, {'outer_idx': 29, 'inner_idx': 2, 'sent_word_count': 11}, {'outer_idx': 29, 'inner_idx': 3, 'sent_word_count': 14}, {'outer_idx': 29, 'inner_idx': 4, 'sent_word_count': 13}, {'outer_idx': 29, 'inner_idx': 5, 'sent_word_count': 14}, {'outer_idx': 29, 'inner_idx': 6, 'sent_word_count': 22}, {'outer_idx': 29, 'inner_idx': 7, 'sent_word_count': 5}, {'outer_idx': 29, 'inner_idx': 8, 'sent_word_count': 7}, {'outer_idx': 29, 'inner_idx': 9, 'sent_word_count': 6}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the frequency of words in each sentence and store the results in 'freq_info'. Then, print 'freq_info'.\n",
        "def frequency_of_words(sentences):\n",
        "    freq_info = []\n",
        "    for outer_idx, inner_sentences in enumerate(sentences):\n",
        "        for inner_idx, sent in enumerate(inner_sentences):\n",
        "            words = word_tokenize(sent)\n",
        "            frequency_dict = {}\n",
        "            for word in words:\n",
        "                if word in frequency_dict.keys():\n",
        "                    frequency_dict[word] = frequency_dict[word] + 1\n",
        "                else:\n",
        "                    frequency_dict[word] = 1\n",
        "            temp = {\"outer_idx\": outer_idx, \"inner_idx\": inner_idx, \"frequency_of_word\": frequency_dict}\n",
        "            freq_info.append(temp)\n",
        "\n",
        "    return freq_info\n",
        "\n",
        "freq_info = frequency_of_words(sentences)\n",
        "print(freq_info)"
      ],
      "metadata": {
        "id": "8meFfPcKkOjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875636ff-35a6-41cb-a56d-8d2293ea5930"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'outer_idx': 0, 'inner_idx': 0, 'frequency_of_word': {'complete': 1, 'targets': 1, 'coverage': 1, 'energy': 1, 'harvesting': 1, 'internet': 2, 'things': 2, 'ambient': 1, 'backscatter': 1, 'article': 1, 'considers': 1, 'deriving': 1, 'set': 2, 'cover': 1, 'active': 1, 'node': 1, 'responsible': 1, 'monitoring': 1, 'target': 1, 'iot': 1, 'network': 1}}, {'outer_idx': 0, 'inner_idx': 1, 'frequency_of_word': {'key': 1, 'distinction': 1, 'prior': 1, 'work': 1, 'article': 1, 'considers': 1, 'sensor': 1, 'node': 1, 'aided': 1, 'backscatter': 1, 'communication': 1, 'allow': 1, 'communicate': 1, 'negligible': 1, 'energy': 1, 'cost': 1, 'using': 1, 'ambient': 1, 'radio': 1, 'frequency': 1, 'rf': 1, 'signal': 1}}, {'outer_idx': 0, 'inner_idx': 2, 'frequency_of_word': {'article': 1, 'contains': 1, 'three': 1, 'main': 1, 'novelty': 1}}, {'outer_idx': 0, 'inner_idx': 3, 'frequency_of_word': {'first': 1, 'present': 1, 'mixed': 1, 'integer': 1, 'linear': 1, 'program': 1, 'milp': 1, 'used': 1, 'compute': 1, 'global': 1, 'optimal': 1, 'solution': 1}}, {'outer_idx': 0, 'inner_idx': 4, 'frequency_of_word': {'second': 1, 'also': 1, 'outline': 1, 'centralized': 1, 'greedy': 1, 'scheduling': 1, 'cgs': 1, 'algorithm': 1, 'selects': 1, 'node': 1, 'based': 1, 'energy': 1, 'number': 1, 'covered': 1, 'target': 1}}, {'outer_idx': 0, 'inner_idx': 5, 'frequency_of_word': {'third': 1, 'present': 1, 'distributed': 1, 'greedy': 1, 'scheduling': 1, 'dgs': 1, 'algorithm': 1, 'selects': 1, 'node': 1, 'according': 1, 'energy': 1, 'level': 1}}, {'outer_idx': 0, 'inner_idx': 6, 'frequency_of_word': {'simulation': 1, 'result': 1, 'show': 1, 'equipping': 1, 'node': 1, 'ambient': 1, 'backscattering': 1, 'capability': 1, 'increase': 1, 'ratio': 1, 'complete': 1, 'target': 1, 'coverage': 1, 'one': 1, 'hundred': 1, 'compared': 1, 'existing': 1, 'technique': 1}}, {'outer_idx': 1, 'inner_idx': 0, 'frequency_of_word': {'nonorthogonal': 2, 'multiple': 2, 'access': 2, 'enabled': 1, 'two': 2, 'way': 2, 'relay': 2, 'system': 2, 'using': 1, 'signal': 1, 'alignment': 1, 'conventional': 1, 'twr': 1, 'noma': 1, 'diversity': 1, 'order': 1, 'equal': 1, 'zero': 1, 'even': 1, 'perfect': 1, 'successive': 1, 'interference': 1, 'cancellation': 1, 'psic': 1}}, {'outer_idx': 1, 'inner_idx': 1, 'frequency_of_word': {'article': 1, 'leverage': 1, 'multiple': 1, 'inputmultiple': 1, 'output': 1, 'mimo': 1, 'technique': 1, 'twr': 1, 'noma': 1, 'system': 1, 'extract': 1, 'spatial': 1, 'diversity': 1, 'gain': 1, 'performance': 1, 'enhancement': 1}}, {'outer_idx': 1, 'inner_idx': 2, 'frequency_of_word': {'communications': 1, 'two': 1, 'user': 1, 'pair': 1, 'assisted': 1, 'relay': 1}}, {'outer_idx': 1, 'inner_idx': 3, 'frequency_of_word': {'precoding': 1, 'vector': 1, 'user': 1, 'well': 1, 'matrix': 1, 'relay': 1, 'specially': 1, 'designed': 1, 'perform': 1, 'signal': 1, 'alignment': 1, 'sa': 1}}, {'outer_idx': 1, 'inner_idx': 4, 'frequency_of_word': {'derive': 1, 'analytical': 1, 'closed': 1, 'form': 1, 'expression': 1, 'outage': 1, 'probability': 1, 'diversity': 1, 'order': 1, 'psic': 1, 'imperfect': 1, 'successive': 1, 'interference': 1, 'cancellation': 1, 'ipsic': 1}}, {'outer_idx': 1, 'inner_idx': 5, 'frequency_of_word': {'addition': 1, 'ergodic': 2, 'rate': 2, 'sum': 1, 'analyzed': 1}}, {'outer_idx': 1, 'inner_idx': 6, 'frequency_of_word': {'optimal': 1, 'power': 1, 'allocation': 1, 'system': 1, 'outage': 1, 'probability': 1, 'ergodic': 1, 'sum': 1, 'rate': 1, 'analyzed': 1, 'based': 1, 'quality': 1, 'service': 1}}, {'outer_idx': 1, 'inner_idx': 7, 'frequency_of_word': {'simulation': 1, 'result': 2, 'provided': 1, 'confirm': 1, 'derived': 1, 'analytical': 1}}, {'outer_idx': 1, 'inner_idx': 8, 'frequency_of_word': {'result': 1, 'show': 1, 'combination': 1, 'sa': 1, 'twr': 1, 'noma': 1, 'decrease': 1, 'outage': 1, 'probability': 1, 'improve': 1, 'diversity': 1, 'order': 1, 'effectively': 1}}, {'outer_idx': 1, 'inner_idx': 9, 'frequency_of_word': {'ergodic': 2, 'rate': 2, 'finally': 1, 'arise': 1, 'ceiling': 1, 'psic': 1, 'ipsic': 1, 'sum': 1, 'keep': 1, 'increasing': 1, 'power': 1, 'allocated': 1, 'distant': 1, 'user': 1, 'increase': 1}}, {'outer_idx': 1, 'inner_idx': 10, 'frequency_of_word': {'however': 1, 'conventional': 1, 'twr': 1, 'noma': 1, 'complete': 1, 'opposite': 1}}, {'outer_idx': 1, 'inner_idx': 11, 'frequency_of_word': {'summary': 1, 'compared': 1, 'twr': 2, 'noma': 1, 'nomasa': 1, 'higher': 1, 'diversity': 1, 'order': 1, 'enables': 1, 'distant': 1, 'user': 1, 'contribute': 1, 'system': 1, 'performance': 1}}, {'outer_idx': 2, 'inner_idx': 0, 'frequency_of_word': {'event': 2, 'triggered': 2, 'adaptive': 2, 'output': 2, 'feedback': 2, 'control': 2, 'stochastic': 2, 'nonlinear': 2, 'systems': 1, 'time': 2, 'varying': 2, 'full': 2, 'state': 2, 'constraints': 1, 'brief': 1, 'investigates': 1, 'issue': 1, 'system': 1, 'constraint': 1}}, {'outer_idx': 2, 'inner_idx': 1, 'frequency_of_word': {'firstly': 1, 'unmeasurable': 1, 'state': 1, 'estimated': 1, 'fuzzy': 1, 'observer': 1}}, {'outer_idx': 2, 'inner_idx': 2, 'frequency_of_word': {'secondly': 1, 'quartic': 1, 'time': 2, 'varying': 2, 'barrier': 1, 'lyapunov': 1, 'function': 1, 'constructed': 1, 'avoid': 1, 'violation': 1, 'constraint': 1}}, {'outer_idx': 2, 'inner_idx': 3, 'frequency_of_word': {'thirdly': 1, 'command': 1, 'filter': 1, 'technique': 1, 'error': 2, 'compensation': 1, 'mechanism': 1, 'incorporated': 1, 'controller': 1, 'design': 1, 'get': 1, 'issue': 1, 'explosion': 1, 'complexity': 1, 'compensate': 1, 'filtered': 1}}, {'outer_idx': 2, 'inner_idx': 4, 'frequency_of_word': {'then': 1, 'event': 1, 'triggered': 1, 'mechanism': 1, 'introduced': 1, 'improve': 1, 'efficiency': 1, 'resource': 1, 'utilization': 1}}, {'outer_idx': 2, 'inner_idx': 5, 'frequency_of_word': {'shown': 1, 'tracking': 1, 'error': 1, 'converge': 1, 'desired': 1, 'neighborhood': 1, 'origin': 1, 'signal': 1, 'closed': 1, 'loop': 1, 'system': 1, 'bounded': 1}}, {'outer_idx': 2, 'inner_idx': 6, 'frequency_of_word': {'finally': 1, 'validity': 1, 'control': 1, 'strategy': 1, 'demonstrated': 1, 'physical': 1, 'example': 1}}, {'outer_idx': 3, 'inner_idx': 0, 'frequency_of_word': {'mmu': 1, 'mpu': 1, 'adaptation': 1, 'pip': 1, 'kernel': 1, 'constrained': 2, 'devices': 1, 'article': 1, 'present': 1, 'hardware': 1, 'based': 1, 'memory': 1, 'isolation': 1, 'solution': 1, 'device': 1}}, {'outer_idx': 3, 'inner_idx': 1, 'frequency_of_word': {'existing': 1, 'solution': 1, 'target': 2, 'high': 1, 'end': 2, 'embedded': 1, 'system': 1, 'typically': 1, 'arm': 1, 'cortex': 1, 'a': 1, 'memory': 1, 'management': 1, 'unit': 1, 'mmu': 1, 'sel4': 1, 'pip': 1, 'formally': 1, 'verified': 1, 'kernel': 1, 'low': 1, 'device': 1, 'aces': 1, 'minion': 1, 'trustlite': 1, 'ewok': 1, 'limited': 1, 'flexibility': 1, 'proposing': 1, 'single': 1, 'level': 1, 'isolation': 1}}, {'outer_idx': 3, 'inner_idx': 2, 'frequency_of_word': {'approach': 1, 'consists': 1, 'adapting': 1, 'pip': 1, 'inherit': 1, 'flexibility': 1, 'multiple': 1, 'level': 1, 'isolation': 1, 'using': 1, 'memory': 1, 'protection': 1, 'unit': 1, 'mpu': 2, 'instead': 1, 'mmu': 1, 'since': 1, 'commonly': 1, 'available': 1, 'constrained': 1, 'embedded': 1, 'system': 1, 'typically': 1, 'armv7': 1, 'cortex': 2, 'm4': 1, 'armv8': 1, 'm33': 1, 'similar': 1, 'device': 1}}, {'outer_idx': 3, 'inner_idx': 3, 'frequency_of_word': {'paper': 1, 'describes': 1, 'design': 1, 'pip': 2, 'mpu': 2, 's': 1, 'variant': 1, 'based': 1, 'rationale': 1, 'behind': 1, 'choice': 1}}, {'outer_idx': 3, 'inner_idx': 4, 'frequency_of_word': {'validate': 1, 'proposal': 1, 'implementation': 1, 'nrf52840': 1, 'development': 1, 'kit': 1, 'perform': 1, 'various': 1, 'evaluation': 1, 'memory': 1, 'footprint': 1, 'cpu': 1, 'cycle': 1, 'energy': 1, 'consumption': 1}}, {'outer_idx': 3, 'inner_idx': 5, 'frequency_of_word': {'demonstrate': 1, 'although': 1, 'prototyped': 1, 'pip': 1, 'mpu': 1, 'cause': 1, 'sixteen': 1, 'overhead': 1, 'performance': 1, 'energy': 1, 'consumption': 1, 'reduce': 1, 'attack': 1, 'surface': 1, 'accessible': 1, 'application': 1, 'memory': 1, 'one': 1, 'hundred': 1, 'two': 1, 'privileged': 1, 'operation': 1, 'ninetynine': 1}}, {'outer_idx': 3, 'inner_idx': 6, 'frequency_of_word': {'pip': 1, 'mpu': 1, 'take': 1, 'le': 1, 'ten': 1, 'kb': 2, 'flash': 1, 'six': 1, 'core': 1, 'component': 1, 'five': 1, 'hundred': 1, 'and': 1, 'fifty': 1, 'b': 1, 'ram': 1}}, {'outer_idx': 4, 'inner_idx': 0, 'frequency_of_word': {'data': 2, 'driven': 2, 'cyber': 2, 'attack': 2, 'detection': 2, 'intelligent': 1, 'attacks': 1, 'islanded': 2, 'dc': 2, 'microgrids': 2, 'letter': 1, 'method': 1, 'proposed': 1}}, {'outer_idx': 4, 'inner_idx': 1, 'frequency_of_word': {'data': 1, 'collected': 1, 'monitoring': 1, 'behavior': 1, 'intelligent': 1, 'attacker': 1, 'able': 1, 'bypass': 1, 'conventional': 1, 'cyber': 1, 'attack': 1, 'detection': 1, 'algorithm': 1, 'disrupt': 1, 'operation': 1, 'system': 1}}, {'outer_idx': 4, 'inner_idx': 2, 'frequency_of_word': {'reinforcement': 1, 'learning': 1, 'algorithm': 2, 'emulates': 1, 'action': 1, 'intelligent': 1, 'attacker': 1, 'exploit': 1, 'vulnerability': 1, 'index': 1, 'based': 1, 'cyber': 1, 'attack': 1, 'detection': 2, 'method': 1, 'discordant': 1}}, {'outer_idx': 4, 'inner_idx': 3, 'frequency_of_word': {'data': 1, 'used': 1, 'train': 1, 'neural': 1, 'networkbased': 1, 'detector': 1, 'complement': 1, 'conventional': 1, 'method': 1, 'additional': 1, 'capability': 1, 'detect': 1, 'larger': 1, 'set': 1, 'possible': 1, 'attack': 1}}, {'outer_idx': 4, 'inner_idx': 4, 'frequency_of_word': {'experiment': 1, 'effectiveness': 1, 'proposed': 1, 'method': 1, 'validated': 1}}, {'outer_idx': 5, 'inner_idx': 0, 'frequency_of_word': {'algorithm': 2, 'determining': 1, 'types': 1, 'inverse': 2, 'kinematics': 2, 'solutions': 1, 'sequential': 1, 'planar': 2, 'robots': 1, 'representation': 1, 'configuration': 1, 'space': 1, 'work': 1, 'defines': 1, 'new': 1, 'way': 1, 'different': 1, 'type': 1, 'solution': 1, 'ik': 1, 'problem': 1, 'robot': 1, 'serial': 1, 'topology': 1, 'present': 1, 'solving': 1, 'it': 1}}, {'outer_idx': 5, 'inner_idx': 1, 'frequency_of_word': {'developed': 1, 'algorithm': 1, 'allows': 1, 'finding': 1, 'solution': 1, 'wide': 1, 'range': 1, 'robot': 1, 'using': 1, 'geometric': 1, 'approach': 1, 'representing': 1, 'point': 1, 'polar': 1, 'coordinate': 1, 'system': 1}}, {'outer_idx': 5, 'inner_idx': 2, 'frequency_of_word': {'inverse': 1, 'kinematics': 1, 'one': 1, 'important': 1, 'studied': 1, 'challenging': 1, 'problem': 1, 'robotics': 1, 'aim': 1, 'calculate': 1, 'value': 1, 'joint': 1, 'variable': 1, 'given': 1, 'desired': 1, 'position': 1, 'orientation': 1, 'robot': 1, 's': 1, 'end': 1, 'effector': 1}}, {'outer_idx': 5, 'inner_idx': 3, 'frequency_of_word': {'configuration': 1, 'space': 1, 'defined': 1, 'joint': 1, 'angle': 1, 'basis': 1, 'motion': 1, 'planning': 1, 'algorithm': 1}}, {'outer_idx': 5, 'inner_idx': 4, 'frequency_of_word': {'areas': 1, 'working': 1, 'configuration': 1, 'space': 1, 'generated': 1, 'reachable': 1, 'different': 1, 'type': 1, 'solution': 1}}, {'outer_idx': 5, 'inner_idx': 5, 'frequency_of_word': {'programs': 1, 'created': 1, 'use': 1, 'proposed': 1, 'algorithm': 1, 'robot': 1, 'two': 1, 'three': 1, 'rotational': 1, 'degree': 1, 'freedom': 1, 'graphically': 1, 'present': 1, 'result': 1, 'workspace': 1, 'configuration': 1, 'space': 1}}, {'outer_idx': 5, 'inner_idx': 6, 'frequency_of_word': {'possibility': 1, 'transitioning': 1, 'one': 1, 'type': 1, 'solution': 1, 'another': 1, 'passing': 1, 'singular': 1, 'configuration': 1, 'discussed': 1}}, {'outer_idx': 5, 'inner_idx': 7, 'frequency_of_word': {'result': 1, 'important': 1, 'planning': 1, 'motion': 1, 'workspace': 1, 'configuration': 1, 'space': 1, 'well': 1, 'design': 1, 'kinematic': 1, 'analysis': 1, 'robot': 1}}, {'outer_idx': 6, 'inner_idx': 0, 'frequency_of_word': {'ultra': 1, 'wideband': 1, 'swarm': 2, 'ranging': 1, 'protocol': 1, 'dynamic': 1, 'dense': 1, 'networks': 1, 'nowadays': 1, 'wearable': 1, 'portable': 1, 'device': 1, 'also': 1, 'aerial': 1, 'ground': 1, 'robot': 1, 'made': 1, 'smaller': 1, 'lighter': 1, 'cheaper': 1, 'thus': 1, 'large': 1, 'hundred': 1, 'may': 1, 'form': 1, 'participate': 1, 'complicated': 1, 'cooperative': 1, 'application': 1, 'searching': 1, 'rescuing': 1, 'mapping': 1, 'war': 1, 'battling': 1}}, {'outer_idx': 6, 'inner_idx': 1, 'frequency_of_word': {'devices': 1, 'robot': 1, 'swarm': 1, 'three': 1, 'important': 1, 'feature': 1, 'namely': 1, 'large': 1, 'number': 1, 'high': 1, 'mobility': 1, 'short': 1, 'distance': 1, 'hence': 1, 'form': 1, 'dynamic': 1, 'dense': 1, 'wireless': 1, 'network': 1}}, {'outer_idx': 6, 'inner_idx': 2, 'frequency_of_word': {'successful': 1, 'swarm': 1, 'cooperative': 1, 'application': 1, 'require': 1, 'low': 1, 'latency': 1, 'communication': 1, 'real': 1, 'time': 1, 'localization': 1}}, {'outer_idx': 6, 'inner_idx': 3, 'frequency_of_word': {'paper': 1, 'proposes': 1, 'use': 1, 'ultra': 1, 'wideband': 1, 'uwb': 2, 'radio': 1, 'technology': 1, 'implement': 1, 'functionality': 1, 'time': 1, 'sensitive': 1, 'accurate': 1, 'distance': 1, 'calculated': 1, 'using': 1, 'transmission': 1, 'reception': 1, 'timestamps': 1, 'data': 1, 'message': 1}}, {'outer_idx': 6, 'inner_idx': 4, 'frequency_of_word': {'uwb': 1, 'swarm': 2, 'ranging': 2, 'protocol': 1, 'designed': 1, 'achieve': 1, 'simultaneously': 1, 'wireless': 1, 'data': 1, 'communication': 1, 'allows': 1, 'devicerobot': 1, 'compute': 1, 'distance': 1, 'peer': 1, 'neighbor': 1, 'time': 1}}, {'outer_idx': 6, 'inner_idx': 5, 'frequency_of_word': {'protocol': 1, 'designed': 1, 'dynamic': 1, 'dense': 1, 'network': 2, 'meanwhile': 1, 'also': 1, 'used': 1, 'various': 2, 'wireless': 1, 'implemented': 1, 'type': 1, 'devicesrobots': 1, 'including': 1, 'low': 1, 'end': 1, 'one': 1}}, {'outer_idx': 6, 'inner_idx': 6, 'frequency_of_word': {'experiment': 1, 'protocol': 1, 'implemented': 1, 'crazyflies': 1, 'stm32': 1, 'microcontroller': 1, 'powered': 1, 'micro': 1, 'drone': 1, 'onboard': 1, 'uwb': 1, 'wireless': 1, 'transceiver': 1, 'chip': 1, 'dw1000': 1}}, {'outer_idx': 6, 'inner_idx': 7, 'frequency_of_word': {'extensive': 1, 'real': 1, 'world': 1, 'experiment': 1, 'conducted': 1, 'verify': 1, 'proposed': 1, 'protocol': 1, 'various': 1, 'performance': 1, 'aspect': 1, 'total': 1, 'nine': 1, 'crazyflie': 1, 'drone': 1, 'compact': 1, 'area': 1}}, {'outer_idx': 6, 'inner_idx': 8, 'frequency_of_word': {'implemented': 1, 'swarm': 1, 'ranging': 1, 'protocol': 1, 'open': 1, 'sourced': 1, 'http': 1, 'githubcomseu': 1, 'netsicrazyflie': 1, 'firmware': 1}}, {'outer_idx': 7, 'inner_idx': 0, 'frequency_of_word': {'enhancing': 1, 'robustness': 1, 'deep': 1, 'learning': 1, 'based': 1, 'fingerprinting': 2, 'improve': 1, 'deepfake': 2, 'attribution': 2, 'artificial': 1, 'af': 1, 'so': 1, 'called': 1, 'digital': 1, 'watermarking': 1, 'technique': 1, 'used': 1, 'conduct': 1, 'ensuring': 1, 'medium': 1, 'authenticity': 1}}, {'outer_idx': 7, 'inner_idx': 1, 'frequency_of_word': {'however': 1, 'af': 1, 'prioritize': 1, 'robustness': 1, 'certain': 1, 'kind': 1, 'distortion': 1, 'making': 1, 'embedded': 1, 'watermark': 1, 'vulnerable': 1, 'standard': 1, 'image': 1, 'processing': 1, 'operation': 1}}, {'outer_idx': 7, 'inner_idx': 2, 'frequency_of_word': {'insufficient': 1, 'robustness': 1, 'reduces': 1, 'practicality': 1, 'digital': 1, 'watermarking': 1, 'technique': 1}}, {'outer_idx': 7, 'inner_idx': 3, 'frequency_of_word': {'address': 1, 'issue': 1, 'propose': 1, 'enhanced': 1, 'distortion': 1, 'agnostic': 1, 'artificial': 1, 'fingerprinting': 1, 'eda': 1, 'af': 1, 'framework': 1, 'introduces': 1, 'novel': 1, 'noise': 1, 'layer': 1, 'consisting': 1, 'attack': 1, 'booster': 1, 'followed': 1, 'convolutional': 1, 'network': 1, 'based': 1, 'attacker': 1}}, {'outer_idx': 7, 'inner_idx': 4, 'frequency_of_word': {'attacker': 1, 'simulates': 1, 'various': 1, 'distortion': 2, 'exploiting': 1, 'adversarial': 1, 'learning': 1, 'af': 1, 'agnostic': 1, 'robustness': 1}}, {'outer_idx': 7, 'inner_idx': 5, 'frequency_of_word': {'meanwhile': 1, 'due': 1, 'modeling': 1, 'limitation': 1, 'convolutional': 1, 'network': 1, 'also': 1, 'employ': 1, 'attack': 1, 'booster': 1, 'apply': 1, 'set': 1, 'differentiable': 1, 'image': 1, 'distortion': 1, 'can': 1, 'not': 1, 'well': 1, 'simulated': 1, 'attacker': 1}}, {'outer_idx': 7, 'inner_idx': 6, 'frequency_of_word': {'extensive': 1, 'experimental': 1, 'result': 1, 'show': 1, 'proposed': 1, 'approach': 1, 'improves': 1, 'quality': 1, 'extracted': 1, 'fingerprint': 1}}, {'outer_idx': 7, 'inner_idx': 7, 'frequency_of_word': {'eda': 1, 'af': 1, 'improve': 1, 'bitwise': 1, 'accuracy': 1, 'thirtysix': 1, 'take': 1, 'another': 1, 'step': 1, 'forward': 1, 'road': 1, 'deepfake': 1, 'attribution': 1}}, {'outer_idx': 8, 'inner_idx': 0, 'frequency_of_word': {'fpga': 1, 'based': 1, 'updatable': 1, 'packet': 1, 'classification': 1, 'using': 1, 'tss': 1, 'combined': 1, 'bit': 1, 'selecting': 1, 'tree': 1, 'openflow': 1, 'switch': 1, 'deployed': 1, 'sdn': 1, 'enable': 1, 'wide': 1, 'spectrum': 1, 'non': 1, 'traditional': 1, 'application': 1}}, {'outer_idx': 8, 'inner_idx': 1, 'frequency_of_word': {'promising': 1, 'alternative': 1, 'brutal': 1, 'force': 1, 'tcams': 1, 'fpga': 1, 'based': 1, 'packet': 1, 'classification': 1, 'actively': 1, 'investigated': 1}}, {'outer_idx': 8, 'inner_idx': 2, 'frequency_of_word': {'however': 1, 'none': 1, 'existing': 1, 'fpga': 1, 'design': 1, 'achieve': 1, 'high': 1, 'performance': 1, 'search': 1, 'update': 1, 'large': 1, 'scale': 1, 'rule': 1, 'set': 1}}, {'outer_idx': 8, 'inner_idx': 3, 'frequency_of_word': {'address': 1, 'issue': 1, 'propose': 1, 'tcbtree': 1, 'fpga': 1, 'based': 1, 'algorithmic': 1, 'scheme': 1, 'packet': 1, 'classification': 1}}, {'outer_idx': 8, 'inner_idx': 4, 'frequency_of_word': {'specifically': 1, 'algorithmic': 1, 'side': 1, 'i': 1, 'two': 1, 'stage': 1, 'framework': 1, 'consisting': 1, 'heterogeneous': 1, 'algorithm': 1, 'proposed': 1, 'rule': 4, 'mapped': 1, 'several': 1, 'balanced': 1, 'tree': 4, 'without': 1, 'replication': 1, 'ii': 1, 'remaining': 1, 'centralized': 1, 'tss': 2, 'tuple': 1, 'space': 1, 'search': 3, 'architecture': 1, 'together': 1, 'real': 1, 'time': 1, 'feedback': 1, 'scheme': 1, 'designed': 2, 'enhance': 1, 'efficiency': 1, 'fpga': 1, 'iii': 1, 'dilution': 1, 'method': 1, 'equalize': 1, 'distribution': 1, 'latency': 1, 'reduced': 1}}, {'outer_idx': 8, 'inner_idx': 5, 'frequency_of_word': {'hardware': 1, 'side': 1, 'i': 1, 'efficient': 1, 'data': 1, 'structure': 1, 'set': 1, 'designed': 1, 'convert': 1, 'tree': 2, 'traversal': 1, 'addressing': 1, 'process': 1, 'break': 1, 'constraint': 1, 'limited': 1, 'depth': 1, 'imbalanced': 1, 'node': 1, 'distribution': 1, 'ii': 1, 'distinct': 1, 'fully': 1, 'pipelined': 1, 'design': 1, 'multiple': 1, 'level': 1, 'parallelism': 1, 'efficiently': 1, 'explored': 1, 'multi': 2, 'core': 1, 'searchengine': 1, 'coarse': 1, 'grained': 1, 'pipeline': 1, 'herein': 1}}, {'outer_idx': 8, 'inner_idx': 6, 'frequency_of_word': {'experimental': 1, 'result': 1, 'using': 1, 'classbench': 1, 'show': 1, 'that': 1, 'implementation': 1, 'tcbtree': 1, 'fpga': 1, 'average': 1, 'classification': 1, 'throughput': 2, '1k': 1, '10k': 1, '32k': 1, '100k': 1, 'rule': 2, 'set': 2, 'achieve': 1, 'seven': 1, 'hundred': 3, 'and': 3, 'eightyeighteight': 1, 'mpps': 4, 'four': 1, 'fourthree': 1, 'two': 1, 'thirtyseven': 1, 'fortyoneeight': 1, 'respectively': 1, 'update': 1, 'benchmark': 1, 'one': 1, 'mups': 1}}, {'outer_idx': 9, 'inner_idx': 0, 'frequency_of_word': {'integrated': 2, 'digital': 4, 'twin': 4, 'simulation': 2, 'scheduling': 2, 'system': 2, 'cyber': 2, 'physical': 2, 'environment': 2, 'paper': 1, 'described': 1, 'research': 1, 'study': 1, 'manufacturing': 1}}, {'outer_idx': 9, 'inner_idx': 1, 'frequency_of_word': {'proposed': 1, 'approach': 1, 'provides': 1, 'optimal': 1, 'production': 1, 'schedule': 1, 'based': 1, 'real': 1, 'time': 1, 'information': 1, 'established': 1, 'cyber': 1, 'physical': 1, 'digital': 1, 'twin': 1, 'platform': 1}}, {'outer_idx': 9, 'inner_idx': 2, 'frequency_of_word': {'system': 3, 'integrated': 1, 'existing': 1, 'manufacturing': 1, 'erp': 1, 'mes': 1, 'scada': 1, 'via': 1, 'cyber': 1, 'physical': 1, 'digital': 1, 'twin': 1, 'platform': 1, 'scheduling': 1, 'result': 1, 'adaptive': 1, 'dynamically': 1, 'changing': 1, 'environment': 1, 'market': 1, 'demand': 1}}, {'outer_idx': 9, 'inner_idx': 3, 'frequency_of_word': {'proposed': 1, 'system': 1, 'designed': 1, 'leverage': 1, 'advantage': 1, 'digital': 1, 'twin': 1, 'simulation': 1, 'optimization': 1, 'scheduling': 1, 'technique': 1}}, {'outer_idx': 9, 'inner_idx': 4, 'frequency_of_word': {'prototype': 1, 'developed': 1, 'illustrate': 1, 'application': 1, 'case': 1, 'electronics': 1, 'manufacturing': 1, 'industry': 1}}, {'outer_idx': 10, 'inner_idx': 0, 'frequency_of_word': {'prototype': 1, 'instrumented': 1, 'rock': 1, 'bolt': 1, 'continuous': 1, 'monitoring': 1, 'roof': 2, 'fall': 2, 'hazard': 1, 'deep': 1, 'underground': 2, 'mines': 1, 'currently': 1, 'one': 1, 'dangerous': 1, 'threat': 1, 'associated': 1, 'mining': 1, 'great': 1, 'depth': 1}}, {'outer_idx': 10, 'inner_idx': 1, 'frequency_of_word': {'every': 1, 'occurrence': 1, 'event': 1, 'pose': 1, 'significant': 1, 'risk': 1, 'mining': 2, 'crew': 1, 'disturbs': 1, 'continuity': 1, 'process': 2, 'clearly': 1, 'affect': 1, 'economy': 1, 'exploitation': 1}}, {'outer_idx': 10, 'inner_idx': 2, 'frequency_of_word': {'development': 1, 'reliable': 1, 'monitoring': 1, 'system': 1, 'may': 1, 'significantly': 1, 'reduce': 1, 'impact': 1, 'eventual': 1, 'roof': 1, 'failure': 1, 'positive': 1, 'effect': 1, 'sustainability': 1, 'extraction': 1, 'process': 1}}, {'outer_idx': 10, 'inner_idx': 3, 'frequency_of_word': {'within': 1, 'research': 1, 'study': 1, 'prototype': 1, 'instrumented': 1, 'rock': 1, 'bolt': 1, 'developed': 1, 'continuous': 1, 'stress': 1, 'measurement': 1, 'presented': 1}}, {'outer_idx': 10, 'inner_idx': 4, 'frequency_of_word': {'procedure': 1, 'four': 1, 'groove': 1, 'multilevel': 1, 'instrumented': 1, 'rock': 1, 'bolt': 1, 'described': 1, 'calibration': 1, 'process': 1, 'shown': 1}}, {'outer_idx': 10, 'inner_idx': 5, 'frequency_of_word': {'then': 1, 'preliminary': 1, 'result': 1, 'long': 1, 'term': 1, 'situ': 1, 'monitoring': 1, 'presented': 1}}, {'outer_idx': 10, 'inner_idx': 6, 'frequency_of_word': {'based': 1, 'continuous': 1, 'monitoring': 1, 'stress': 1, 'distribution': 1, 'within': 1, 'immediate': 1, 'roof': 2, 'stratum': 1, 'concluded': 1, 'developed': 1, 'instrumented': 1, 'rock': 1, 'bolt': 1, 'provides': 1, 'reliable': 1, 'result': 1, 'useful': 1, 'device': 1, 'ensuring': 1, 'possibility': 1, 'early': 1, 'warning': 1, 'miner': 1, 'increasing': 1, 'fall': 1, 'risk': 1}}, {'outer_idx': 11, 'inner_idx': 0, 'frequency_of_word': {'repqc': 1, 'threefour': 1, 'ujop': 1, 'fortyeight': 1, 'kops': 1, 'post': 2, 'quantum': 3, 'crypto': 1, 'processor': 1, 'multiple': 1, 'mathematical': 1, 'problems': 1, 'cryptography': 2, 'pqc': 1, 'investigated': 1, 'replace': 1, 'classical': 1, 'public': 1, 'algorithm': 1, 'would': 1, 'completely': 1, 'broken': 1, 'large': 1, 'scale': 1, 'computer': 1}}, {'outer_idx': 11, 'inner_idx': 1, 'frequency_of_word': {'however': 1, 'current': 1, 'pqc': 2, 'scheme': 1, 'completely': 1, 'different': 1, 'mathematical': 1, 'foundation': 1, 'parameter': 1, 'set': 1, 'make': 1, 'implementation': 1, 'unified': 1, 'processor': 1, 'extremely': 1, 'challenging': 1}}, {'outer_idx': 11, 'inner_idx': 2, 'frequency_of_word': {'address': 1, 'issue': 1, 'agile': 1, 'pqc': 1, 'processor': 1, 'repqc': 1, 'proposed': 1, 'work': 1, 'support': 1, 'scheme': 1, 'multiple': 1, 'mathematical': 1, 'problem': 1}}, {'outer_idx': 11, 'inner_idx': 3, 'frequency_of_word': {'first': 1, 'hierarchical': 1, 'calculation': 1, 'framework': 1, 'ranging': 1, 'algorithm': 1, 'level': 3, 'task': 1, 'coefficient': 1, 'proposed': 1, 'achieve': 1, 'desirable': 1, 'flexibility': 1, 'energy': 1, 'efficiency': 1}}, {'outer_idx': 11, 'inner_idx': 4, 'frequency_of_word': {'second': 1, 'hybrid': 1, 'processing': 1, 'element': 1, 'array': 1, 'built': 1, 'support': 1, 'arithmetic': 1, 'logical': 1, 'operation': 1, 'simultaneously': 1, 'algorithm': 2, 'hardware': 1, 'co': 1, 'design': 1, 'utilized': 1, 'task': 1, 'level': 1, 'scheduler': 1, 'improve': 1, 'oriented': 1, 'energy': 1, 'efficiency': 1}}, {'outer_idx': 11, 'inner_idx': 5, 'frequency_of_word': {'finally': 1, 'parallelism': 1, 'exploration': 1, 'algorithm': 1, 'level': 1, 'computation': 1, 'transformation': 1, 'utilized': 1, 'optimize': 1, 'configuration': 1, 'repqc': 1, 'higher': 1, 'throughput': 1}}, {'outer_idx': 11, 'inner_idx': 6, 'frequency_of_word': {'fabricated': 1, 'twentyeight': 1, 'nm': 1, 'process': 1, 'repqc': 1, 'achieves': 1, 'energy': 1, 'efficiency': 1, 'threefour': 1, 'ujop': 1, 'throughput': 1, 'fortyeight': 1, 'kops': 1, 'twotimes': 1, 'twentythreetimes': 1, 'higher': 1, 'state': 1, 'ofthe': 1, 'art': 1, 'work': 1, 'respectively': 1}}, {'outer_idx': 11, 'inner_idx': 7, 'frequency_of_word': {'best': 1, 'knowledge': 1, 'repqc': 1, 'first': 1, 'silicon': 1, 'proven': 1, 'pqc': 1, 'processor': 1, 'different': 1, 'mathematical': 1, 'problem': 1}}, {'outer_idx': 12, 'inner_idx': 0, 'frequency_of_word': {'tackling': 1, 'climate': 2, 'change': 2, 'machine': 2, 'learning': 2, 'one': 1, 'greatest': 1, 'challenge': 1, 'facing': 1, 'humanity': 1, 'we': 1, 'ml': 1, 'expert': 1, 'may': 1, 'wonder': 1, 'help': 1}}, {'outer_idx': 12, 'inner_idx': 1, 'frequency_of_word': {'describe': 1, 'ml': 1, 'powerful': 1, 'tool': 1, 'reducing': 1, 'greenhouse': 1, 'gas': 1, 'emission': 1, 'helping': 1, 'society': 1, 'adapt': 1, 'changing': 1, 'climate': 1}}, {'outer_idx': 12, 'inner_idx': 2, 'frequency_of_word': {'smart': 1, 'grid': 1, 'disaster': 1, 'management': 1, 'identify': 1, 'high': 1, 'impact': 1, 'problem': 1, 'existing': 1, 'gap': 1, 'filled': 1, 'ml': 1, 'collaboration': 1, 'field': 1}}, {'outer_idx': 12, 'inner_idx': 3, 'frequency_of_word': {'recommendation': 1, 'encompass': 1, 'exciting': 1, 'research': 1, 'question': 1, 'well': 1, 'promising': 1, 'business': 1, 'opportunity': 1}}, {'outer_idx': 12, 'inner_idx': 4, 'frequency_of_word': {'call': 1, 'ml': 1, 'community': 1, 'join': 1, 'global': 1, 'effort': 1, 'climate': 1, 'change': 1}}, {'outer_idx': 13, 'inner_idx': 0, 'frequency_of_word': {'many': 1, 'objective': 1, 'optimization': 1, 'based': 1, 'federal': 1, 'deep': 1, 'generation': 1, 'model': 1, 'enhancing': 1, 'data': 1, 'processing': 1, 'capability': 1, 'iot': 2, 'rapid': 1, 'progress': 1, 'artificial': 1, 'intelligence': 1, 'expands': 1, 'wide': 1, 'applicability': 1, 'internet': 1, 'things': 1}}, {'outer_idx': 13, 'inner_idx': 1, 'frequency_of_word': {'meanwhile': 1, 'data': 2, 'insufficient': 1, 'source': 1, 'privacy': 1, 'key': 1, 'supply': 1, 'chain': 1, 'challenge': 1, 'facing': 1, 'iot': 1, 'especially': 1, 'healthcare': 1, 'industry': 1}}, {'outer_idx': 13, 'inner_idx': 2, 'frequency_of_word': {'address': 1, 'problem': 1, 'healthcare': 1, 'iot': 1, 'article': 1, 'propose': 1, 'skin': 1, 'cancer': 1, 'detection': 1, 'model': 2, 'based': 1, 'federated': 1, 'learning': 1, 'integrated': 1, 'deep': 1, 'generation': 1}}, {'outer_idx': 13, 'inner_idx': 3, 'frequency_of_word': {'first': 1, 'employ': 1, 'dual': 1, 'generative': 1, 'adversarial': 1, 'network': 1, 'address': 1, 'problem': 1, 'insufficient': 1, 'data': 1}}, {'outer_idx': 13, 'inner_idx': 4, 'frequency_of_word': {'addition': 1, 'improve': 1, 'quality': 1, 'generated': 1, 'image': 3, 'synchronously': 1, 'optimize': 1, 'sharpness': 1, 'frechet': 1, 'inception': 1, 'distance': 1, 'diversity': 1, 'loss': 1, 'using': 1, 'knee': 1, 'point': 1, 'driven': 1, 'evolutionary': 1, 'algorithm': 1, 'knea': 1}}, {'outer_idx': 13, 'inner_idx': 5, 'frequency_of_word': {'then': 1, 'protect': 1, 'patient': 1, 'information': 1, 'privacy': 1, 'training': 1, 'federated': 1, 'skin': 1, 'cancer': 1, 'framework': 1}}, {'outer_idx': 13, 'inner_idx': 6, 'frequency_of_word': {'finally': 1, 'employ': 1, 'isic': 1, 'two': 1, 'thousand': 1, 'and': 1, 'eighteen': 1, 'dataset': 1, 'test': 1, 'performance': 1, 'proposed': 1, 'training': 1, 'model': 1, 'different': 1, 'situation': 1, 'including': 1, 'using': 1, 'identically': 1, 'distributed': 2, 'data': 2, 'nonidentically': 1, 'sparse': 1, 'convolutional': 2, 'neural': 2, 'network': 2, 'fully': 1, 'connected': 1}}, {'outer_idx': 13, 'inner_idx': 7, 'frequency_of_word': {'experiment': 1, 'result': 1, 'demonstrate': 1, 'accuracy': 1, 'area': 1, 'curve': 1, 'reach': 1, 'ninetyone': 1, 'eightyeight': 1, 'respectively': 1}}, {'outer_idx': 13, 'inner_idx': 8, 'frequency_of_word': {'model': 1, 'help': 1, 'resolve': 1, 'problem': 1, 'insufficient': 1, 'data': 2, 'smart': 1, 'medicine': 1, 'iot': 1, 'protect': 1, 'privacy': 1, 'user': 1, 'also': 1, 'providing': 1, 'excellent': 1, 'detection': 1, 'rate': 1}}, {'outer_idx': 14, 'inner_idx': 0, 'frequency_of_word': {'exploiting': 1, 'combined': 1, 'gracegrace': 1, 'fo': 2, 'solutions': 1, 'determine': 1, 'gravimetric': 2, 'excitations': 1, 'polar': 2, 'motion': 2, 'observations': 1, 'gravity': 1, 'recovery': 1, 'climate': 1, 'experiment': 1, 'grace': 3, 'follow': 1, 'on': 1, 'mission': 1, 'used': 1, 'estimate': 1, 'excitation': 1, 'pm': 2, 'reflects': 1, 'contribution': 1, 'mass': 1, 'change': 1, 'continental': 1, 'hydrosphere': 1, 'cryosphere': 1, 'variation': 1}}, {'outer_idx': 14, 'inner_idx': 1, 'frequency_of_word': {'many': 1, 'solution': 1, 'earth': 1, 's': 1, 'gravity': 1, 'field': 1, 'variation': 1, 'developed': 1, 'institute': 1, 'around': 1, 'world': 1, 'based': 1, 'gracegrace': 1, 'fo': 1, 'data': 1, 'however': 1, 'remains': 1, 'inconclusive': 1, 'reliable': 1, 'determination': 1, 'pm': 1, 'excitation': 1}}, {'outer_idx': 14, 'inner_idx': 2, 'frequency_of_word': {'study': 1, 'present': 1, 'combined': 2, 'series': 1, 'gracegrace': 1, 'fobased': 1, 'gravimetric': 1, 'excitation': 1, 'pm': 1, 'computed': 1, 'using': 1, 'three': 1, 'corneredhat': 1, 'tch': 1, 'method': 1, 'wherein': 1, 'internal': 1, 'noise': 1, 'level': 1, 'solution': 1, 'reduced': 1, 'minimum': 1}}, {'outer_idx': 14, 'inner_idx': 3, 'frequency_of_word': {'compare': 1, 'combined': 2, 'series': 1, 'result': 1, 'obtained': 1, 'gracegrace': 1, 'fo': 1, 'solution': 2, 'provided': 1, 'cost': 1, 'g': 1, 'international': 1, 'combination': 1, 'service': 1, 'time': 1, 'variable': 1, 'gravity': 1, 'fields': 1, 'single': 1, 'elaborated': 1, 'center': 1, 'space': 1, 'research': 1, 'csr': 1}}, {'outer_idx': 14, 'inner_idx': 4, 'frequency_of_word': {'gravimetric': 1, 'excitation': 2, 'series': 1, 'evaluated': 1, 'comparison': 1, 'sum': 1, 'hydrological': 1, 'cryospheric': 1, 'signal': 1, 'geodetically': 1, 'observed': 1, 'pm': 1, 'called': 1, 'gao': 1}}, {'outer_idx': 14, 'inner_idx': 5, 'frequency_of_word': {'result': 1, 'show': 1, 'minimizing': 1, 'internal': 1, 'noise': 1, 'level': 1, 'combined': 1, 'excitation': 1, 'series': 1, 'using': 1, 'tch': 1, 'method': 1, 'receive': 1, 'higher': 1, 'consistency': 1, 'gao': 1, 'case': 1, 'cost': 1, 'g': 1, 'csr': 1, 'solution': 1, 'especially': 1, 'non': 1, 'seasonal': 1, 'oscillation': 1}}, {'outer_idx': 14, 'inner_idx': 6, 'frequency_of_word': {'spectral': 1, 'band': 1, 'obtained': 1, 'correlation': 1, 'gao': 1, 'best': 1, 'combined': 1, 'series': 1, 'high': 1, 'zerosixtyfive': 1, 'zeroseventytwo': 1, 'Ï‡1': 1, 'Ï‡2': 1, 'equatorial': 1, 'component': 1, 'pm': 1, 'excitation': 1, 'respectively': 1}}, {'outer_idx': 14, 'inner_idx': 7, 'frequency_of_word': {'corresponding': 1, 'value': 1, 'seasonal': 1, 'oscillation': 1, 'zeroninetyone': 1, 'Ï‡1': 1, 'zeroeightynine': 1, 'Ï‡2': 1}}, {'outer_idx': 14, 'inner_idx': 8, 'frequency_of_word': {'combined': 1, 'series': 1, 'developed': 1, 'study': 1, 'explain': 1, 'sixtyeight': 1, 'sixty': 1, 'overall': 1, 'gao': 1, 'variability': 1, 'Ï‡1': 1, 'Ï‡2': 1, 'respectively': 1}}, {'outer_idx': 15, 'inner_idx': 0, 'frequency_of_word': {'correlations': 1, 'emg': 1, 'structure': 1, 'movement': 1, 'patterns': 1, 'activity': 1, 'postural': 1, 'muscles': 1, 'able': 2, 'bodied': 2, 'wheelchair': 2, 'fencers': 1, 'study': 1, 'involved': 1, 'paralympic': 2, 'fencer': 2, 'n': 2, 'seven': 2, 'two': 1, 'disability': 1, 'category': 1, 'female': 1, 'epee': 1, 'member': 1, 'polish': 1, 'fencing': 1, 'team': 1}}, {'outer_idx': 15, 'inner_idx': 1, 'frequency_of_word': {'performance': 1, 'postural': 1, 'muscle': 3, 'sword': 1, 'arm': 1, 'group': 1, 'fencer': 2, 'front': 1, 'rear': 1, 'leg': 1, 'able': 1, 'bodied': 1, 'examined': 1, 'using': 1, 'surface': 1, 'electromyography': 1, 'accelerometer': 1, 'optitrack': 1, 'motion': 1, 'analysis': 1, 'system': 1, 'well': 1, 'ground': 1, 'force': 1, 'reaction': 1, 'platform': 1}}, {'outer_idx': 15, 'inner_idx': 2, 'frequency_of_word': {'activation': 1, 'sequence': 1, 'individual': 1, 'muscle': 1, 'determined': 1, 'structure': 1, 'movement': 1, 'pattern': 1, 'able': 1, 'bodied': 1, 'wheelchair': 1, 'fencer': 1, 'formulated': 1}}, {'outer_idx': 15, 'inner_idx': 3, 'frequency_of_word': {'statistically': 1, 'significant': 1, 'correlation': 1, 'found': 1, 'complex': 1, 'motor': 1, 'reaction': 1, 'time': 1, 'latissimus': 1, 'dorsi': 1, 'muscle': 1, 'activation': 1, 'p': 1, 'zerothirtynine': 1, 'z': 1, 'twosixtytwo': 1, 'wheelchair': 1, 'fencer': 1}}, {'outer_idx': 15, 'inner_idx': 4, 'frequency_of_word': {'high': 1, 'correlation': 1, 'vertical': 1, 'force': 1, 'emg': 1, 'signal': 1, 'value': 1, 'gastrocnemius': 1, 'caput': 1, 'laterale': 1, 'muscle': 1, 'zeroeightyfive': 1, 'p': 1, 'zerotwentytwo': 1, 'found': 1, 'able': 1, 'bodied': 1, 'fencer': 1}}, {'outer_idx': 15, 'inner_idx': 5, 'frequency_of_word': {'heuristic': 1, 'analysis': 1, 'indicated': 1, 'significance': 1, 'postural': 1, 'muscle': 1, 'movement': 1, 'pattern': 1, 'wheelchair': 1, 'able': 1, 'bodied': 1, 'fencer': 1}}, {'outer_idx': 15, 'inner_idx': 6, 'frequency_of_word': {'muscle': 1, 'play': 1, 'crucial': 1, 'role': 1, 'anticipatory': 1, 'postural': 1, 'adjustment': 1, 'trunk': 1, 'technical': 1, 'fencing': 1, 'action': 1, 'including': 1, 'attack': 1, 'opponent': 1, 's': 1, 'body': 1}}, {'outer_idx': 16, 'inner_idx': 0, 'frequency_of_word': {'effects': 1, 'inter': 2, 'intra': 2, 'specific': 2, 'interactions': 1, 'moose': 1, 'habitat': 2, 'selection': 2, 'limited': 1, 'temperature': 2, 'daily': 1, 'activity': 1, 'pattern': 1, 'large': 1, 'herbivore': 1, 'might': 1, 'affected': 1, 'interaction': 1, 'change': 1, 'spatial': 1, 'scale': 1, 'seasonal': 1}}, {'outer_idx': 16, 'inner_idx': 1, 'frequency_of_word': {'reveal': 1, 'factor': 1, 'driving': 1, 'habitat': 3, 'selection': 3, 'moose': 3, 'collected': 1, 'alces': 2, 'roe': 2, 'deer': 2, 'capreolus': 1, 'pygargus': 1, 'bedfordi': 1, 'occurrence': 2, 'data': 1, 'analyzed': 1, 'multi': 1, 'scale': 1, 'daily': 1, 'activity': 1, 'pattern': 1, 'quantified': 1, 'effect': 1, 'spatial': 1, 'heterogeneity': 1, 'distribution': 1, 'temperature': 1, 'well': 1, 'process': 1}}, {'outer_idx': 16, 'inner_idx': 2, 'frequency_of_word': {'result': 1, 'suggested': 1, 'moose': 2, 'roe': 1, 'deer': 1, 'distribution': 1, 'spatially': 1, 'overlap': 1, 'habitat': 1, 'selection': 1, 'especially': 1, 'sensitive': 1, 'landscape': 1, 'variable': 1, 'large': 1, 'scale': 1}}, {'outer_idx': 16, 'inner_idx': 3, 'frequency_of_word': {'also': 1, 'found': 1, 'activity': 1, 'pattern': 1, 'sex': 1, 'moose': 1, 'degree': 1, 'temporal': 1, 'separation': 1, 'roe': 1, 'deer': 1}}, {'outer_idx': 16, 'inner_idx': 4, 'frequency_of_word': {'snow': 1, 'free': 1, 'season': 2, 'temperature': 3, 'drove': 1, 'moose': 1, 'habitat': 1, 'selection': 1, 'limited': 1, 'threshold': 1, 'seventeen': 1, 'c': 1, 'snowy': 1, 'similar': 1, 'driving': 1, 'pattern': 1, 'due': 1, 'severe': 1, 'cold': 1, 'environment': 1}}, {'outer_idx': 16, 'inner_idx': 5, 'frequency_of_word': {'daily': 1, 'activity': 1, 'pattern': 1, 'moose': 1, 'showed': 1, 'seasonal': 1, 'change': 1, 'active': 2, 'dawn': 1, 'nightfall': 1, 'avoid': 1, 'heat': 1, 'pressure': 1, 'snow': 2, 'free': 1, 'season': 2, 'daytime': 1, 'cold': 1, 'adaptation': 1}}, {'outer_idx': 16, 'inner_idx': 6, 'frequency_of_word': {'consequently': 1, 'study': 1, 'provides': 1, 'new': 1, 'insight': 1, 'comprehensive': 1, 'effect': 1, 'environmental': 1, 'change': 1, 'inter': 1, 'intra': 1, 'specific': 1, 'relationship': 1, 'influence': 1, 'habitat': 1, 'selection': 1, 'daily': 1, 'activity': 1, 'pattern': 1, 'moose': 1, 'heat': 1, 'sensitive': 1, 'animal': 1, 'global': 1, 'warming': 1}}, {'outer_idx': 17, 'inner_idx': 0, 'frequency_of_word': {'capability': 2, 'exploration': 1, 'extended': 2, 'state': 2, 'observer': 2, 'based': 2, 'control': 3, 'uncertain': 2, 'case': 1, 'disturbance': 3, 'actuator': 2, 'saturation': 2, 'considering': 1, 'class': 1, 'nonlinear': 1, 'system': 1, 'subject': 1, 'input': 1, 'article': 1, 'explored': 1, 'active': 1, 'rejection': 1, 'controller': 1, 'presence': 1, 'uncertainty': 1}}, {'outer_idx': 17, 'inner_idx': 1, 'frequency_of_word': {'first': 1, 'actuator': 1, 'saturation': 1, 'handled': 1, 'convex': 1, 'hull': 1, 'extended': 1, 'state': 1, 'observer': 1, 'based': 1, 'control': 2, 'scheme': 1, 'ellipsoid': 1, 'invariant': 1, 'set': 1, 'inside': 1, 'domain': 1, 'attraction': 1, 'system': 1, 'obtained': 1, 'using': 1, 'lyapunov': 1, 'method': 1}}, {'outer_idx': 17, 'inner_idx': 2, 'frequency_of_word': {'second': 1, 'enlargement': 2, 'problem': 1, 'invariant': 3, 'set': 3, 'also': 2, 'studied': 1, 'illustrates': 1, 'convex': 1, 'hull': 1, 'ellipsoid': 1}}, {'outer_idx': 17, 'inner_idx': 3, 'frequency_of_word': {'finally': 1, 'comprehensive': 1, 'comparison': 1, 'existing': 1, 'method': 1, 'brushless': 1, 'dc': 1, 'motor': 1, 'demonstrate': 1, 'stronger': 1, 'capability': 1, 'designed': 1, 'controller': 1, 'disturbance': 1, 'rejection': 1, 'actuator': 1, 'saturation': 1}}, {'outer_idx': 17, 'inner_idx': 4, 'frequency_of_word': {'result': 1, 'show': 1, 'disturbance': 2, 'offset': 1, 'performance': 1, 'index': 1, 'integrated': 1, 'time': 1, 'absolute': 2, 'error': 2, 'integral': 2, 'half': 1, 'classical': 1, 'proportionalintegralderivative': 1, 'pid': 1, 'double': 1, 'sliding': 1, 'mode': 1, 'control': 1, 'rejection': 1, 'capability': 1, 'twice': 1, 'case': 1}}, {'outer_idx': 18, 'inner_idx': 0, 'frequency_of_word': {'sequential': 1, 'frame': 1, 'interpolation': 1, 'dct': 1, 'based': 1, 'video': 3, 'compression': 1, 'framework': 1, 'data': 2, 'ubiquitous': 1, 'capturing': 1, 'transferring': 1, 'storing': 1, 'even': 1, 'compressed': 1, 'challenging': 1, 'requires': 1, 'substantial': 1, 'resource': 1}}, {'outer_idx': 18, 'inner_idx': 1, 'frequency_of_word': {'large': 1, 'amount': 1, 'video': 1, 'traffic': 1, 'transmitted': 1, 'internet': 1, 'improvement': 1, 'compressing': 1, 'data': 1, 'even': 1, 'small': 1, 'drastically': 1, 'impact': 1, 'resource': 1, 'consumption': 1}}, {'outer_idx': 18, 'inner_idx': 2, 'frequency_of_word': {'paper': 1, 'present': 1, 'hybrid': 1, 'video': 2, 'compression': 2, 'framework': 2, 'unites': 1, 'advantage': 1, 'dct': 1, 'based': 2, 'interpolation': 1, 'method': 1, 'single': 1}}, {'outer_idx': 18, 'inner_idx': 3, 'frequency_of_word': {'show': 1, 'work': 1, 'deliver': 1, 'visual': 2, 'quality': 2, 'or': 1, 'case': 1, 'improve': 1, 'reducing': 1, 'bandwidth': 1, 'ten': 1, 'twenty': 1}}, {'outer_idx': 19, 'inner_idx': 0, 'frequency_of_word': {'detecting': 1, 'continuous': 2, 'integration': 2, 'skip': 1, 'commits': 1, 'using': 1, 'multi': 1, 'objective': 1, 'evolutionary': 1, 'search': 1, 'ci': 1, 'consists': 1, 'integrating': 1, 'change': 1, 'introduced': 1, 'different': 1, 'developer': 1, 'frequently': 1, 'automation': 1, 'build': 1, 'process': 1}}, {'outer_idx': 19, 'inner_idx': 1, 'frequency_of_word': {'nevertheless': 1, 'ci': 1, 'build': 1, 'process': 1, 'seen': 1, 'major': 1, 'barrier': 1, 'cause': 1, 'delay': 1, 'product': 1, 'release': 1, 'date': 1}}, {'outer_idx': 19, 'inner_idx': 2, 'frequency_of_word': {'one': 1, 'main': 1, 'reason': 1, 'delay': 1, 'simple': 1, 'change': 1, 'ie': 1, 'skipped': 1, 'trigger': 1, 'build': 1, 'represents': 1, 'unnecessary': 1, 'overhead': 1, 'particularly': 1, 'painful': 1, 'large': 1, 'project': 1}}, {'outer_idx': 19, 'inner_idx': 3, 'frequency_of_word': {'order': 1, 'cut': 1, 'expense': 1, 'ci': 2, 'build': 1, 'time': 1, 'propose': 1, 'paper': 1, 'skipci': 1, 'novel': 1, 'search': 1, 'based': 2, 'approach': 1, 'automatically': 1, 'detect': 1, 'skip': 1, 'commits': 1, 'adaptation': 1, 'strength': 1, 'pareto': 1, 'evolutionary': 1, 'algorithm': 1, 'spea': 1, 'two': 1}}, {'outer_idx': 19, 'inner_idx': 4, 'frequency_of_word': {'approach': 1, 'aim': 1, 'provide': 1, 'optimal': 1, 'trade': 1, 'off': 1, 'two': 1, 'conflicting': 1, 'objective': 1, 'deal': 1, 'skipped': 2, 'non': 1, 'commits': 1}}, {'outer_idx': 19, 'inner_idx': 5, 'frequency_of_word': {'evaluate': 1, 'approach': 1, 'investigate': 1, 'performance': 1, 'within': 1, 'cross': 1, 'project': 2, 'validation': 1, 'benchmark': 1, 'fourteen': 1, 'two': 1, 'hundred': 1, 'and': 1, 'ninetyfour': 1, 'ci': 2, 'commits': 1, 'fifteen': 1, 'use': 1, 'travis': 1, 'system': 1}}, {'outer_idx': 19, 'inner_idx': 6, 'frequency_of_word': {'statistical': 1, 'test': 1, 'revealed': 1, 'approach': 2, 'show': 1, 'clear': 1, 'advantage': 1, 'baseline': 1, 'average': 1, 'score': 1, 'ninetytwo': 1, 'eightyfour': 1, 'term': 1, 'auc': 1, 'cross': 2, 'validation': 2, 'project': 1, 'respectively': 1}}, {'outer_idx': 19, 'inner_idx': 7, 'frequency_of_word': {'furthermore': 1, 'feature': 2, 'analysis': 1, 'reveals': 1, 'documentation': 1, 'change': 1, 'term': 1, 'appearing': 1, 'commit': 1, 'message': 1, 'committer': 1, 'experience': 1, 'prominent': 1, 'ci': 1, 'skip': 1, 'detection': 1}}, {'outer_idx': 19, 'inner_idx': 8, 'frequency_of_word': {'come': 1, 'cross': 1, 'project': 1, 'scenario': 1, 'result': 2, 'reveal': 1, 'besides': 1, 'documentation': 1, 'change': 1, 'strong': 1, 'link': 1, 'current': 1, 'previous': 1, 'commits': 1}}, {'outer_idx': 19, 'inner_idx': 9, 'frequency_of_word': {'moreover': 1, 'deployed': 1, 'evaluated': 1, 'usefulness': 1, 'skipci': 1, 'industrial': 1, 'partner': 1}}, {'outer_idx': 19, 'inner_idx': 10, 'frequency_of_word': {'qualitative': 1, 'result': 1, 'demonstrate': 1, 'effectiveness': 1, 'skipci': 1, 'providing': 1, 'relevant': 1, 'ci': 1, 'skip': 1, 'commit': 1, 'recommendation': 1, 'developer': 1, 'two': 1, 'large': 1, 'software': 1, 'project': 1, 'practitioner': 1, 's': 1, 'point': 1, 'view': 1}}, {'outer_idx': 20, 'inner_idx': 0, 'frequency_of_word': {'heimdallr': 1, 'fingerprinting': 1, 'sd': 2, 'wan': 2, 'control': 2, 'plane': 1, 'architecture': 1, 'via': 1, 'encrypted': 1, 'traffic': 1, 'software': 2, 'defined': 2, 'wide': 1, 'area': 1, 'network': 3, 'emerged': 1, 'new': 1, 'paradigm': 1, 'steering': 1, 'large': 1, 'scale': 1, 'flexibly': 1, 'adopting': 1, 'distributed': 1, 'sdn': 1, 'controller': 1}}, {'outer_idx': 20, 'inner_idx': 1, 'frequency_of_word': {'key': 1, 'building': 1, 'logically': 1, 'centralized': 1, 'physically': 1, 'distributed': 1, 'control': 2, 'plane': 1, 'running': 1, 'diverse': 1, 'cluster': 1, 'management': 1, 'protocol': 1, 'achieve': 1, 'consistency': 1, 'exchange': 1, 'traffic': 1}}, {'outer_idx': 20, 'inner_idx': 2, 'frequency_of_word': {'meanwhile': 1, 'observe': 1, 'control': 2, 'traffic': 2, 'expose': 1, 'unique': 1, 'time': 1, 'series': 1, 'pattern': 2, 'directional': 1, 'relationship': 1, 'due': 1, 'operational': 1, 'structure': 1, 'even': 1, 'though': 1, 'encrypted': 1, 'disclose': 1, 'confidential': 1, 'information': 1, 'plane': 1, 'topology': 1, 'protocol': 1, 'dependency': 1, 'exploited': 1, 'severe': 1, 'attack': 1}}, {'outer_idx': 20, 'inner_idx': 3, 'frequency_of_word': {'insight': 1, 'propose': 1, 'new': 1, 'sd': 1, 'wan': 1, 'fingerprinting': 1, 'system': 1, 'called': 1, 'heimdallr': 1}}, {'outer_idx': 20, 'inner_idx': 4, 'frequency_of_word': {'analyzes': 1, 'periodical': 1, 'operational': 1, 'pattern': 1, 'sd': 1, 'wan': 1, 'cluster': 2, 'management': 2, 'protocol': 2, 'context': 1, 'flow': 1, 'direction': 1, 'collected': 1, 'control': 2, 'traffic': 2, 'utilizing': 1, 'deep': 1, 'learning': 1, 'based': 1, 'approach': 1, 'classify': 1, 'automatically': 1, 'miscellaneous': 1, 'datasets': 1}}, {'outer_idx': 20, 'inner_idx': 5, 'frequency_of_word': {'evaluation': 1, 'performed': 1, 'realistic': 1, 'sd': 2, 'wan': 2, 'environment': 1, 'consisting': 1, 'geographically': 1, 'distant': 1, 'three': 1, 'campus': 1, 'network': 2, 'one': 2, 'enterprise': 1, 'show': 1, 'heimdallr': 1, 'classify': 1, 'control': 2, 'traffic': 1, 'ninetythree': 1, 'identify': 1, 'individual': 1, 'protocol': 1, 'eighty': 1, 'macro': 1, 'f': 1, 'score': 1, 'finally': 1, 'infer': 1, 'plane': 1, 'topology': 1, 'seventy': 1, 'similarity': 1}}, {'outer_idx': 21, 'inner_idx': 0, 'frequency_of_word': {'question': 1, 'answering': 1, 'stack': 3, 'applying': 1, 'string': 2, 'similarity': 1, 'present': 1, 'method': 1, 'evaluate': 1, 'fill': 1, 'inthe': 1, 'blank': 1, 'student': 1, 'answer': 1, 'using': 1, 'metric': 1, 'possible': 1, 'current': 1, 'version': 1}}, {'outer_idx': 21, 'inner_idx': 1, 'frequency_of_word': {'increase': 1, 'quality': 1, 'evaluation': 1, 'use': 1, 'whitelist': 1, 'blacklist': 1, 'instead': 1, 'single': 1, 'teacher': 1, 'answer': 1}}, {'outer_idx': 21, 'inner_idx': 2, 'frequency_of_word': {'performance': 1, 'stack': 1, 'question': 1, 'equipped': 1, 'string': 1, 'metric': 1, 'quantitatively': 1, 'demonstrated': 1, 'evaluating': 1, 'use': 1, 'mathematics': 1, 'course': 1}}, {'outer_idx': 22, 'inner_idx': 0, 'frequency_of_word': {'regulation': 2, 'tracking': 2, 'control': 1, 'omnidirectional': 2, 'rotation': 2, 'spherical': 2, 'motors': 1, 'motor': 1, 'capable': 1, 'ball': 1, 'joint': 1, 'provide': 1, 'highly': 1, 'dexterous': 1, 'actuator': 1, 'system': 1, 'yet': 1, 'also': 1, 'present': 1, 'great': 1, 'challenge': 1, 'developing': 1, 'high': 1, 'performance': 1, 'controller': 1, 'due': 1, 'complex': 1, 'rotor': 1, 'dynamic': 1}}, {'outer_idx': 22, 'inner_idx': 1, 'frequency_of_word': {'article': 1, 'present': 1, 'complementary': 1, 'italic': 10, 'h': 4, 'sub': 8, 'two': 2, 'c': 1, 'control': 1, 'method': 1, 'precisely': 1, 'controlling': 1, 'multidegree': 1, 'freedom': 1, 'dof': 1, 'orientation': 1, 'spherical': 1, 'motor': 1, 'presence': 1, 'external': 1, 'disturbance': 1, 'well': 1, 'model': 1, 'uncertainty': 1, 'inaccuracy': 1}}, {'outer_idx': 22, 'inner_idx': 2, 'frequency_of_word': {'order': 1, 'deal': 1, 'tradeoff': 1, 'robustness': 2, 'control': 2, 'performance': 1, 'resulted': 1, 'conventional': 1, 'method': 1, 'proposed': 1, 'controller': 2, 'featured': 1, 'two': 2, 'dof': 1, 'structure': 1, 'present': 1, 'italic': 4, 'h': 2, 'sub': 4, 'nominal': 1, 'plant': 1, 'complement': 1, 'additional': 1, 'regulator': 1, 'designed': 1, 'sense': 1, 'assure': 1}}, {'outer_idx': 22, 'inner_idx': 3, 'frequency_of_word': {'unlike': 1, 'traditional': 1, 'italic': 10, 'h': 5, 'sub': 10, 'mixed': 1, 'two': 2, 'and': 1, 'controller': 1, 'regulation': 1, 'conducted': 1, 'online': 1, 'accordance': 1, 'monitored': 1, 'modeling': 1, 'mismatch': 1, 'way': 1, 'adversely': 1, 'degrade': 1, 'performance': 2, 'delivered': 1, 'control': 2, 'hence': 1, 'improving': 1, 'conservativeness': 1, 'total': 1, 'leading': 1, 'significant': 1, 'improvement': 1, 'tracking': 1, 'accuracy': 1, 'response': 1, 'time': 2}}, {'outer_idx': 22, 'inner_idx': 4, 'frequency_of_word': {'numerical': 1, 'simulation': 1, 'experiment': 1, 'spherical': 1, 'motor': 1, 'testbed': 1, 'conducted': 1, 'validate': 1, 'superior': 1, 'performance': 1, 'proposed': 1, 'controller': 1, 'versus': 1, 'conventional': 1, 'control': 1, 'method': 1}}, {'outer_idx': 23, 'inner_idx': 0, 'frequency_of_word': {'investigating': 1, 'effects': 1, 'synchronized': 1, 'visuo': 1, 'tactile': 1, 'stimuli': 1, 'inducing': 1, 'kinesthetic': 1, 'illusion': 1, 'observational': 1, 'learning': 1, 'whole': 1, 'body': 1, 'movements': 1, 'skill': 1, 'improvement': 1, 'sport': 1, 'essential': 1, 'factor': 1, 'keeping': 1, 'motivation': 1, 'continue': 1}}, {'outer_idx': 23, 'inner_idx': 1, 'frequency_of_word': {'previous': 1, 'study': 1, 'showed': 1, 'kinesthetic': 1, 'illusion': 1, 'enhances': 1, 'observational': 1, 'learning': 1}}, {'outer_idx': 23, 'inner_idx': 2, 'frequency_of_word': {'however': 1, 'study': 1, 'dealt': 1, 'learning': 2, 'movement': 2, 'using': 1, 'single': 1, 'part': 1, 'body': 2, 'whether': 1, 'kinesthetic': 1, 'illusion': 1, 'induced': 1, 'observational': 1, 'whole': 1, 'clarified': 1}}, {'outer_idx': 23, 'inner_idx': 3, 'frequency_of_word': {'study': 1, 'conducted': 1, 'experiment': 1, 'involving': 1, 'human': 1, 'subject': 1, 'confirmed': 1, 'synchronized': 1, 'visuo': 1, 'tactile': 1, 'stimulus': 1, 'induce': 1, 'kinesthetic': 1, 'illusion': 1, 'even': 1, 'whole': 1, 'body': 1, 'movement': 1}}, {'outer_idx': 23, 'inner_idx': 4, 'frequency_of_word': {'moreover': 1, 'also': 1, 'demonstrated': 1, 'complete': 1, 'mediation': 1, 'model': 1, 'synchronization': 1, 'visuo': 1, 'tactile': 1, 'stimulus': 1, 'influence': 1, 'kinesthetic': 1, 'illusion': 1, 'mediated': 1, 'body': 1, 'ownership': 1}}, {'outer_idx': 24, 'inner_idx': 0, 'frequency_of_word': {'imu': 1, 'smartphone': 1, 'camera': 1, 'fusion': 1, 'knee': 2, 'adduction': 1, 'flexion': 1, 'moment': 1, 'estimation': 1, 'walking': 1, 'wearable': 1, 'sensing': 1, 'computer': 1, 'vision': 1, 'could': 1, 'move': 1, 'biomechanics': 1, 'specialized': 1, 'laboratory': 1, 'natural': 1, 'environment': 1, 'better': 1, 'algorithm': 1, 'needed': 1, 'extract': 1, 'meaningful': 1, 'outcome': 1, 'emerging': 1, 'modality': 1}}, {'outer_idx': 24, 'inner_idx': 1, 'frequency_of_word': {'article': 1, 'present': 1, 'new': 1, 'model': 1, 'estimating': 1, 'biomechanical': 1, 'outcomesthe': 1, 'knee': 2, 'adduction': 1, 'moment': 2, 'kam': 1, 'flexion': 1, 'kfm': 1, 'from': 1, 'fusion': 1, 'smartphone': 1, 'camera': 1, 'wearable': 1, 'inertial': 1, 'measurement': 1, 'unit': 1, 'imus': 1, 'among': 1, 'young': 1, 'healthy': 1, 'nonobese': 1, 'male': 1}}, {'outer_idx': 24, 'inner_idx': 2, 'frequency_of_word': {'deep': 1, 'learning': 1, 'model': 1, 'developed': 1, 'extract': 1, 'feature': 1, 'fuse': 1, 'multimodal': 1, 'data': 1, 'estimate': 1, 'kam': 1, 'kfm': 1}}, {'outer_idx': 24, 'inner_idx': 3, 'frequency_of_word': {'walking': 1, 'data': 1, 'seventeen': 1, 'subject': 1, 'recorded': 1, 'eight': 1, 'imus': 1, 'two': 1, 'smartphone': 1, 'camera': 1}}, {'outer_idx': 24, 'inner_idx': 4, 'frequency_of_word': {'model': 1, 'used': 1, 'imu': 1, 'camera': 2, 'fusion': 1, 'significantly': 1, 'accurate': 1, 'using': 1, 'imus': 1, 'alone': 1}}, {'outer_idx': 24, 'inner_idx': 5, 'frequency_of_word': {'root': 1, 'meansquare': 1, 'error': 1, 'fusion': 1, 'model': 1, 'zerofortynine': 1, 'inline': 4, 'formula': 4, 'tex': 4, 'math': 4, 'notation': 2, 'latex': 2, 'mathbf': 4, 'bw': 2, 'cdot': 2, 'bh': 2, 'kam': 1, 'zerosixtysix': 1, 'kfm': 1, 'estimation': 1, 'lower': 1, 'clinically': 1, 'significant': 1, 'threshold': 1}}, {'outer_idx': 24, 'inner_idx': 6, 'frequency_of_word': {'larger': 1, 'diverse': 1, 'data': 1, 'model': 1, 'could': 1, 'enable': 1, 'assessment': 1, 'knee': 1, 'moment': 1, 'clinic': 1, 'home': 1}}, {'outer_idx': 25, 'inner_idx': 0, 'frequency_of_word': {'mfra': 1, 'max': 1, 'flowbased': 1, 'routing': 1, 'future': 1, 'interplanetary': 1, 'networks': 1, 'artificial': 1, 'satellite': 1, 'space': 2, 'station': 1, 'lander': 1, 'rover': 1, 'continuously': 1, 'deployed': 1, 'deep': 1, 'explore': 1, 'planet': 1, 'potential': 1, 'resource': 1, 'solar': 1, 'system': 1}}, {'outer_idx': 25, 'inner_idx': 1, 'frequency_of_word': {'data': 1, 'transmission': 1, 'deep': 1, 'space': 1, 'therefore': 1, 'prescheduled': 1, 'timebandwidth': 1, 'specific': 1, 'communication': 1, 'exists': 1, 'now': 1}}, {'outer_idx': 25, 'inner_idx': 2, 'frequency_of_word': {'number': 1, 'source': 1, 'deep': 1, 'space': 1, 'may': 1, 'simultaneously': 1, 'transmit': 1, 'vast': 1, 'amount': 1, 'sensitive': 1, 'data': 1, 'earth': 1, 'station': 1, 'destination': 1, 'using': 1, 'limited': 1, 'bandwidth': 1, 'multiple': 1, 'hop': 1}}, {'outer_idx': 25, 'inner_idx': 3, 'frequency_of_word': {'article': 1, 'using': 1, 'brute': 1, 'force': 1, 'approach': 1, 'first': 1, 'show': 1, 'computing': 1, 'maximum': 1, 'flow': 1, 'node': 1, 'deep': 1, 'space': 1, 'network': 1, 'superexponential': 1, 'nature': 1}}, {'outer_idx': 25, 'inner_idx': 4, 'frequency_of_word': {'evolve': 1, 'number': 1, 'pruning': 1, 'technique': 1, 'reduce': 1, 'search': 1, 'space': 2, 'complex': 1, 'augmented': 1, 'deep': 1, 'network': 1, 'trivial': 1, 'case': 1, 'maximum': 1, 'flow': 1, 'and': 1, 'corresponding': 1, 'routing': 1, 'may': 1, 'easily': 1, 'derived': 1}}, {'outer_idx': 25, 'inner_idx': 5, 'frequency_of_word': {'case': 1, 'possible': 1, 'reduce': 1, 'heuristic': 1, 'developed': 1, 'find': 1, 'good': 1, 'solution': 1, 'maximizing': 1, 'data': 1, 'flow': 1}}, {'outer_idx': 25, 'inner_idx': 6, 'frequency_of_word': {'finally': 1, 'give': 1, 'comparative': 1, 'simulation': 1, 'study': 1, 'analysis': 1, 'proposed': 1, 'technique': 1, 'standard': 1, 'contact': 1, 'graph': 1, 'routing': 1, 'cgr': 1, 'protocol': 1}}, {'outer_idx': 25, 'inner_idx': 7, 'frequency_of_word': {'algorithm': 1, 'outperforms': 1, 'cgr': 1, 'significant': 1, 'margin': 1, 'tested': 1, 'different': 1, 'network': 1, 'topology': 1, 'various': 1, 'traffic': 1, 'generation': 1, 'rate': 1, 'source': 1}}, {'outer_idx': 26, 'inner_idx': 0, 'frequency_of_word': {'embodied': 1, 'virtual': 1, 'interactions': 1, 'equity': 1, 'mean': 1, 'you': 1}}, {'outer_idx': 26, 'inner_idx': 1, 'frequency_of_word': {'preliminary': 1, 'results': 1, 'impact': 1, 'transgender': 1, 'avatar': 1, 'embodiment': 1, 'empathy': 2, 'embodied': 1, 'virtual': 1, 'interaction': 1, 'contribute': 1, 'immersive': 1, 'perspective': 1, 'taking': 1, 'experience': 1, 'turn': 1, 'increase': 1, 'affiliation': 1}}, {'outer_idx': 26, 'inner_idx': 2, 'frequency_of_word': {'study': 1, 'sought': 1, 'investigate': 1, 'outcome': 1, 'context': 1, 'unconscious': 1, 'bias': 1, 'related': 1, 'gender': 1, 'identity': 1, 'interpersonal': 1, 'interaction': 1}}, {'outer_idx': 26, 'inner_idx': 3, 'frequency_of_word': {'conducted': 1, 'simulated': 1, 'interview': 1, 'virtual': 1, 'reality': 1, 'participant': 1, 'embodied': 1, 'transgender': 2, 'cisgender': 1, 'avatar': 1, 'interacted': 1, 'human': 1, 'controlled': 1, 'agent': 1, 'woman': 1}}, {'outer_idx': 26, 'inner_idx': 4, 'frequency_of_word': {'preliminary': 1, 'result': 1, 'reveal': 1, 'difference': 1, 'woman': 1, 'men': 1, 'experience': 1, 'empathy': 1, 'emotional': 1, 'state': 1, 'embodied': 1, 'transgender': 1, 'avatar': 1}}, {'outer_idx': 27, 'inner_idx': 0, 'frequency_of_word': {'eco': 1, 'driving': 1, 'scientometric': 1, 'bibliometric': 1, 'analysis': 1, 'fuel': 2, 'efficiency': 1, 'transportation': 1, 'sector': 1, 'become': 1, 'key': 1, 'factor': 1, 'reduce': 1, 'greenhouse': 1, 'gas': 1, 'emission': 1, 'consumption': 1, 'response': 1, 'negative': 1, 'impact': 1, 'global': 1, 'warming': 1}}, {'outer_idx': 27, 'inner_idx': 1, 'frequency_of_word': {'approach': 1, 'energy': 1, 'saving': 1, 'environmental': 1, 'sustainability': 1, 'eco': 1, 'driving': 1, 'attracted': 1, 'considerable': 1, 'research': 1, 'interest': 1, 'past': 1, 'decade': 1}}, {'outer_idx': 27, 'inner_idx': 2, 'frequency_of_word': {'review': 2, 'aim': 1, 'provide': 1, 'comprehensive': 1, 'research': 1, 'eco': 1, 'driving': 1, 'using': 1, 'methodology': 1, 'literature': 1, 'bibliometrics': 1, 'content': 1, 'analysis': 1, 'vosviewer': 1, 'software': 1}}, {'outer_idx': 27, 'inner_idx': 3, 'frequency_of_word': {'following': 1, 'keywords': 1, 'ecological': 5, 'driving': 2, 'routing': 2, 'bus': 2, 'car': 2, 'vehicle': 2, 'eco': 6, 'driver': 1, 'used': 1, 'paper': 1, 'retrieval': 1}}, {'outer_idx': 27, 'inner_idx': 4, 'frequency_of_word': {'query': 1, 'conducted': 1, 'january': 1, 'twenty': 1, 'two': 1, 'thousand': 1, 'and': 1, 'twentyone': 1}}, {'outer_idx': 27, 'inner_idx': 5, 'frequency_of_word': {'result': 1, 'take': 1, 'account': 1, 'journal': 1, 'article': 1, 'proceeding': 1, 'paper': 1, 'review': 1, 'without': 1, 'time': 1, 'limitation': 1}}, {'outer_idx': 27, 'inner_idx': 6, 'frequency_of_word': {'finally': 1, 'total': 2, 'seven': 1, 'hundred': 1, 'and': 3, 'sixtyseven': 1, 'document': 1, 'retrieved': 1, 'publication': 1, 'viewed': 1, 'period': 1, 'two': 1, 'thousand': 2, 'onetwo': 1, 'twenty': 1, 'based': 1, 'web': 1, 'science': 1, 'wos': 1, 'core': 1, 'collection': 1, 'database': 1}}, {'outer_idx': 27, 'inner_idx': 7, 'frequency_of_word': {'publication': 1, 'year': 1, 'leading': 4, 'country': 1, 'source': 1, 'institution': 1, 'author': 1, 'document': 2, 'citation': 2, 'co': 1, 'analyzed': 1, 'explore': 1, 'primary': 1, 'trend': 1}}, {'outer_idx': 27, 'inner_idx': 8, 'frequency_of_word': {'in': 1, 'depth': 1, 'analysis': 1, 'reveals': 1, 'five': 2, 'cluster': 1, 'keywords': 1, 'review': 1, 'relevant': 1, 'study': 1, 'eco': 2, 'driving': 2, 'different': 1, 'perspective': 1, 'carried': 1, 'identify': 1, 'potential': 1, 'trend': 1, 'future': 1, 'research': 1, 'hot': 1, 'spot': 1}}, {'outer_idx': 28, 'inner_idx': 0, 'frequency_of_word': {'variational': 1, 'bayesian': 1, 'gaussian': 1, 'mixture': 1, 'nonnegative': 2, 'matrix': 2, 'factorization': 2, 'model': 1, 'extract': 1, 'movement': 1, 'primitives': 1, 'robust': 1, 'control': 1, 'nmf': 1, 'powerful': 1, 'tool': 1, 'parameter': 1, 'estimation': 1, 'applied': 1, 'numerous': 1, 'robotics': 1, 'application': 1, 'path': 1, 'planning': 1, 'motion': 2, 'trajectory': 1, 'prediction': 1, 'intention': 1, 'detection': 1}}, {'outer_idx': 28, 'inner_idx': 1, 'frequency_of_word': {'particular': 1, 'nmf': 1, 'successfully': 1, 'used': 1, 'extract': 1, 'simplified': 1, 'organized': 1, 'movement': 1, 'primitive': 1, 'myoelectric': 1, 'signal': 1, 'mes': 1, 'robust': 1, 'control': 1, 'multi': 1, 'degree': 1, 'freedom': 1, 'humanoid': 1, 'robot': 1}}, {'outer_idx': 28, 'inner_idx': 2, 'frequency_of_word': {'however': 1, 'mes': 1, 'typically': 1, 'contaminated': 1, 'complex': 1, 'noise': 1, 'source': 1}}, {'outer_idx': 28, 'inner_idx': 3, 'frequency_of_word': {'system': 1, 'performance': 1, 'often': 1, 'degrades': 1, 'due': 1, 'simplified': 1, 'gaussian': 1, 'assumption': 1, 'noise': 1, 'distribution': 1, 'existing': 1, 'nmf': 1, 'method': 1}}, {'outer_idx': 28, 'inner_idx': 4, 'frequency_of_word': {'furthermore': 1, 'existing': 1, 'nmf': 1, 'model': 1, 'unable': 1, 'automatically': 1, 'determine': 1, 'rank': 1, 'latent': 1, 'matrix': 1}}, {'outer_idx': 28, 'inner_idx': 5, 'frequency_of_word': {'address': 1, 'issue': 1, 'article': 1, 'present': 1, 'hybrid': 1, 'variational': 1, 'bayesian': 1, 'gaussian': 2, 'mixture': 2, 'nmf': 1, 'gmnmf': 1, 'model': 2, 'finite': 1, 'adopted': 1, 'fit': 1, 'mixed': 1, 'noise': 1, 'density': 1, 'function': 1, 'mes': 1}}, {'outer_idx': 28, 'inner_idx': 6, 'frequency_of_word': {'addition': 1, 'automatic': 1, 'relevant': 1, 'determination': 1, 'criterion': 1, 'applied': 1, 'automatically': 1, 'infer': 1, 'number': 1, 'movement': 1, 'primitive': 1}}, {'outer_idx': 28, 'inner_idx': 7, 'frequency_of_word': {'coordinate': 1, 'descent': 1, 'update': 1, 'rule': 1, 'proposed': 1, 'model': 1, 'formulated': 1, 'mean': 1, 'field': 1, 'variational': 1, 'bayesian': 1, 'inference': 1}}, {'outer_idx': 28, 'inner_idx': 8, 'frequency_of_word': {'ass': 1, 'model': 1, 'performance': 1, 'five': 1, 'synthetic': 1, 'noise': 1, 'distribution': 1, 'function': 1, 'experimental': 1, 'mes': 1, 'dataset': 1, 'perform': 1, 'six': 1, 'wrist': 1, 'movement': 1}}, {'outer_idx': 28, 'inner_idx': 9, 'frequency_of_word': {'result': 1, 'demonstrate': 1, 'gmnmf': 1, 'yield': 1, 'low': 1, 'error': 1, 'high': 1, 'robustness': 1, 'extracting': 1, 'movement': 1, 'primitive': 1, 'four': 1, 'competitive': 1, 'method': 1, 'robust': 1, 'cybernetic': 1, 'control': 1}}, {'outer_idx': 29, 'inner_idx': 0, 'frequency_of_word': {'fast': 1, 'actuation': 1, 'mechanism': 1, 'energy': 1, 'effective': 1, 'driving': 1, 'method': 1, 'pulse': 1, 'closers': 1, 'recloser': 1, 'detects': 1, 'fault': 2, 'current': 1, 'electric': 1, 'power': 1, 'distribution': 1, 'line': 3, 'break': 1, 'restores': 1, 'several': 1, 'test': 1}}, {'outer_idx': 29, 'inner_idx': 1, 'frequency_of_word': {'check': 1, 'whether': 1, 'fault': 1, 'remains': 1, 'recloser': 1, 'performs': 1, 'co': 1, 'operation': 1, 'opening': 1, 'contact': 1, 'reclosing': 1, 'while': 1}}, {'outer_idx': 29, 'inner_idx': 2, 'frequency_of_word': {'operation': 1, 'fault': 1, 'current': 1, 'flow': 1, 'line': 2, 'potentially': 1, 'causing': 1, 'damage': 1, 'equipment': 1, 'connected': 1}}, {'outer_idx': 29, 'inner_idx': 3, 'frequency_of_word': {'therefore': 1, 'reducing': 1, 'current': 1, 'carrying': 1, 'time': 1, 'fault': 1, 'test': 1, 'desirable': 1, 'reduction': 1, 'difficult': 1, 'slow': 1, 'actuation': 1, 'mechanism': 1, 'reclosers': 1}}, {'outer_idx': 29, 'inner_idx': 4, 'frequency_of_word': {'article': 1, 'novel': 1, 'actuation': 1, 'mechanism': 1, 'combining': 1, 'permanent': 1, 'magnetic': 1, 'actuator': 2, 'pma': 1, 'solenoid': 1, 'sa': 1, 'proposed': 1}}, {'outer_idx': 29, 'inner_idx': 5, 'frequency_of_word': {'co': 1, 'operation': 1, 'fault': 1, 'test': 1, 'proposed': 1, 'mechanism': 1, 'rapidly': 1, 'break': 1, 'closed': 1, 'contact': 2, 'hitting': 1, 'moving': 1, 'sa': 1}}, {'outer_idx': 29, 'inner_idx': 6, 'frequency_of_word': {'increase': 1, 'energy': 1, 'efficiency': 1, 'actuation': 1, 'mechanism': 1, 'distinctive': 1, 'pma': 2, 'driving': 1, 'method': 1, 'also': 1, 'proposed': 1, 'weakens': 1, 'strong': 1, 'holding': 1, 'force': 2, 'using': 1, 'assistive': 1, 'magneto': 1, 'motive': 1, 'mmf': 1}}, {'outer_idx': 29, 'inner_idx': 7, 'frequency_of_word': {'structure': 1, 'configuration': 1, 'proposed': 1, 'mechanism': 1, 'explained': 1}}, {'outer_idx': 29, 'inner_idx': 8, 'frequency_of_word': {'performance': 1, 'mechanism': 1, 'effectiveness': 1, 'driving': 1, 'method': 1, 'studied': 1, 'cosimulations': 1}}, {'outer_idx': 29, 'inner_idx': 9, 'frequency_of_word': {'finally': 1, 'prototype': 1, 'developed': 1, 'practicality': 1, 'verified': 1, 'experiment': 1}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find unique words in the 'sentences' list and print the count of unique words.\n",
        "def unik_word(sentences):\n",
        "    unik = []\n",
        "    for outer_sentences in sentences:\n",
        "        for sent in outer_sentences:\n",
        "            words = word_tokenize(sent)\n",
        "            for word in words:\n",
        "                if word not in unik:\n",
        "                    unik.append(word)\n",
        "    return unik\n",
        "\n",
        "unik_words = unik_word(sentences)\n",
        "print(len(unik_words))"
      ],
      "metadata": {
        "id": "RD5BjzigleE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa8902e-cb96-49e0-ba5c-49cd78dfe423"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the 'lil_matrix' and 'csr_matrix' classes from 'scipy.sparse' and the 'numpy' library as 'np'.\n",
        "from scipy.sparse import lil_matrix, csr_matrix\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3lcHbygzFJvS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation of TF Values"
      ],
      "metadata": {
        "id": "o3ugSQF12yy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate TF (Term Frequency) values for each word in each sentence using the provided information and store the results in 'tf_values'. Then, print 'tf_values'.\n",
        "def tf_calculation(sent_info, freq_info, unik_words):\n",
        "    tf_values = lil_matrix((len(sent_info), len(unik_words)), dtype=np.float32)\n",
        "\n",
        "    for i, freq_info_dict in enumerate(freq_info):\n",
        "        for word, freq in freq_info_dict[\"frequency_of_word\"].items():\n",
        "            tf = freq / sent_info[i][\"sent_word_count\"]\n",
        "            tf_values[i, unik_words.index(word)] = tf\n",
        "\n",
        "    return tf_values.tocsr()\n",
        "\n",
        "tf_values=tf_calculation(sent_info,freq_info,unik_words)\n",
        "print(tf_values)"
      ],
      "metadata": {
        "id": "4iXBNd-kFKck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb53c26-c255-4f22-b3b3-af043be76c70"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t0.041666668\n",
            "  (0, 1)\t0.041666668\n",
            "  (0, 2)\t0.041666668\n",
            "  (0, 3)\t0.041666668\n",
            "  (0, 4)\t0.041666668\n",
            "  (0, 5)\t0.083333336\n",
            "  (0, 6)\t0.083333336\n",
            "  (0, 7)\t0.041666668\n",
            "  (0, 8)\t0.041666668\n",
            "  (0, 9)\t0.041666668\n",
            "  (0, 10)\t0.041666668\n",
            "  (0, 11)\t0.041666668\n",
            "  (0, 12)\t0.083333336\n",
            "  (0, 13)\t0.041666668\n",
            "  (0, 14)\t0.041666668\n",
            "  (0, 15)\t0.041666668\n",
            "  (0, 16)\t0.041666668\n",
            "  (0, 17)\t0.041666668\n",
            "  (0, 18)\t0.041666668\n",
            "  (0, 19)\t0.041666668\n",
            "  (0, 20)\t0.041666668\n",
            "  (1, 3)\t0.045454547\n",
            "  (1, 7)\t0.045454547\n",
            "  (1, 8)\t0.045454547\n",
            "  (1, 9)\t0.045454547\n",
            "  :\t:\n",
            "  (211, 1654)\t0.045454547\n",
            "  (211, 1655)\t0.045454547\n",
            "  (211, 1656)\t0.045454547\n",
            "  (211, 1657)\t0.045454547\n",
            "  (211, 1658)\t0.045454547\n",
            "  (211, 1659)\t0.045454547\n",
            "  (211, 1660)\t0.045454547\n",
            "  (212, 200)\t0.2\n",
            "  (212, 330)\t0.2\n",
            "  (212, 366)\t0.2\n",
            "  (212, 633)\t0.2\n",
            "  (212, 1661)\t0.2\n",
            "  (213, 112)\t0.14285715\n",
            "  (213, 200)\t0.14285715\n",
            "  (213, 329)\t0.14285715\n",
            "  (213, 355)\t0.14285715\n",
            "  (213, 390)\t0.14285715\n",
            "  (213, 1059)\t0.14285715\n",
            "  (213, 1662)\t0.14285715\n",
            "  (214, 151)\t0.16666667\n",
            "  (214, 249)\t0.16666667\n",
            "  (214, 354)\t0.16666667\n",
            "  (214, 379)\t0.16666667\n",
            "  (214, 542)\t0.16666667\n",
            "  (214, 694)\t0.16666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation of IDF Values"
      ],
      "metadata": {
        "id": "OlotVTD724gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate IDF (Inverse Document Frequency) values for each word in each sentence using the provided information and store the results in 'idf_values'. Then, print 'idf_values'.\n",
        "def idf_calculate(sent_info, freq_info, unik_words):\n",
        "    idf_values = lil_matrix((len(sent_info), len(unik_words)), dtype=np.float32)\n",
        "\n",
        "    num_sentences = len(sent_info)\n",
        "    word_sum_dict = {word: sum(freq_info_dict[\"frequency_of_word\"].get(word, 0) for freq_info_dict in freq_info) for word in unik_words}\n",
        "\n",
        "    for i, freq_info_dict in enumerate(freq_info):\n",
        "        for word in freq_info_dict[\"frequency_of_word\"]:\n",
        "            idf = math.log(num_sentences / (1 + word_sum_dict[word]))\n",
        "            idf_values[i, unik_words.index(word)] = idf\n",
        "\n",
        "    return idf_values.tocsr()\n",
        "\n",
        "idf_values=idf_calculate(sent_info,freq_info,unik_words)\n",
        "print(idf_values)"
      ],
      "metadata": {
        "id": "ffUARr0CFWUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdf1dac-b0cb-449e-82d0-a0d10880fa41"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t3.7612002\n",
            "  (0, 1)\t4.6774907\n",
            "  (0, 2)\t4.2720256\n",
            "  (0, 3)\t2.8056886\n",
            "  (0, 4)\t4.6774907\n",
            "  (0, 5)\t3.7612002\n",
            "  (0, 6)\t3.9843438\n",
            "  (0, 7)\t3.9843438\n",
            "  (0, 8)\t4.2720256\n",
            "  (0, 9)\t2.7315807\n",
            "  (0, 10)\t4.2720256\n",
            "  (0, 11)\t4.6774907\n",
            "  (0, 12)\t2.7315807\n",
            "  (0, 13)\t4.6774907\n",
            "  (0, 14)\t3.7612002\n",
            "  (0, 15)\t3.2911966\n",
            "  (0, 16)\t4.6774907\n",
            "  (0, 17)\t3.424728\n",
            "  (0, 18)\t3.5788786\n",
            "  (0, 19)\t3.424728\n",
            "  (0, 20)\t2.4802663\n",
            "  (1, 3)\t2.8056886\n",
            "  (1, 7)\t3.9843438\n",
            "  (1, 8)\t4.2720256\n",
            "  (1, 9)\t2.7315807\n",
            "  :\t:\n",
            "  (211, 1654)\t4.6774907\n",
            "  (211, 1655)\t4.6774907\n",
            "  (211, 1656)\t4.6774907\n",
            "  (211, 1657)\t4.6774907\n",
            "  (211, 1658)\t4.6774907\n",
            "  (211, 1659)\t4.6774907\n",
            "  (211, 1660)\t4.6774907\n",
            "  (212, 200)\t3.068053\n",
            "  (212, 330)\t2.3749058\n",
            "  (212, 366)\t3.1734135\n",
            "  (212, 633)\t3.424728\n",
            "  (212, 1661)\t4.6774907\n",
            "  (213, 112)\t2.426199\n",
            "  (213, 200)\t3.068053\n",
            "  (213, 329)\t2.3749058\n",
            "  (213, 355)\t3.9843438\n",
            "  (213, 390)\t3.9843438\n",
            "  (213, 1059)\t2.8056886\n",
            "  (213, 1662)\t4.6774907\n",
            "  (214, 151)\t3.068053\n",
            "  (214, 249)\t4.2720256\n",
            "  (214, 354)\t3.1734135\n",
            "  (214, 379)\t3.068053\n",
            "  (214, 542)\t4.2720256\n",
            "  (214, 694)\t3.7612002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation of TF-IDF Values"
      ],
      "metadata": {
        "id": "APUlEFJi3Da8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate TF-IDF (Term Frequency-Inverse Document Frequency) values by multiplying TF and IDF matrices and store the results in 'tfidf_values'. Then, print 'tfidf_values'.\n",
        "def tfidf_calculation(tf_values, idf_values):\n",
        "    tfidf_values = tf_values.multiply(idf_values)\n",
        "    return tfidf_values\n",
        "\n",
        "tfidf_values=tfidf_calculation(tf_values,idf_values)\n",
        "print(tfidf_values)"
      ],
      "metadata": {
        "id": "2uVhfYlMFf8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648246bd-a663-4a32-d395-51b0d78de821"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t0.15671667\n",
            "  (0, 1)\t0.19489545\n",
            "  (0, 2)\t0.17800108\n",
            "  (0, 3)\t0.11690369\n",
            "  (0, 4)\t0.19489545\n",
            "  (0, 5)\t0.31343335\n",
            "  (0, 6)\t0.33202866\n",
            "  (0, 7)\t0.16601433\n",
            "  (0, 8)\t0.17800108\n",
            "  (0, 9)\t0.11381587\n",
            "  (0, 10)\t0.17800108\n",
            "  (0, 11)\t0.19489545\n",
            "  (0, 12)\t0.22763173\n",
            "  (0, 13)\t0.19489545\n",
            "  (0, 14)\t0.15671667\n",
            "  (0, 15)\t0.1371332\n",
            "  (0, 16)\t0.19489545\n",
            "  (0, 17)\t0.142697\n",
            "  (0, 18)\t0.14911994\n",
            "  (0, 19)\t0.142697\n",
            "  (0, 20)\t0.10334443\n",
            "  (1, 3)\t0.1275313\n",
            "  (1, 7)\t0.18110654\n",
            "  (1, 8)\t0.19418299\n",
            "  (1, 9)\t0.12416276\n",
            "  :\t:\n",
            "  (211, 1654)\t0.21261322\n",
            "  (211, 1655)\t0.21261322\n",
            "  (211, 1656)\t0.21261322\n",
            "  (211, 1657)\t0.21261322\n",
            "  (211, 1658)\t0.21261322\n",
            "  (211, 1659)\t0.21261322\n",
            "  (211, 1660)\t0.21261322\n",
            "  (212, 200)\t0.6136106\n",
            "  (212, 330)\t0.47498116\n",
            "  (212, 366)\t0.6346827\n",
            "  (212, 633)\t0.6849456\n",
            "  (212, 1661)\t0.9354982\n",
            "  (213, 112)\t0.34659988\n",
            "  (213, 200)\t0.4382933\n",
            "  (213, 329)\t0.3392723\n",
            "  (213, 355)\t0.569192\n",
            "  (213, 390)\t0.569192\n",
            "  (213, 1059)\t0.4008127\n",
            "  (213, 1662)\t0.668213\n",
            "  (214, 151)\t0.51134217\n",
            "  (214, 249)\t0.7120043\n",
            "  (214, 354)\t0.5289023\n",
            "  (214, 379)\t0.51134217\n",
            "  (214, 542)\t0.7120043\n",
            "  (214, 694)\t0.6268667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def tf_calculation(sent_info,freq_info):\n",
        "#   tf_values=np.zeros(shape=(len(sent_info),len(unik_words)))\n",
        "#   for i,freq_info_dict in enumerate(freq_info):\n",
        "#     for word in freq_info_dict[\"frequency_of_word\"].keys():\n",
        "#       tf=freq_info_dict[\"frequency_of_word\"][word]/sent_info[i][\"sent_word_count\"]\n",
        "#       tf_values[i][unik_words.index(word)]=tf\n",
        "#   return tf_values\n",
        "\n",
        "\n",
        "# tf_values=tf_calculation(sent_info,freq_info)\n",
        "# print(tf_values)"
      ],
      "metadata": {
        "id": "9F_Xbf-ilyKb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "\n",
        "# def idf_calculate(sent_info,freq_info):\n",
        "#   idf_values=np.zeros(shape=(len(sent_info),len(unik_words)))\n",
        "#   for i,freq_info_dict in enumerate(freq_info):\n",
        "#     for word in freq_info_dict[\"frequency_of_word\"].keys():\n",
        "#       sum=0\n",
        "#       for word_in_dict in freq_info:\n",
        "#         if word in set(word_in_dict[\"frequency_of_word\"].keys()):\n",
        "#           sum=sum+word_in_dict[\"frequency_of_word\"][word]\n",
        "#       idf_values[i][unik_words.index(word)]=math.log(len(sentences)/sum)\n",
        "#   return idf_values\n",
        "\n",
        "# idf_values=idf_calculate(sent_info,freq_info)\n",
        "# print(idf_values)"
      ],
      "metadata": {
        "id": "XWwmCLzol8nP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def tfidf_calculation(tf_values,idf_values):\n",
        "#   tfidf_values=np.zeros(shape=(len(sent_info),len(unik_words)))\n",
        "#   tfidf_values=np.multiply(tf_values,idf_values)\n",
        "#   return tfidf_values\n",
        "\n",
        "# tfidf_values=tfidf_calculation(tf_values,idf_values)\n",
        "# print(tfidf_values)\n"
      ],
      "metadata": {
        "id": "Wh2pld3fl99K"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the shape (dimensions) of the TF-IDF values matrix.\n",
        "print(tfidf_values.shape)"
      ],
      "metadata": {
        "id": "HT-M_ldMmCIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7c8941-16ca-4996-97be-242453405c7b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(215, 1663)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Context Matrix using tf-idf based vector modeling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ii41A-Xl6oox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the TF-IDF values matrix 'tfidf_values' to a dense NumPy array and store it in 'context_matrix'.\n",
        "context_matrix = tfidf_values.toarray()"
      ],
      "metadata": {
        "id": "7relB1bHnOvf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "mOD-YKWfuWRO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the 'device' to use GPU (\"cuda\") if available; otherwise, use CPU (\"cpu\").\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "4wAjiAe7ucdN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "word_freq = defaultdict(int)\n",
        "for sentence in sentences:\n",
        "    for token in sentence:\n",
        "        word_freq[token] += 1\n",
        "\n",
        "min_word_frequency = 1\n",
        "filtered_word_freq = {word: freq for word, freq in word_freq.items() if freq >= min_word_frequency}\n",
        "# Sort words by frequency in descending order\n",
        "sorted_word_freq = dict(sorted(filtered_word_freq.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Create the vocabulary based on the sorted frequencies\n",
        "vocabulary = list(sorted_word_freq.keys())\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "total_word_count = sum(filtered_word_freq.values())\n",
        "word_probabilities = [freq / total_word_count for freq in sorted_word_freq.values()]\n",
        "\n",
        "# Convert word probabilities to a tensor\n",
        "word_probabilities_tensor = torch.Tensor(word_probabilities).to(device)\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocabulary_size = len(vocabulary)"
      ],
      "metadata": {
        "id": "lLb4zQ3Ct9dJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display vocabulary size\n",
        "vocabulary_size"
      ],
      "metadata": {
        "id": "9KFOuMECueVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190af845-6197-4f3f-8a48-527de5ae9de1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skipgram Word Embeddings"
      ],
      "metadata": {
        "id": "Bv6-JMT05-pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a SkipGramBatchModel class for skip-gram word embeddings using PyTorch.\n",
        "class SkipGramBatchModel(nn.Module):\n",
        "    def __init__(self, device, vocabulary_size, embedding_dim, neg_num=5):\n",
        "        super(SkipGramBatchModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.neg_num = neg_num\n",
        "        self.target_embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "        self.context_embeddings = nn.Embedding(vocabulary_size, embedding_dim)\n",
        "        self.target_embeddings.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
        "        self.context_embeddings.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)\n",
        "\n",
        "    def forward(self, centers, contexts):\n",
        "        u_embeds = self.target_embeddings(centers)\n",
        "        v_embeds = self.context_embeddings(contexts)\n",
        "        scores = torch.bmm(u_embeds, v_embeds.transpose(1, 2)).squeeze()\n",
        "        return scores\n",
        "    def get_embeddings(self):\n",
        "        return self.target_embeddings.weight.data, self.context_embeddings.weight.data\n"
      ],
      "metadata": {
        "id": "Kad1vbXPuwVX"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate batches of context and target matrices from the context_matrix for training a word embedding model.\n",
        "def generate_batch(context_matrix, batch_size=8):\n",
        "    num_sentences, sentence_length = context_matrix.shape\n",
        "    indices = np.arange(num_sentences)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(0, num_sentences, batch_size):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "        contexts = context_matrix[batch_indices]\n",
        "\n",
        "        # Create the target matrix by shifting the context matrix by one word\n",
        "        targets = np.roll(contexts, shift=-1, axis=1)\n",
        "\n",
        "        yield torch.LongTensor(contexts).to(device), torch.LongTensor(targets).to(device)"
      ],
      "metadata": {
        "id": "W-bAovAvu3JW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the given model using the provided data for a specified number of epochs and return the learned word embeddings.\n",
        "def train_batch(model, data, num_epochs=5, learning_rate=0.025, batch_size=8):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for contexts, targets in generate_batch(data, batch_size=batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            scores = model(contexts, targets)\n",
        "            loss = loss_fn(scores, torch.ones_like(scores))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}\")\n",
        "    return model.get_embeddings()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cv9dPpMLu7Lz"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train a SkipGramBatchModel to learn word embeddings using context_matrix data with specific hyperparameters.\n",
        "embedding_dim = 100\n",
        "vocabulary_size = len(vocabulary)\n",
        "model = SkipGramBatchModel(device, vocabulary_size, embedding_dim)\n",
        "\n",
        "word_embeddings, context_embeddings = train_batch(model, context_matrix, num_epochs=10, learning_rate=0.025, batch_size=128)"
      ],
      "metadata": {
        "id": "0Jl8lEVDuox5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143c219f-c6a6-4b92-ee4e-8ec9ba773813"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3813509345054626\n",
            "Epoch 2/10, Loss: 1.227132797241211\n",
            "Epoch 3/10, Loss: 0.9412875771522522\n",
            "Epoch 4/10, Loss: 0.5624028891324997\n",
            "Epoch 5/10, Loss: 0.24632269889116287\n",
            "Epoch 6/10, Loss: 0.07869831286370754\n",
            "Epoch 7/10, Loss: 0.020718513056635857\n",
            "Epoch 8/10, Loss: 0.005279654986225069\n",
            "Epoch 9/10, Loss: 0.0014431329909712076\n",
            "Epoch 10/10, Loss: 0.0004430811677593738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#serialize and save word embeddings and their corresponding dictionary to a binary file.\n",
        "def save_embeddings(filename=\"embeddings.bin\", embeddings=None, dictionary=None):\n",
        "    \"\"\"Embeddings and reverse dictionary serialization and dump to a file.\"\"\"\n",
        "    if embeddings is not None:\n",
        "        # Reshape the embeddings to (sentences * words, embedding_dim)\n",
        "        flattened_embeddings = embeddings.reshape(-1, embeddings.shape[-1])\n",
        "\n",
        "        data = {\n",
        "            'emb': flattened_embeddings,\n",
        "            'dict': dictionary\n",
        "        }\n",
        "\n",
        "        with open(filename, 'wb') as file:\n",
        "            print(\"Saving embeddings to file:\", filename)\n",
        "            pickle.dump(data, file)\n",
        "    else:\n",
        "        print(\"Embeddings are None. Nothing to save.\")\n",
        "\n",
        "# Example usage:\n",
        "save_embeddings(filename=\"batchwise_embeddings.bin\", embeddings=word_embeddings, dictionary=word2idx)"
      ],
      "metadata": {
        "id": "5rEVzetjv1Ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a0f21a-0b89-4a6d-f637-17510415dfdb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving embeddings to file: batchwise_embeddings.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a Word2Vec Class\n"
      ],
      "metadata": {
        "id": "3WbSoJsp45pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "#Word2Vec class for loading and using word embeddings, computing sentence similarity, and generating next sentences.\n",
        "class Word2Vec(object):\n",
        "    def __init__(self):\n",
        "        self.embeddings = None\n",
        "        self.dictionary = None\n",
        "        self.reverse_dictionary = None\n",
        "\n",
        "    def from_file(self, filename):\n",
        "        file = open(filename, 'rb')\n",
        "        data = pickle.load(file)\n",
        "        print(data)\n",
        "        self.embeddings = data['emb']\n",
        "        self.dictionary = data['dict']\n",
        "        self.reverse_dictionary = {v: k for k, v in self.dictionary.items()}\n",
        "\n",
        "    def from_object(self, embeddings, dictionary):\n",
        "        self.embeddings = embeddings\n",
        "        self.dictionary = dictionary\n",
        "        self.reverse_dictionary = {v: k for k, v in dictionary.items()}\n",
        "\n",
        "    def inference(self, sentence):\n",
        "        assert self.embeddings is not None and self.dictionary is not None, \\\n",
        "            'Embeddings not initialized, use from_file or from_object to load data.'\n",
        "        sentence_idx = self.dictionary.get(sentence)\n",
        "        # Unknown sentence returns UNK's sentence_idx\n",
        "        if sentence_idx is None:\n",
        "            sentence_idx = 0\n",
        "        return self.embeddings[sentence_idx]\n",
        "\n",
        "    def similarity(self, sentence1, sentence2):\n",
        "        v1 = self.inference(sentence1)\n",
        "        v2 = self.inference(sentence2)\n",
        "        # Perform cosine similarity using torch\n",
        "        cosine_similarity = torch.nn.functional.cosine_similarity(v1, v2, dim=0)\n",
        "        return cosine_similarity.item()\n",
        "\n",
        "    def most_similar(self, sentence, topk=10):\n",
        "      assert self.embeddings is not None and self.dictionary is not None, \\\n",
        "          'Embeddings not initialized, use from_file or from_object to load data.'\n",
        "\n",
        "      # Get the embedding for the input sentence\n",
        "      input_embedding = self.inference(sentence)\n",
        "\n",
        "      # Calculate cosine similarities between the input sentence and all other sentences\n",
        "      similarities = []\n",
        "      input_norm = np.linalg.norm(input_embedding)  # Calculate the norm of the input embedding\n",
        "      for idx in range(len(self.embeddings)):\n",
        "          if idx != 0:  # Skip the unknown sentence\n",
        "              other_embedding = self.embeddings[idx]\n",
        "              other_norm = np.linalg.norm(other_embedding)  # Calculate the norm of the other embedding\n",
        "              if input_norm != 0 and other_norm != 0:\n",
        "                  similarity = np.dot(input_embedding, other_embedding) / (input_norm * other_norm)\n",
        "                  similarities.append((self.reverse_dictionary.get(idx, \"\"), similarity))\n",
        "\n",
        "      # Sort the sentences by similarity in descending order\n",
        "      similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Get the top-k most similar sentences and their similarities\n",
        "      topk_results = similarities[:topk]\n",
        "\n",
        "      return topk_results\n",
        "\n",
        "    def generate_next_sentence(self, input_sentence):\n",
        "        most_similar_sentences = self.most_similar(input_sentence, topk=10)\n",
        "        if most_similar_sentences:\n",
        "            similar_sentence = most_similar_sentences[0][0]\n",
        "            return similar_sentence.split()  # Split the similar sentence into words\n",
        "        else:\n",
        "            return []\n"
      ],
      "metadata": {
        "id": "fstq_26Swihv"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize a Word2Vec instance 'wv' and load word embeddings from the specified file.\n",
        "wv = Word2Vec()\n",
        "wv.from_file(\"/content/batchwise_embeddings.bin\")\n"
      ],
      "metadata": {
        "id": "assSu4uewjgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc50f6d5-5027-4f60-b4d1-ac636fb6bffa"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'emb': tensor([[ 3.9987e-01, -1.9919e-01, -2.0536e-01,  ...,  2.4661e-01,\n",
            "          3.9974e-01, -6.4620e-02],\n",
            "        [ 2.7921e-01, -1.8047e-01, -6.6184e-02,  ...,  9.4707e-02,\n",
            "          2.8297e-01, -1.2112e-01],\n",
            "        [ 4.6208e-03,  4.3982e-03,  2.2192e-03,  ...,  4.1203e-04,\n",
            "         -1.7552e-03,  4.1399e-03],\n",
            "        ...,\n",
            "        [-1.4523e-03, -5.6460e-04,  9.7506e-04,  ..., -3.4152e-04,\n",
            "         -1.2712e-03, -2.3757e-03],\n",
            "        [-1.9478e-03, -2.9548e-03, -4.7510e-03,  ...,  2.2883e-03,\n",
            "         -2.2760e-03, -4.6729e-03],\n",
            "        [ 4.3906e-03, -3.4374e-04,  3.8812e-03,  ...,  4.6459e-03,\n",
            "          4.1093e-03, -4.4534e-03]]), 'dict': {'complete targets coverage energy harvesting internet things ambient backscatter article considers deriving set cover set active node responsible monitoring target internet things iot network': 0, 'key distinction prior work article considers sensor node aided backscatter communication allow communicate negligible energy cost using ambient radio frequency rf signal': 1, 'article contains three main novelty': 2, 'first present mixed integer linear program milp used compute global optimal solution': 3, 'second also outline centralized greedy scheduling cgs algorithm selects node based energy number covered target': 4, 'third present distributed greedy scheduling dgs algorithm selects node according energy level': 5, 'simulation result show equipping node ambient backscattering capability increase ratio complete target coverage one hundred compared existing technique': 6, 'nonorthogonal multiple access enabled two way relay system using signal alignment conventional two way relay nonorthogonal multiple access twr noma system diversity order equal zero even perfect successive interference cancellation psic': 7, 'article leverage multiple inputmultiple output mimo technique twr noma system extract spatial diversity gain performance enhancement': 8, 'communications two user pair assisted relay': 9, 'precoding vector user well matrix relay specially designed perform signal alignment sa': 10, 'derive analytical closed form expression outage probability diversity order psic imperfect successive interference cancellation ipsic': 11, 'addition ergodic rate ergodic sum rate analyzed': 12, 'optimal power allocation system outage probability ergodic sum rate analyzed based quality service': 13, 'simulation result provided confirm derived analytical result': 14, 'result show combination sa twr noma decrease outage probability improve diversity order effectively': 15, 'ergodic rate finally arise ceiling psic ipsic ergodic sum rate keep increasing power allocated distant user increase': 16, 'however conventional twr noma complete opposite': 17, 'summary compared twr noma twr nomasa higher diversity order enables distant user contribute system performance': 18, 'event triggered adaptive output feedback control stochastic nonlinear systems time varying full state constraints brief investigates issue event triggered adaptive output feedback control stochastic nonlinear system time varying full state constraint': 19, 'firstly unmeasurable state estimated fuzzy observer': 20, 'secondly quartic time varying barrier lyapunov function constructed avoid violation time varying constraint': 21, 'thirdly command filter technique error compensation mechanism incorporated controller design get issue explosion complexity compensate filtered error': 22, 'then event triggered mechanism introduced improve efficiency resource utilization': 23, 'shown tracking error converge desired neighborhood origin signal closed loop system bounded': 24, 'finally validity control strategy demonstrated physical example': 25, 'mmu mpu adaptation pip kernel constrained devices article present hardware based memory isolation solution constrained device': 26, 'existing solution target high end embedded system typically arm cortex a memory management unit mmu sel4 pip formally verified kernel target low end device aces minion trustlite ewok limited flexibility proposing single level isolation': 27, 'approach consists adapting pip inherit flexibility multiple level isolation using memory protection unit mpu instead mmu since mpu commonly available constrained embedded system typically armv7 cortex m4 armv8 cortex m33 similar device': 28, 'paper describes design pip mpu pip s variant based mpu rationale behind choice': 29, 'validate proposal implementation nrf52840 development kit perform various evaluation memory footprint cpu cycle energy consumption': 30, 'demonstrate although prototyped pip mpu cause sixteen overhead performance energy consumption reduce attack surface accessible application memory one hundred two privileged operation ninetynine': 31, 'pip mpu take le ten kb flash six kb core component five hundred and fifty b ram': 32, 'data driven cyber attack detection intelligent attacks islanded dc microgrids letter data driven cyber attack detection method islanded dc microgrids proposed': 33, 'data collected monitoring behavior intelligent attacker able bypass conventional cyber attack detection algorithm disrupt operation system': 34, 'reinforcement learning algorithm emulates action intelligent attacker exploit vulnerability index based cyber attack detection method discordant detection algorithm': 35, 'data used train neural networkbased detector complement conventional method additional capability detect larger set possible attack': 36, 'experiment effectiveness proposed method validated': 37, 'algorithm determining types inverse kinematics solutions sequential planar robots representation configuration space work defines new way different type solution inverse kinematics ik problem planar robot serial topology present algorithm solving it': 38, 'developed algorithm allows finding solution wide range robot using geometric approach representing point polar coordinate system': 39, 'inverse kinematics one important studied challenging problem robotics aim calculate value joint variable given desired position orientation robot s end effector': 40, 'configuration space defined joint angle basis motion planning algorithm': 41, 'areas working configuration space generated reachable different type solution': 42, 'programs created use proposed algorithm robot two three rotational degree freedom graphically present result workspace configuration space': 43, 'possibility transitioning one type solution another passing singular configuration discussed': 44, 'result important planning motion workspace configuration space well design kinematic analysis robot': 45, 'ultra wideband swarm ranging protocol dynamic dense networks nowadays wearable portable device also aerial ground robot made smaller lighter cheaper thus large hundred may form swarm participate complicated cooperative application searching rescuing mapping war battling': 46, 'devices robot swarm three important feature namely large number high mobility short distance hence form dynamic dense wireless network': 47, 'successful swarm cooperative application require low latency communication real time localization': 48, 'paper proposes use ultra wideband uwb radio technology implement functionality uwb time sensitive accurate distance calculated using transmission reception timestamps data message': 49, 'uwb swarm ranging protocol designed achieve simultaneously wireless data communication swarm ranging allows devicerobot compute distance peer neighbor time': 50, 'protocol designed dynamic dense network meanwhile also used various wireless network implemented various type devicesrobots including low end one': 51, 'experiment protocol implemented crazyflies stm32 microcontroller powered micro drone onboard uwb wireless transceiver chip dw1000': 52, 'extensive real world experiment conducted verify proposed protocol various performance aspect total nine crazyflie drone compact area': 53, 'implemented swarm ranging protocol open sourced http githubcomseu netsicrazyflie firmware': 54, 'enhancing robustness deep learning based fingerprinting improve deepfake attribution artificial fingerprinting af so called digital watermarking technique used conduct deepfake attribution ensuring medium authenticity': 55, 'however af prioritize robustness certain kind distortion making embedded watermark vulnerable standard image processing operation': 56, 'insufficient robustness reduces practicality digital watermarking technique': 57, 'address issue propose enhanced distortion agnostic artificial fingerprinting eda af framework introduces novel noise layer consisting attack booster followed convolutional network based attacker': 58, 'attacker simulates various distortion exploiting adversarial learning af distortion agnostic robustness': 59, 'meanwhile due modeling limitation convolutional network also employ attack booster apply set differentiable image distortion can not well simulated attacker': 60, 'extensive experimental result show proposed approach improves quality extracted fingerprint': 61, 'eda af improve bitwise accuracy thirtysix take another step forward road deepfake attribution': 62, 'fpga based updatable packet classification using tss combined bit selecting tree openflow switch deployed sdn enable wide spectrum non traditional application': 63, 'promising alternative brutal force tcams fpga based packet classification actively investigated': 64, 'however none existing fpga design achieve high performance search update large scale rule set': 65, 'address issue propose tcbtree fpga based algorithmic scheme packet classification': 66, 'specifically algorithmic side i two stage framework consisting heterogeneous algorithm proposed rule mapped several balanced tree without rule replication ii remaining rule centralized tss tuple space search architecture together real time feedback scheme designed enhance efficiency tss search fpga iii tree dilution method designed equalize rule distribution tree latency tree search reduced': 67, 'hardware side i efficient data structure set designed convert tree traversal addressing process break constraint limited tree depth imbalanced node distribution ii distinct fully pipelined design multiple level parallelism efficiently explored multi core multi searchengine coarse grained pipeline herein': 68, 'experimental result using classbench show that implementation tcbtree fpga average classification throughput 1k 10k 32k 100k rule set achieve seven hundred and eightyeighteight mpps four hundred and fourthree mpps two hundred and thirtyseven mpps fortyoneeight mpps respectively update throughput benchmark rule set one mups': 69, 'integrated digital twin simulation scheduling system cyber physical digital twin environment paper described research study integrated digital twin simulation scheduling system cyber physical digital twin environment manufacturing': 70, 'proposed approach provides optimal production schedule based real time information established cyber physical digital twin platform': 71, 'system integrated existing manufacturing system erp mes scada system via cyber physical digital twin platform scheduling result adaptive dynamically changing environment market demand': 72, 'proposed system designed leverage advantage digital twin simulation optimization scheduling technique': 73, 'prototype developed illustrate application case electronics manufacturing industry': 74, 'prototype instrumented rock bolt continuous monitoring roof fall hazard deep underground mines roof fall currently one dangerous threat associated underground mining great depth': 75, 'every occurrence event pose significant risk mining crew disturbs continuity mining process clearly affect economy exploitation process': 76, 'development reliable monitoring system may significantly reduce impact eventual roof failure positive effect sustainability extraction process': 77, 'within research study prototype instrumented rock bolt developed continuous stress measurement presented': 78, 'procedure four groove multilevel instrumented rock bolt described calibration process shown': 79, 'then preliminary result long term situ monitoring presented': 80, 'based continuous monitoring stress distribution within immediate roof stratum concluded developed instrumented rock bolt provides reliable result useful device ensuring possibility early warning miner increasing roof fall risk': 81, 'repqc threefour ujop fortyeight kops post quantum crypto processor multiple mathematical problems post quantum cryptography pqc investigated replace classical public cryptography algorithm would completely broken large scale quantum computer': 82, 'however current pqc scheme completely different mathematical foundation parameter set make implementation unified pqc processor extremely challenging': 83, 'address issue agile pqc processor repqc proposed work support scheme multiple mathematical problem': 84, 'first hierarchical calculation framework ranging algorithm level task level coefficient level proposed achieve desirable flexibility energy efficiency': 85, 'second hybrid processing element array built support arithmetic logical operation simultaneously algorithm hardware co design utilized task level scheduler improve algorithm oriented energy efficiency': 86, 'finally parallelism exploration algorithm level computation transformation utilized optimize configuration repqc higher throughput': 87, 'fabricated twentyeight nm process repqc achieves energy efficiency threefour ujop throughput fortyeight kops twotimes twentythreetimes higher state ofthe art work respectively': 88, 'best knowledge repqc first silicon proven pqc processor different mathematical problem': 89, 'tackling climate change machine learning climate change one greatest challenge facing humanity we machine learning ml expert may wonder help': 90, 'describe ml powerful tool reducing greenhouse gas emission helping society adapt changing climate': 91, 'smart grid disaster management identify high impact problem existing gap filled ml collaboration field': 92, 'recommendation encompass exciting research question well promising business opportunity': 93, 'call ml community join global effort climate change': 94, 'many objective optimization based federal deep generation model enhancing data processing capability iot rapid progress artificial intelligence expands wide applicability internet things iot': 95, 'meanwhile data insufficient data source privacy key supply chain challenge facing iot especially healthcare industry': 96, 'address problem healthcare iot article propose skin cancer detection model based federated learning integrated deep generation model': 97, 'first employ dual generative adversarial network address problem insufficient data': 98, 'addition improve quality generated image synchronously optimize sharpness image frechet inception distance image diversity loss using knee point driven evolutionary algorithm knea': 99, 'then protect patient information privacy training federated skin cancer framework': 100, 'finally employ isic two thousand and eighteen dataset test performance proposed training model different situation including using identically distributed data nonidentically distributed data sparse convolutional neural network fully connected convolutional neural network': 101, 'experiment result demonstrate accuracy area curve reach ninetyone eightyeight respectively': 102, 'model help resolve problem insufficient data smart medicine iot protect privacy user data also providing excellent detection rate': 103, 'exploiting combined gracegrace fo solutions determine gravimetric excitations polar motion observations gravity recovery climate experiment grace grace follow on grace fo mission used estimate gravimetric excitation polar motion pm reflects contribution mass change continental hydrosphere cryosphere pm variation': 104, 'many solution earth s gravity field variation developed institute around world based gracegrace fo data however remains inconclusive reliable determination pm excitation': 105, 'study present combined series gracegrace fobased gravimetric excitation pm computed using three corneredhat tch method wherein internal noise level combined solution reduced minimum': 106, 'compare combined series result obtained combined gracegrace fo solution provided cost g international combination service time variable gravity fields single solution elaborated center space research csr': 107, 'gravimetric excitation series evaluated comparison sum hydrological cryospheric signal geodetically observed pm excitation called gao': 108, 'result show minimizing internal noise level combined excitation series using tch method receive higher consistency gao case cost g csr solution especially non seasonal oscillation': 109, 'spectral band obtained correlation gao best combined series high zerosixtyfive zeroseventytwo Ï‡1 Ï‡2 equatorial component pm excitation respectively': 110, 'corresponding value seasonal oscillation zeroninetyone Ï‡1 zeroeightynine Ï‡2': 111, 'combined series developed study explain sixtyeight sixty overall gao variability Ï‡1 Ï‡2 respectively': 112, 'correlations emg structure movement patterns activity postural muscles able bodied wheelchair fencers study involved paralympic wheelchair fencer n seven two disability category able bodied female epee fencer n seven member polish paralympic fencing team': 113, 'performance postural muscle sword arm muscle group fencer front rear leg muscle able bodied fencer examined using surface electromyography accelerometer optitrack motion analysis system well ground force reaction platform': 114, 'activation sequence individual muscle determined structure movement pattern able bodied wheelchair fencer formulated': 115, 'statistically significant correlation found complex motor reaction time latissimus dorsi muscle activation p zerothirtynine z twosixtytwo wheelchair fencer': 116, 'high correlation vertical force emg signal value gastrocnemius caput laterale muscle zeroeightyfive p zerotwentytwo found able bodied fencer': 117, 'heuristic analysis indicated significance postural muscle movement pattern wheelchair able bodied fencer': 118, 'muscle play crucial role anticipatory postural adjustment trunk technical fencing action including attack opponent s body': 119, 'effects inter intra specific interactions moose habitat selection limited temperature habitat selection daily activity pattern large herbivore might affected inter intra specific interaction change spatial scale seasonal temperature': 120, 'reveal factor driving habitat selection moose collected moose alces alces roe deer capreolus pygargus bedfordi occurrence data analyzed multi scale habitat selection daily activity pattern moose quantified effect spatial heterogeneity distribution temperature well occurrence roe deer habitat selection process': 121, 'result suggested moose roe deer distribution spatially overlap moose habitat selection especially sensitive landscape variable large scale': 122, 'also found activity pattern sex moose degree temporal separation roe deer': 123, 'snow free season temperature drove moose habitat selection limited threshold temperature seventeen c snowy season similar temperature driving pattern due severe cold environment': 124, 'daily activity pattern moose showed seasonal change active dawn nightfall avoid heat pressure snow free season active daytime cold adaptation snow season': 125, 'consequently study provides new insight comprehensive effect environmental change inter intra specific relationship influence habitat selection daily activity pattern moose heat sensitive animal global warming': 126, 'capability exploration extended state observer based control uncertain case disturbance actuator saturation considering class nonlinear system subject actuator saturation uncertain control input article explored control capability extended state observer based active disturbance rejection controller presence disturbance uncertainty': 127, 'first actuator saturation handled convex hull extended state observer based control scheme ellipsoid invariant set inside domain attraction control system obtained using lyapunov method': 128, 'second enlargement problem invariant set also studied illustrates convex hull ellipsoid invariant set also enlargement invariant set': 129, 'finally comprehensive comparison existing method brushless dc motor demonstrate stronger capability designed controller disturbance rejection actuator saturation': 130, 'result show disturbance offset performance index integrated time absolute error integral absolute error half classical proportionalintegralderivative pid double integral sliding mode control disturbance rejection capability twice case': 131, 'sequential frame interpolation dct based video compression framework video data ubiquitous capturing transferring storing even compressed video data challenging requires substantial resource': 132, 'large amount video traffic transmitted internet improvement compressing data even small drastically impact resource consumption': 133, 'paper present hybrid video compression framework unites advantage dct based interpolation based video compression method single framework': 134, 'show work deliver visual quality or case improve visual quality reducing bandwidth ten twenty': 135, 'detecting continuous integration skip commits using multi objective evolutionary search continuous integration ci consists integrating change introduced different developer frequently automation build process': 136, 'nevertheless ci build process seen major barrier cause delay product release date': 137, 'one main reason delay simple change ie skipped trigger build represents unnecessary overhead particularly painful large project': 138, 'order cut expense ci build time propose paper skipci novel search based approach automatically detect ci skip commits based adaptation strength pareto evolutionary algorithm spea two': 139, 'approach aim provide optimal trade off two conflicting objective deal skipped non skipped commits': 140, 'evaluate approach investigate performance within cross project validation benchmark fourteen two hundred and ninetyfour ci commits fifteen project use travis ci system': 141, 'statistical test revealed approach show clear advantage baseline approach average score ninetytwo eightyfour term auc cross validation cross project validation respectively': 142, 'furthermore feature analysis reveals documentation change term appearing commit message committer experience prominent feature ci skip detection': 143, 'come cross project scenario result reveal besides documentation change strong link current previous commits result': 144, 'moreover deployed evaluated usefulness skipci industrial partner': 145, 'qualitative result demonstrate effectiveness skipci providing relevant ci skip commit recommendation developer two large software project practitioner s point view': 146, 'heimdallr fingerprinting sd wan control plane architecture via encrypted control traffic software defined wide area network sd wan emerged new paradigm steering large scale network flexibly adopting distributed software defined network sdn controller': 147, 'key building logically centralized physically distributed control plane running diverse cluster management protocol achieve consistency exchange control traffic': 148, 'meanwhile observe control traffic expose unique time series pattern directional relationship due operational structure even though traffic encrypted pattern disclose confidential information control plane topology protocol dependency exploited severe attack': 149, 'insight propose new sd wan fingerprinting system called heimdallr': 150, 'analyzes periodical operational pattern sd wan cluster management protocol context flow direction collected control traffic utilizing deep learning based approach classify cluster management protocol automatically miscellaneous control traffic datasets': 151, 'evaluation performed realistic sd wan environment consisting geographically distant three campus network one enterprise network show heimdallr classify sd wan control traffic ninetythree identify individual protocol eighty macro f one score finally infer control plane topology seventy similarity': 152, 'question answering stack applying string similarity present method evaluate fill inthe blank student answer stack using string metric possible current version stack': 153, 'increase quality evaluation use whitelist blacklist instead single teacher answer': 154, 'performance stack question equipped string metric quantitatively demonstrated evaluating use mathematics course': 155, 'regulation tracking control omnidirectional rotation spherical motors spherical motor capable omnidirectional rotation ball joint provide highly dexterous actuator system yet also present great challenge developing high performance regulation tracking controller due complex rotor dynamic': 156, 'article present complementary italic h italic sub two sub italic h sub sub italic italic c italic italic h italic sub two sub italic h sub sub italic control method precisely controlling multidegree freedom dof orientation spherical motor presence external disturbance well model uncertainty inaccuracy': 157, 'order deal tradeoff robustness control performance resulted conventional control method proposed controller featured two dof structure present italic h italic sub two sub controller nominal plant complement additional regulator designed italic h sub sub italic sense assure robustness': 158, 'unlike traditional italic h sub sub italic mixed italic h italic sub two sub and italic h sub sub italic controller italic h sub sub italic regulation conducted online accordance monitored modeling mismatch way adversely degrade performance delivered italic h italic sub two sub control hence improving conservativeness total control performance leading significant improvement tracking accuracy response time time': 159, 'numerical simulation experiment spherical motor testbed conducted validate superior performance proposed controller versus conventional control method': 160, 'investigating effects synchronized visuo tactile stimuli inducing kinesthetic illusion observational learning whole body movements skill improvement sport essential factor keeping motivation continue': 161, 'previous study showed kinesthetic illusion enhances observational learning': 162, 'however study dealt learning movement using single part body whether kinesthetic illusion induced observational learning whole body movement clarified': 163, 'study conducted experiment involving human subject confirmed synchronized visuo tactile stimulus induce kinesthetic illusion even whole body movement': 164, 'moreover also demonstrated complete mediation model synchronization visuo tactile stimulus influence kinesthetic illusion mediated body ownership': 165, 'imu smartphone camera fusion knee adduction knee flexion moment estimation walking wearable sensing computer vision could move biomechanics specialized laboratory natural environment better algorithm needed extract meaningful outcome emerging modality': 166, 'article present new model estimating biomechanical outcomesthe knee adduction moment kam knee flexion moment kfm from fusion smartphone camera wearable inertial measurement unit imus among young healthy nonobese male': 167, 'deep learning model developed extract feature fuse multimodal data estimate kam kfm': 168, 'walking data seventeen subject recorded eight imus two smartphone camera': 169, 'model used imu camera fusion significantly accurate using imus camera alone': 170, 'root meansquare error fusion model zerofortynine inline formula tex math notation latex mathbf bw cdot mathbf bh tex math inline formula kam zerosixtysix inline formula tex math notation latex mathbf bw cdot mathbf bh tex math inline formula kfm estimation lower clinically significant threshold': 171, 'larger diverse data model could enable assessment knee moment clinic home': 172, 'mfra max flowbased routing future interplanetary networks artificial satellite space station lander rover continuously deployed deep space explore planet potential resource solar system': 173, 'data transmission deep space therefore prescheduled timebandwidth specific communication exists now': 174, 'number source deep space may simultaneously transmit vast amount sensitive data earth station destination using limited bandwidth multiple hop': 175, 'article using brute force approach first show computing maximum flow node deep space network superexponential nature': 176, 'evolve number pruning technique reduce search space complex augmented deep space network trivial case maximum flow and corresponding routing may easily derived': 177, 'case possible reduce heuristic developed find good solution maximizing data flow': 178, 'finally give comparative simulation study analysis proposed technique standard contact graph routing cgr protocol': 179, 'algorithm outperforms cgr significant margin tested different network topology various traffic generation rate source': 180, 'embodied virtual interactions equity mean you': 181, 'preliminary results impact transgender avatar embodiment empathy embodied virtual interaction contribute immersive perspective taking experience turn increase empathy affiliation': 182, 'study sought investigate outcome context unconscious bias related gender identity interpersonal interaction': 183, 'conducted simulated interview virtual reality participant embodied transgender cisgender avatar interacted human controlled agent transgender woman': 184, 'preliminary result reveal difference woman men experience empathy emotional state embodied transgender avatar': 185, 'eco driving scientometric bibliometric analysis fuel efficiency transportation sector become key factor reduce greenhouse gas emission fuel consumption response negative impact global warming': 186, 'approach energy saving environmental sustainability eco driving attracted considerable research interest past decade': 187, 'review aim provide comprehensive review research eco driving using methodology literature bibliometrics content analysis vosviewer software': 188, 'following keywords ecological driving ecological routing ecological bus ecological car ecological vehicle eco driving eco routing eco driver eco bus eco car eco vehicle used paper retrieval': 189, 'query conducted january twenty two thousand and twentyone': 190, 'result take account journal article proceeding paper review without time limitation': 191, 'finally total seven hundred and sixtyseven document retrieved total publication viewed period two thousand and onetwo thousand and twenty based web science wos core collection database': 192, 'publication year leading country leading source leading institution leading author document citation document co citation analyzed explore primary trend': 193, 'in depth analysis reveals five cluster keywords review relevant study eco driving five different perspective carried identify potential trend future research hot spot eco driving': 194, 'variational bayesian gaussian mixture nonnegative matrix factorization model extract movement primitives robust control nonnegative matrix factorization nmf powerful tool parameter estimation applied numerous robotics application path planning motion trajectory prediction motion intention detection': 195, 'particular nmf successfully used extract simplified organized movement primitive myoelectric signal mes robust control multi degree freedom humanoid robot': 196, 'however mes typically contaminated complex noise source': 197, 'system performance often degrades due simplified gaussian assumption noise distribution existing nmf method': 198, 'furthermore existing nmf model unable automatically determine rank latent matrix': 199, 'address issue article present hybrid variational bayesian gaussian mixture nmf gmnmf model finite gaussian mixture model adopted fit mixed noise density function mes': 200, 'addition automatic relevant determination criterion applied automatically infer number movement primitive': 201, 'coordinate descent update rule proposed model formulated mean field variational bayesian inference': 202, 'ass model performance five synthetic noise distribution function experimental mes dataset perform six wrist movement': 203, 'result demonstrate gmnmf yield low error high robustness extracting movement primitive four competitive method robust cybernetic control': 204, 'fast actuation mechanism energy effective driving method pulse closers recloser detects fault current electric power distribution line break line restores line several fault test': 205, 'check whether fault remains recloser performs co operation opening contact reclosing while': 206, 'operation fault current flow line potentially causing damage equipment connected line': 207, 'therefore reducing current carrying time fault test desirable reduction difficult slow actuation mechanism reclosers': 208, 'article novel actuation mechanism combining permanent magnetic actuator pma solenoid actuator sa proposed': 209, 'co operation fault test proposed mechanism rapidly break closed contact hitting moving contact sa': 210, 'increase energy efficiency actuation mechanism distinctive pma driving method also proposed weakens strong holding force pma using assistive magneto motive force mmf': 211, 'structure configuration proposed mechanism explained': 212, 'performance mechanism effectiveness driving method studied cosimulations': 213, 'finally prototype developed practicality verified experiment': 214}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve sentences from the 'sentences' column of the DataFrame 'test' at index 3900.\n",
        "sentences1 = test['sentences'][3000]\n",
        "sentences1"
      ],
      "metadata": {
        "id": "5Wufnkv_SCeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8aa180e-90b4-49e6-9389-1b1dcc6e31c7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['detecting fake co visitation injection attack graph based recommendation systems recommendation system vulnerable injection attack malicious user due fundamental openness',\n",
              " 'one vulnerability fake co visitation injection attack significantly impact recommendation system since modifies system according attacker s wish',\n",
              " 'date detection co visitation injection attack challenging a one choice attribute representation node hard two practical evidence analyzing detecting anomaly real world data insufficient three challenging filter original injected co visitation data term node behavior',\n",
              " 'paper investigates unified detection framework combine attribute network structure information synergistically detect outlier node based cur decomposition residual analysis',\n",
              " 'first co visitation graph constructed using association rule attribute representation node developed',\n",
              " 'then attribute network structure information blended order identify suspicious node',\n",
              " 'extensive experiment synthetic real world dataset exhibit efficacy proposed detection approach compared state ofthe art approach',\n",
              " 'experimental result show detection performance improve fifty co visitation injection attack baseline term false alarm rate far keeping highest detection rate dr']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the similarity score and most similar sentences"
      ],
      "metadata": {
        "id": "lJwf_eMZ5N91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find and print the most similar sentences to 'input_sentence' using the Word2Vec model 'wv'.\n",
        "input_sentence_index = 1\n",
        "input_sentence = sentences1[input_sentence_index]\n",
        "similar_sentences = wv.most_similar(input_sentence, topk=10)\n",
        "\n",
        "# Print the most similar sentences\n",
        "print(f\"Most similar sentences to '{input_sentence}':\")\n",
        "for sentence, similarity in similar_sentences:\n",
        "    print(f\"Similarity: {similarity:.4f} - Sentence: '{sentence}'\")"
      ],
      "metadata": {
        "id": "H_i5Ge4DSQbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e258899-8a3a-48f2-ecc8-1ee69ba74b80"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar sentences to 'one vulnerability fake co visitation injection attack significantly impact recommendation system since modifies system according attacker s wish':\n",
            "Similarity: 0.9664 - Sentence: 'key distinction prior work article considers sensor node aided backscatter communication allow communicate negligible energy cost using ambient radio frequency rf signal'\n",
            "Similarity: 0.2360 - Sentence: 'fpga based updatable packet classification using tss combined bit selecting tree openflow switch deployed sdn enable wide spectrum non traditional application'\n",
            "Similarity: 0.2205 - Sentence: 'structure configuration proposed mechanism explained'\n",
            "Similarity: 0.2099 - Sentence: 'furthermore existing nmf model unable automatically determine rank latent matrix'\n",
            "Similarity: 0.1956 - Sentence: 'nonorthogonal multiple access enabled two way relay system using signal alignment conventional two way relay nonorthogonal multiple access twr noma system diversity order equal zero even perfect successive interference cancellation psic'\n",
            "Similarity: 0.1947 - Sentence: 'qualitative result demonstrate effectiveness skipci providing relevant ci skip commit recommendation developer two large software project practitioner s point view'\n",
            "Similarity: 0.1757 - Sentence: 'insight propose new sd wan fingerprinting system called heimdallr'\n",
            "Similarity: 0.1751 - Sentence: 'however conventional twr noma complete opposite'\n",
            "Similarity: 0.1541 - Sentence: 'insufficient robustness reduces practicality digital watermarking technique'\n",
            "Similarity: 0.1534 - Sentence: 'meanwhile observe control traffic expose unique time series pattern directional relationship due operational structure even though traffic encrypted pattern disclose confidential information control plane topology protocol dependency exploited severe attack'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors, TfidfModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IvqHr4ZMUh-8"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "word2vec_model = api.load(\"glove-wiki-gigaword-200\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kywzgQHpUiqR",
        "outputId": "1c0c0a3f-df31-4288-a183-0f3cf7975e7a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_sentences = [' '.join(sentences) for sentences in sentences]\n",
        "word_lists = [sentence.lower().split() for sentence in combined_sentences]\n"
      ],
      "metadata": {
        "id": "7M5hnneGUxdi"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "import numpy as np\n",
        "def tf_idf_v2(sent):\n",
        "    dct = Dictionary(sent)\n",
        "    corpus = [dct.doc2bow(line) for line in sent]\n",
        "    tf_idf_model = TfidfModel(corpus)\n",
        "    vector = tf_idf_model[corpus]\n",
        "    d = {dct.get(id): value for doc in vector for id, value in doc}\n",
        "    sents_emd = []\n",
        "    no_of_sent = sum(1 for i in sent)\n",
        "    for i in range(no_of_sent):\n",
        "        sent_emd = []\n",
        "        for j in range(len(sent[i])):\n",
        "            word = sent[i][j]\n",
        "            if word in word2vec_model:\n",
        "                emd = d[word]*word2vec_model[word]\n",
        "                sent_emd.append(emd)\n",
        "        sent_emd_np = np.array(sent_emd)\n",
        "        sum_ = sent_emd_np.sum(axis=0)\n",
        "        result = sum_/np.sqrt((sum_**2).sum())\n",
        "        sents_emd.append(result)\n",
        "\n",
        "    return sents_emd"
      ],
      "metadata": {
        "id": "0oJsWq4wZb51"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_emd2 = tf_idf_v2(word_lists)\n",
        "print(len(sentences_emd2))\n",
        "d2 = cosine(sentences_emd2[0],sentences_emd2[2])\n",
        "print(d2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGwHkSAlZ3MW",
        "outputId": "bba5f684-227e-4a48-e574-f3c95466c56f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "0.16141235828399658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = test['sentences'].tolist()"
      ],
      "metadata": {
        "id": "jMU40HUsgC8j"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_sent = [' '.join(sentences) for sentences in sent]\n",
        "words = [sentence.lower().split() for sentence in combined_sent]"
      ],
      "metadata": {
        "id": "gM3VfY1Rb1Xp"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "hG3Dlu-tcBmB"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence_embedding = tf_idf_v2(words)\n",
        "#sentences_emd2_2d = [sentence_embedding for sentence_embedding in sentences_emd2]\n",
        "cosine_similarities = cosine_similarity(test_sentence_embedding[1].reshape(1, -1), sentences_emd2)\n",
        "most_similar_indices = cosine_similarities.argsort()[0][::-1]\n",
        "most_similar_sentences = [word_lists[i] for i in most_similar_indices[:5]]\n",
        "\n",
        "# Print the most similar sentences\n",
        "for i, sentence in enumerate(most_similar_sentences):\n",
        "    print(f\"Similar Sentence {i + 1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8lVptDNa0Jl",
        "outputId": "e7172963-c224-4dbd-8cb6-3a0a613a0e9f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar Sentence 1: ['many', 'objective', 'optimization', 'based', 'federal', 'deep', 'generation', 'model', 'enhancing', 'data', 'processing', 'capability', 'iot', 'rapid', 'progress', 'artificial', 'intelligence', 'expands', 'wide', 'applicability', 'internet', 'things', 'iot', 'meanwhile', 'data', 'insufficient', 'data', 'source', 'privacy', 'key', 'supply', 'chain', 'challenge', 'facing', 'iot', 'especially', 'healthcare', 'industry', 'address', 'problem', 'healthcare', 'iot', 'article', 'propose', 'skin', 'cancer', 'detection', 'model', 'based', 'federated', 'learning', 'integrated', 'deep', 'generation', 'model', 'first', 'employ', 'dual', 'generative', 'adversarial', 'network', 'address', 'problem', 'insufficient', 'data', 'addition', 'improve', 'quality', 'generated', 'image', 'synchronously', 'optimize', 'sharpness', 'image', 'frechet', 'inception', 'distance', 'image', 'diversity', 'loss', 'using', 'knee', 'point', 'driven', 'evolutionary', 'algorithm', 'knea', 'then', 'protect', 'patient', 'information', 'privacy', 'training', 'federated', 'skin', 'cancer', 'framework', 'finally', 'employ', 'isic', 'two', 'thousand', 'and', 'eighteen', 'dataset', 'test', 'performance', 'proposed', 'training', 'model', 'different', 'situation', 'including', 'using', 'identically', 'distributed', 'data', 'nonidentically', 'distributed', 'data', 'sparse', 'convolutional', 'neural', 'network', 'fully', 'connected', 'convolutional', 'neural', 'network', 'experiment', 'result', 'demonstrate', 'accuracy', 'area', 'curve', 'reach', 'ninetyone', 'eightyeight', 'respectively', 'model', 'help', 'resolve', 'problem', 'insufficient', 'data', 'smart', 'medicine', 'iot', 'protect', 'privacy', 'user', 'data', 'also', 'providing', 'excellent', 'detection', 'rate']\n",
            "Similar Sentence 2: ['event', 'triggered', 'adaptive', 'output', 'feedback', 'control', 'stochastic', 'nonlinear', 'systems', 'time', 'varying', 'full', 'state', 'constraints', 'brief', 'investigates', 'issue', 'event', 'triggered', 'adaptive', 'output', 'feedback', 'control', 'stochastic', 'nonlinear', 'system', 'time', 'varying', 'full', 'state', 'constraint', 'firstly', 'unmeasurable', 'state', 'estimated', 'fuzzy', 'observer', 'secondly', 'quartic', 'time', 'varying', 'barrier', 'lyapunov', 'function', 'constructed', 'avoid', 'violation', 'time', 'varying', 'constraint', 'thirdly', 'command', 'filter', 'technique', 'error', 'compensation', 'mechanism', 'incorporated', 'controller', 'design', 'get', 'issue', 'explosion', 'complexity', 'compensate', 'filtered', 'error', 'then', 'event', 'triggered', 'mechanism', 'introduced', 'improve', 'efficiency', 'resource', 'utilization', 'shown', 'tracking', 'error', 'converge', 'desired', 'neighborhood', 'origin', 'signal', 'closed', 'loop', 'system', 'bounded', 'finally', 'validity', 'control', 'strategy', 'demonstrated', 'physical', 'example']\n",
            "Similar Sentence 3: ['mfra', 'max', 'flowbased', 'routing', 'future', 'interplanetary', 'networks', 'artificial', 'satellite', 'space', 'station', 'lander', 'rover', 'continuously', 'deployed', 'deep', 'space', 'explore', 'planet', 'potential', 'resource', 'solar', 'system', 'data', 'transmission', 'deep', 'space', 'therefore', 'prescheduled', 'timebandwidth', 'specific', 'communication', 'exists', 'now', 'number', 'source', 'deep', 'space', 'may', 'simultaneously', 'transmit', 'vast', 'amount', 'sensitive', 'data', 'earth', 'station', 'destination', 'using', 'limited', 'bandwidth', 'multiple', 'hop', 'article', 'using', 'brute', 'force', 'approach', 'first', 'show', 'computing', 'maximum', 'flow', 'node', 'deep', 'space', 'network', 'superexponential', 'nature', 'evolve', 'number', 'pruning', 'technique', 'reduce', 'search', 'space', 'complex', 'augmented', 'deep', 'space', 'network', 'trivial', 'case', 'maximum', 'flow', 'and', 'corresponding', 'routing', 'may', 'easily', 'derived', 'case', 'possible', 'reduce', 'heuristic', 'developed', 'find', 'good', 'solution', 'maximizing', 'data', 'flow', 'finally', 'give', 'comparative', 'simulation', 'study', 'analysis', 'proposed', 'technique', 'standard', 'contact', 'graph', 'routing', 'cgr', 'protocol', 'algorithm', 'outperforms', 'cgr', 'significant', 'margin', 'tested', 'different', 'network', 'topology', 'various', 'traffic', 'generation', 'rate', 'source']\n",
            "Similar Sentence 4: ['complete', 'targets', 'coverage', 'energy', 'harvesting', 'internet', 'things', 'ambient', 'backscatter', 'article', 'considers', 'deriving', 'set', 'cover', 'set', 'active', 'node', 'responsible', 'monitoring', 'target', 'internet', 'things', 'iot', 'network', 'key', 'distinction', 'prior', 'work', 'article', 'considers', 'sensor', 'node', 'aided', 'backscatter', 'communication', 'allow', 'communicate', 'negligible', 'energy', 'cost', 'using', 'ambient', 'radio', 'frequency', 'rf', 'signal', 'article', 'contains', 'three', 'main', 'novelty', 'first', 'present', 'mixed', 'integer', 'linear', 'program', 'milp', 'used', 'compute', 'global', 'optimal', 'solution', 'second', 'also', 'outline', 'centralized', 'greedy', 'scheduling', 'cgs', 'algorithm', 'selects', 'node', 'based', 'energy', 'number', 'covered', 'target', 'third', 'present', 'distributed', 'greedy', 'scheduling', 'dgs', 'algorithm', 'selects', 'node', 'according', 'energy', 'level', 'simulation', 'result', 'show', 'equipping', 'node', 'ambient', 'backscattering', 'capability', 'increase', 'ratio', 'complete', 'target', 'coverage', 'one', 'hundred', 'compared', 'existing', 'technique']\n",
            "Similar Sentence 5: ['repqc', 'threefour', 'ujop', 'fortyeight', 'kops', 'post', 'quantum', 'crypto', 'processor', 'multiple', 'mathematical', 'problems', 'post', 'quantum', 'cryptography', 'pqc', 'investigated', 'replace', 'classical', 'public', 'cryptography', 'algorithm', 'would', 'completely', 'broken', 'large', 'scale', 'quantum', 'computer', 'however', 'current', 'pqc', 'scheme', 'completely', 'different', 'mathematical', 'foundation', 'parameter', 'set', 'make', 'implementation', 'unified', 'pqc', 'processor', 'extremely', 'challenging', 'address', 'issue', 'agile', 'pqc', 'processor', 'repqc', 'proposed', 'work', 'support', 'scheme', 'multiple', 'mathematical', 'problem', 'first', 'hierarchical', 'calculation', 'framework', 'ranging', 'algorithm', 'level', 'task', 'level', 'coefficient', 'level', 'proposed', 'achieve', 'desirable', 'flexibility', 'energy', 'efficiency', 'second', 'hybrid', 'processing', 'element', 'array', 'built', 'support', 'arithmetic', 'logical', 'operation', 'simultaneously', 'algorithm', 'hardware', 'co', 'design', 'utilized', 'task', 'level', 'scheduler', 'improve', 'algorithm', 'oriented', 'energy', 'efficiency', 'finally', 'parallelism', 'exploration', 'algorithm', 'level', 'computation', 'transformation', 'utilized', 'optimize', 'configuration', 'repqc', 'higher', 'throughput', 'fabricated', 'twentyeight', 'nm', 'process', 'repqc', 'achieves', 'energy', 'efficiency', 'threefour', 'ujop', 'throughput', 'fortyeight', 'kops', 'twotimes', 'twentythreetimes', 'higher', 'state', 'ofthe', 'art', 'work', 'respectively', 'best', 'knowledge', 'repqc', 'first', 'silicon', 'proven', 'pqc', 'processor', 'different', 'mathematical', 'problem']\n"
          ]
        }
      ]
    }
  ]
}